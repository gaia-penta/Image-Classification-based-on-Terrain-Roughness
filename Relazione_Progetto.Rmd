---
title: "Classificazione di immagini sulla base della rugosità del terreno"
author: "Gaia Penta, Gianluca Ivo Tori, Nicolò Zane"
date: "08-01-2023"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Descrizione del problema e dei dati

Il dataset originale è composto da 12.982 fotogrammi, ognuno di dimensioni 3840 x 2160 pixels, raffiguranti varie sezioni di un terreno sterrato: questi sono estratti da video (effettuati in 5 giornate differenti) ripresi da una telecamera monoculare collocata sul manubrio di una mountain bike.
Ad accompagnare i fotogrammi sono presenti altri dati relativi a giroscopio (*gyroscope_calibrated_split.csv*), accelerometro (*accelerometer_calibrated_split.csv*), magnetometro (*magnetometer_split.csv*), rilevati con cadenza di 10 ms, e gps (*gps.csv*) con cadenza di 100 ms. Inoltre, sono presenti anche dati relativi a longitudine, latitudine, altitudine e velocità, rilevati con cadenza di 1000 ms, presenti nel dataset *record.csv*.
Tra queste, i ricercatori in questione hanno utilizzato come indicatore per la ruvidezza del terreno *z-axis acceleration* che rappresenta i movimenti verso l'alto e il basso del veicolo durante il percorso.
Sono infatti disponibili 8 diverse serie di etichette, sei di queste create attraverso un k-means clustering (con k = 2, 3 e 4) basato su statistiche della variabile *z-axis acceleration* e due analizzando la distribuzione dei dati, suddividendo quindi le immagini in classi sulla base della ruvidezza del terreno.
Infine, per rispettare alcuni criteri riguardanti i sensori e, affinché in ogni fotogramma fosse presente il percorso, l'insieme originale è stato filtrato fino a raggiungere una numerosità di 7.061 fotogrammi.

```{r Base, include=FALSE}
rm(list=ls())

current.path <- getwd()

# Librerie utilizzate -----------------------------------------------------

# install.packages("BiocManager")
# BiocManager::install("EBImage")
library(EBImage)
library(imager)
library(grid)
```

```{r Immagine esempio (giorno 1), include=FALSE}
# Carico labels kaggle ----------------------------------------------------

labels1_kaggle <- read.csv("Dataset/tsm_1_labels.csv")
# Caricamento delle prime labels proposte sul sito kaggle

ID_images_in <- as.numeric(gsub("s.*","",labels1_kaggle$image))
# Identificativi dell'immagine, tolgo centesimi di secondo da id

# Immagine ----------------------------------------------------------------

path_image_day1 <- "/Dataset/Images/Images/2020-07-28"
setwd(paste0(current.path,path_image_day1))
# Set WD nel percorso della cartella contenente le immagini della prima
# giornata 

k = 0
# Nel caso presente siamo interessati a visualizzare una sola immagine
# precisamente la prima appartenente all'insieme di immagini utilizzate 
# dai ricercatori di kaggle, a tal proposito si utilizza un contatore
# che ci permetta di uscire dalla cartella delle immagini del primo giorno
# una volta trovata la prima immagine 

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_image_day1 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_image_day1 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente
    
    Image_day1 <- load.image(immagine)
    # Salvo immagine di interesse

    k = k +1
    # Porto avanti il contatore
  }
  
  if(k>0) break
  # Esco dal ciclo una volta presa la prima immagine
  
}


# plot(Image(Image_day1[,,,1]))

# Visualizzo immagine nella sua grandezza reale
# 
# plot(Image(imager::resize(Image_day1,
#                           size_x = 190,
#                           size_y = 340))[,,,1])
# Visualizzo immagine nella grandezza in cui è stata salvata
```

```{r Immagine esempio giorno 1, echo=FALSE, fig.cap="Esempio di immagine a dimensioni reali", out.width = '60%',out.height='60%', fig.align="center"}
plot(Image(Image_day1[,,,1]))
```

# 2. Obiettivi

Nella creazione di questo insieme di dati, l'obiettivo dei ricercatori è stato quello di definire delle metriche di ruvidezza del terreno per poter allenare algoritmi in grado di riconoscere terreni più o meno sconnessi, in modo da poter essere utilizzati in applicazioni per la guida autonoma sia su strada, sia per gli *Autonomous Ground Vehicles*, cioè dei veicoli creati per completare delle azioni specifiche senza la supervisione umana (come ad esempio l'esplorazione spaziale).

L'obiettivo del presente progetto è stato quello di proporre diverse metodologie per la definizione delle etichette (da confrontare con le etichette proposte originariamente dai ricercatori), utilizzando poi metodi adeguati per cercare degli spunti interpretativi, sfruttando la sparsità, che può essere ipotizzabile data la struttura delle immagini, a scapito della capacità di previsione.

Per quest'ultimo fine, infatti, i ricercatori hanno utilizzato un *Convolutional Neural Network* (nome), modello che permette di considerare la complessa struttura dei dati, in particolare l'utilizzo simultaneo dei 3 canali RGB di ciascuna immagine.

# 3. Dataset

Per prima cosa si è creato il dataset sul quale successivamente sono stati stimati i modelli.

Per quanto riguarda le immagini, al fine di poterle utilizzare successivamente, esse sono state caricate per mezzo della funzione *load.image* della libreria *imager*, ottenendo inizialmente un array di dimensioni 3840x2160x3 per ciascun fotogramma.
Visto quindi l'onere computazionale che sarebbe stato richiesto utilizzando tutte e tre le matrici, si è deciso di utilizzare solo uno dei tre canali contenuti nei file PNG.
Inoltre, si passa da una dimensione di 3840x2160 (7 516 800 covariate), come visibile in Figura 1, a 190x340 (64 600 covariate), visibile in Figura 2.

Quindi, una volta caricate le immagini precedentemente selezionate dai ricercatori, queste sono state vettorizzate: ogni colonna dell'immagine è stata concatenata alla successiva per formare la singola osservazione.

```{r Immagine ridimensionata, echo=FALSE, out.width = '60%', out.height='60%', fig.align="center", fig.cap="Esempio di immagine alla grandezza utilizzata nell'analisi"}

plot(Image(imager::resize(Image_day1, 
                          size_x = 190,
                          size_y = 340))[,,,1])
# Visualizzo immagine nella grandezza in cui è stata salvata
```

```{r Processing immagini (giorno 1), eval=FALSE, include=FALSE}
# Librerie utilizzate -----------------------------------------------------
library(imager)
library(matrixcalc)

# Carico labels kaggle ----------------------------------------------------

labels1_kaggle <- read.csv("Dataset/tsm_1_labels.csv")
# Caricamento delle prime labels proposte sul sito kaggle

ID_images_in <- as.numeric(gsub("s.*","",labels1_kaggle$image))
# Identificativi dell'immagine, tolgo centesimi di secondo da id

# _________ ---------------------------------------------------------------
# 2020-07-28 --------------------------------------------------------------


path_image_day1 <- "/Dataset/Images/Images/2020-07-28"
setwd(paste0(current.path,path_image_day1))
# Set WD nel percorso della cartella contenente le immagini della prima
# giornata 

data <- NULL
ID_images_day1_in <- NULL
# Inizializzo data per salvare i pixel, e l'identificativo delle immagini
# utilizzate in fase di modellazione

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_images_day1 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day1 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente
    
    im <- load.image(immagine)
    im <- imager::resize(im, size_x = 190, size_y = 340)
    data <- rbind(data,t(matrixcalc::vec(t(im[,,,1]))))
    # Salvo per ogni immagine 190*340 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day1_in <- c(ID_images_day1_in, ID_images_day1)
    # Salvo identificativo delle immagini che sono tenute

  }
  
}

dati <- cbind(ID_images_day1_in, data)
# Unisco pixel immagini a relativo identificativo dell'immagine

nome.dataset <- paste0(current.path,"/Dataset/Pixel_day1.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(dati, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"

```

```{r Processing immagini (giorno 2), eval=F, include=F}

# _________ ---------------------------------------------------------------
# 2020-09-23 --------------------------------------------------------------

path_image_day2 <- "/Dataset/Images/Images/2020-09-23"
setwd(paste0(current.path,path_image_day2))
# Set WD nel percorso della cartella contenente le immagini della seconda
# giornata 

data <- NULL
ID_images_day2_in <- NULL
# Inizializzo data per salvare i pixel, e l'identificativo delle immagini
# utilizzate in fase di modellazione

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_images_day2 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day2 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente
    
    im <- load.image(immagine)
    im <- resize(im, size_x = 190, size_y = 340)
    data <- rbind(data,t(matrixcalc::vec(t(im[,,,1]))))
    # Salvo per ogni immagine 190*340 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day2_in <- c(ID_images_day2_in, ID_images_day2)
    # Salvo identificativo delle immagini che sono tenute
    
  }
  
}

dati <- cbind(ID_images_day2_in, data)
# Unisco pixel immagini a relativo identificativo dell'immagine

nome.dataset <- paste0(current.path,"/Dataset/Pixel_day2.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(dati, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"


```

```{r Processing immagini (giorno 3), eval=F, include=F}

# _________ ---------------------------------------------------------------
# 2020-09-24 --------------------------------------------------------------

path_image_day3 <- "/Dataset/Images/Images/2020-09-24"
setwd(paste0(current.path,path_image_day3))
# Set WD nel percorso della cartella contenente le immagini della terza
# giornata 

data <- NULL
ID_images_day3_in <- NULL
# Inizializzo data per salvare i pixel, e l'identificativo delle immagini
# utilizzate in fase di modellazione

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_images_day3 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day3 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente
    
    im <- load.image(immagine)
    im <- resize(im, size_x = 190, size_y = 340)
    data <- rbind(data,t(matrixcalc::vec(t(im[,,,1]))))
    # Salvo per ogni immagine 190*340 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day3_in <- c(ID_images_day3_in, ID_images_day3)
    # Salvo identificativo delle immagini che sono tenute
    
  }
  
}

dati <- cbind(ID_images_day3_in, data)
# Unisco pixel immagini a relativo identificativo dell'immagine

nome.dataset <- paste0(current.path,"/Dataset/Pixel_day3.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(dati, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"

```

```{r Processing immagini (giorno 4), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-09-29 --------------------------------------------------------------

path_image_day4 <- "/Dataset/Images/Images/2020-09-29"
setwd(paste0(current.path,path_image_day4))
# Set WD nel percorso della cartella contenente le immagini della quarta
# giornata 

data <- NULL
ID_images_day4_in <- NULL
# Inizializzo data per salvare i pixel, e l'identificativo delle immagini
# utilizzate in fase di modellazione

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_images_day4 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day4 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente
    
    im <- load.image(immagine)
    im <- resize(im, size_x = 190, size_y = 340)
    data <- rbind(data,t(matrixcalc::vec(t(im[,,,1]))))
    # Salvo per ogni immagine 190*340 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day4_in <- c(ID_images_day4_in, ID_images_day4)
    # Salvo identificativo delle immagini che sono tenute
    
  }
  
}

dati <- cbind(ID_images_day4_in, data)
# Unisco pixel immagini a relativo identificativo dell'immagine

nome.dataset <- paste0(current.path,"/Dataset/Pixel_day4.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(dati, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"


```

```{r Processing immagini (giorno 5), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-10-02 --------------------------------------------------------------


path_image_day5 <- "/Dataset/Images/Images/2020-10-02"
setwd(paste0(current.path,path_image_day5))
# Set WD nel percorso della cartella contenente le immagini della quinta
# giornata 

data <- NULL
ID_images_day5_in <- NULL
# Inizializzo data per salvare i pixel, e l'identificativo delle immagini
# utilizzate in fase di modellazione

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_images_day5 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day5 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente
    
    im <- load.image(immagine)
    im <- resize(im, size_x = 190, size_y = 340)
    data <- rbind(data,t(matrixcalc::vec(t(im[,,,1]))))
    # Salvo per ogni immagine 190*340 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day5_in <- c(ID_images_day5_in, ID_images_day5)
    # Salvo identificativo delle immagini che sono tenute
    
  }
  
}

dati <- cbind(ID_images_day5_in, data)
# Unisco pixel immagini a relativo identificativo dell'immagine

nome.dataset <- paste0(current.path,"/Dataset/Pixel_day5.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(dati, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"
```

Sono stati processati anche i dati relativi all'accelerometro.
Questi danno informazione sul movimento del veicolo verso l'alto e il basso e di conseguenza sulla ruvidezza del terreno.
In particolare, si è tenuto conto della deviazione standard della variabile *accelerometer\$calibrated_accel_z..g.* nella finestra di tempo relativa alla foto.

```{r Esempio accelerometro (giorno 1), include=FALSE, out.height='60%', out.width='60%'}
# Accelerometro Day 1 ----------------------------------------------

path.acc <- paste0(current.path,"/Dataset/accelerometer_calibrated_split1.csv")
# Path per dati accelerometro relativi al primo giorno

accelerometer <- read.csv(path.acc, 
                          header=T)
# Scarico i dati relativi all'accelerometro rilevati nella prima
# giornata 

accelerometer$X <- NULL
# Cancello contatore di riga

ind.acc <- which(accelerometer$utc_s..s. == ID_image_day1)
# Prendo identificativo relativo all'immagine mostrata sopra

knitr::kable(head(accelerometer[ind.acc,1:5], 3))
knitr::kable(head(accelerometer[ind.acc,6:10], 3))
knitr::kable(head(accelerometer[ind.acc,11:13], 3))
# Visualizzo dati relativi ad accelerometro per il frame mostratro sopra

```

```{r Processing accelerometro (giorno 1), eval=FALSE, include=FALSE}

# _________ ---------------------------------------------------------------
# 2020-07-28 --------------------------------------------------------------

accelerometer <- read.csv("Dataset/accelerometer_calibrated_split1.csv", 
                          header=T)
# Scarico i dati relativi all'accelerometro rilevati nella prima
# giornata 

Z_Acc_Day1 <- accelerometer$calibrated_accel_z..g.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Acc <- unique(accelerometer$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Acc <- data.frame(ID_Acc, SD_z_Acc = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Acc){
  
  index.accelerometro1 <- which(accelerometer$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(accelerometer$calibrated_accel_z..g.[index.accelerometro1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Acc$ID_Acc == i)
  Y_Sd_Acc[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Y_Sd_Acc$Day <- rep(1, nrow(Y_Sd_Acc))
# Aggiungo un identificativo relativo al giorno

Data_Day1 <- Y_Sd_Acc
# Salvo il dataset da unire agli altri giorni


```

```{r Processing accelerometro (giorno 2), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-09-23 --------------------------------------------------------------

accelerometer <- read.csv("Dataset/accelerometer_calibrated_split2.csv", 
                          header=T)
# Scarico i dati relativi all'accelerometro rilevati nella seconda
# giornata 

Z_Acc_Day2 <- accelerometer$calibrated_accel_z..g.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Acc <- unique(accelerometer$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Acc <- data.frame(ID_Acc, SD_z_Acc = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Acc){
  
  index.accelerometro1 <- which(accelerometer$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(accelerometer$calibrated_accel_z..g.[index.accelerometro1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Acc$ID_Acc == i)
  Y_Sd_Acc[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Y_Sd_Acc$Day <- rep(2, nrow(Y_Sd_Acc))
# Aggiungo un identificativo relativo al giorno

Data_Day2 <- Y_Sd_Acc
# Salvo il dataset da unire agli altri giorni


```

```{r Processing accelerometro (giorno 3), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-09-24 --------------------------------------------------------------

accelerometer <- read.csv("Dataset/accelerometer_calibrated_split3.csv", 
                          header=T)
# Scarico i dati relativi all'accelerometro rilevati nella terza
# giornata 

Z_Acc_Day3 <- accelerometer$calibrated_accel_z..g.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Acc <- unique(accelerometer$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Acc <- data.frame(ID_Acc, SD_z_Acc = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Acc){
  
  index.accelerometro1 <- which(accelerometer$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(accelerometer$calibrated_accel_z..g.[index.accelerometro1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Acc$ID_Acc == i)
  Y_Sd_Acc[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Y_Sd_Acc$Day <- rep(3, nrow(Y_Sd_Acc))
# Aggiungo un identificativo relativo al giorno

Data_Day3 <- Y_Sd_Acc
# Salvo il dataset da unire agli altri giorni


```

```{r Processing accelerometro (giorno 4), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-09-29 --------------------------------------------------------------

accelerometer <- read.csv("Dataset/accelerometer_calibrated_split4.csv", 
                          header=T)
# Scarico i dati relativi all'accelerometro rilevati nella quarta
# giornata 

Z_Acc_Day4 <- accelerometer$calibrated_accel_z..g.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Acc <- unique(accelerometer$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Acc <- data.frame(ID_Acc, SD_z_Acc = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Acc){
  
  index.accelerometro1 <- which(accelerometer$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(accelerometer$calibrated_accel_z..g.[index.accelerometro1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Acc$ID_Acc == i)
  Y_Sd_Acc[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Y_Sd_Acc$Day <- rep(4, nrow(Y_Sd_Acc))
# Aggiungo un identificativo relativo al giorno

Data_Day4 <- Y_Sd_Acc
# Salvo il dataset da unire agli altri giorni

```

```{r Processing accelerometro (giorno 5), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-10-02 --------------------------------------------------------------


accelerometer <- read.csv("Dataset/accelerometer_calibrated_split5.csv", 
                          header=T)
# Scarico i dati relativi all'accelerometro rilevati nella quinta
# giornata 

Z_Acc_Day5 <- accelerometer$calibrated_accel_z..g.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Acc <- unique(accelerometer$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Acc <- data.frame(ID_Acc, SD_z_Acc = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Acc){
  
  index.accelerometro1 <- which(accelerometer$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(accelerometer$calibrated_accel_z..g.[index.accelerometro1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Acc$ID_Acc == i)
  Y_Sd_Acc[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Y_Sd_Acc$Day <- rep(5, nrow(Y_Sd_Acc))
# Aggiungo un identificativo relativo al giorno

Data_Day5 <- Y_Sd_Acc
# Salvo il dataset da unire agli altri giorni




```

```{r Unione dati accelerometro, eval=FALSE, include=FALSE}
# Unione dati -------------------------------------------------------------


Data_final <- rbind(Data_Day1,
                    Data_Day2,
                    Data_Day3,
                    Data_Day4,
                    Data_Day5)
# Unisco in un unico dataset le standard deviation relative alle z 
# dell'accelerometro in un secondo per millisecondo

nome.dataset <- paste0(current.path,"/Dataset/Y_Sd_Acc.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(Data_final, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"

Z_Acc <- c(Z_Acc_Day1,
           Z_Acc_Day2,
           Z_Acc_Day3,
           Z_Acc_Day4,
           Z_Acc_Day5)
# Unisco in un unico vettore i valori delle z dell'accelerometro per 
# millisecondo al fine di usarle per visualizzare in confronto alle relative
# standard deviation

nome.dataset <- paste0(current.path,"/Dataset/Z_Acc.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(Z_Acc, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"

```

Dei dati relativi al giroscopio invece si decide di tenere in considerazione la variabile relativa all'asse y (*gyroscope\$calibrated_gyro_y..deg.s.*), che tiene conto delle rotazioni del veicolo sul proprio asse.
Anche qui, si considera la sua deviazione standard, che riflette, come prima, le oscillazioni del veicolo date dal suo andamento sul terreno.

```{r Esempio giroscopio (giorno 1), include=FALSE, out.height='60%', out.width='60%'}
# Gyroscopio Day 1 ------------------------------------------------------------

path.gyro <- paste0(current.path,"/Dataset/gyroscope_calibrated_split1.csv")
# Path per dati giroscopio relativi al primo giorno

gyroscope <- read.csv(path.gyro, 
                      header=T)
# Scarico i dati relativi al giroscopio rilevati nella prima
# giornata 

gyroscope$X <- NULL
# Cancello contatore di riga

ind.gyro <- which(gyroscope$utc_s..s. == ID_image_day1)
# Prendo identificativo relativo all'immagine mostrata sopra

knitr::kable(head(gyroscope[ind.gyro,1:5], 3))
knitr::kable(head(gyroscope[ind.gyro,6:10], 3))
# Visualizzo dati relativi a giroscopio per il frame mostratro sopra

```

```{r Processing giroscopio (giorno 1), eval=FALSE, include=FALSE}
# _________ ---------------------------------------------------------------
# 2020-07-28 --------------------------------------------------------------

gyroscope <- read.csv("Dataset/gyroscope_calibrated_split1.csv", 
                          header=T)
# Scarico i dati relativi al giroscopio rilevati nella prima
# giornata 

Y_Gyro_Day1 <- gyroscope$calibrated_gyro_y..deg.s.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Gyro <- unique(gyroscope$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Gyro <- data.frame(ID_Gyro, SD_y_Gyro = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Gyro){
  
  index.giroscopio1 <- which(gyroscope$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(gyroscope$calibrated_gyro_y..deg.s.[index.giroscopio1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Gyro$ID_Gyro == i)
  Y_Sd_Gyro[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Data_Day1 <- Y_Sd_Gyro
# Salvo il dataset da unire agli altri giorni

```

```{r Processing giroscopio (giorno 2), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-09-23 --------------------------------------------------------------

gyroscope <- read.csv("Dataset/gyroscope_calibrated_split2.csv", 
                      header=T)
# Scarico i dati relativi al giroscopio rilevati nella seconda
# giornata 

Y_Gyro_Day2 <- gyroscope$calibrated_gyro_y..deg.s.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Gyro <- unique(gyroscope$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Gyro <- data.frame(ID_Gyro, SD_y_Gyro = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Gyro){
  
  index.giroscopio1 <- which(gyroscope$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(gyroscope$calibrated_gyro_y..deg.s.[index.giroscopio1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Gyro$ID_Gyro == i)
  Y_Sd_Gyro[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Data_Day2 <- Y_Sd_Gyro
# Salvo il dataset da unire agli altri giorni


```

```{r Processing giroscopio (giorno 3), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-09-24 --------------------------------------------------------------

gyroscope <- read.csv("Dataset/gyroscope_calibrated_split3.csv", 
                      header=T)
# Scarico i dati relativi al giroscopio rilevati nella terza
# giornata 

Y_Gyro_Day3 <- gyroscope$calibrated_gyro_y..deg.s.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Gyro <- unique(gyroscope$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Gyro <- data.frame(ID_Gyro, SD_y_Gyro = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Gyro){
  
  index.giroscopio1 <- which(gyroscope$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(gyroscope$calibrated_gyro_y..deg.s.[index.giroscopio1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Gyro$ID_Gyro == i)
  Y_Sd_Gyro[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Data_Day3 <- Y_Sd_Gyro
# Salvo il dataset da unire agli altri giorni


```

```{r Processing giroscopio (giorno 4), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-09-29 --------------------------------------------------------------

gyroscope <- read.csv("Dataset/gyroscope_calibrated_split4.csv", 
                      header=T)
# Scarico i dati relativi al giroscopio rilevati nella quarta
# giornata 

Y_Gyro_Day4 <- gyroscope$calibrated_gyro_y..deg.s.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Gyro <- unique(gyroscope$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Gyro <- data.frame(ID_Gyro, SD_y_Gyro = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Gyro){
  
  index.giroscopio1 <- which(gyroscope$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(gyroscope$calibrated_gyro_y..deg.s.[index.giroscopio1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Gyro$ID_Gyro == i)
  Y_Sd_Gyro[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Data_Day4 <- Y_Sd_Gyro
# Salvo il dataset da unire agli altri giorni

```

```{r Processing giroscopio (giorno 5), eval=F, include=F}
# _________ ---------------------------------------------------------------
# 2020-10-02 --------------------------------------------------------------

gyroscope <- read.csv("Dataset/gyroscope_calibrated_split5.csv", 
                      header=T)
# Scarico i dati relativi al giroscopio rilevati nella quinta
# giornata 

Y_Gyro_Day5 <- gyroscope$calibrated_gyro_y..deg.s.
# Salvo le misurazioni totali da riutilizzare alla fine come confronto

ID_Gyro <- unique(gyroscope$utc_s..s.)
# Salvo ID relativi ai secondi per ogni millisecondo, per ricondurmi
# all'identificativo dell'immagine

Y_Sd_Gyro <- data.frame(ID_Gyro, SD_y_Gyro = NA)
# Inizializzo un dataset per ogni immagine della relativa giornata,
# e una colonna di NA ai quali associare la standard deviation tra le
# le registrazioni per millisecondo all'interno di un secondo

for(i in ID_Gyro){
  
  index.giroscopio1 <- which(gyroscope$utc_s..s. == i)
  # Identuficativo del secondo corrente 
  
  var.ima.zc <- sd(gyroscope$calibrated_gyro_y..deg.s.[index.giroscopio1])
  # Calco standard deviation relativa al secondo
  
  index.var <- which(Y_Sd_Gyro$ID_Gyro == i)
  Y_Sd_Gyro[index.var, 2] <- var.ima.zc
  # Alloco standard deviation calcolata al secondo corrispondente
  
  
}

Data_Day5 <- Y_Sd_Gyro
# Salvo il dataset da unire agli altri giorni



```

```{r Unione dati giroscopio, eval=FALSE, include=FALSE}
# Unione dati -------------------------------------------------------------


Data_final <- rbind(Data_Day1,
                    Data_Day2,
                    Data_Day3,
                    Data_Day4,
                    Data_Day5)
# Unisco in un unico dataset le standard deviation relative alle z 
# dell'accelerometro in un secondo per millisecondo

nome.dataset <- paste0(current.path,"/Dataset/Y_Sd_Gyro.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(Data_final, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"

Y_Gyro <- c(Y_Gyro_Day1,
            Y_Gyro_Day2,
            Y_Gyro_Day3,
            Y_Gyro_Day4,
            Y_Gyro_Day5)
# Unisco in un unico vettore i valori delle z dell'accelerometro per 
# millisecondo al fine di usarle per visualizzare in confronto alle relative
# standard deviation

nome.dataset <- paste0(current.path,"/Dataset/Y_Gyro.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(Y_Gyro, 
          file = nome.dataset)
# Salvo dati in Directory "Progetto"



```

Si è deciso poi di utilizzare anche il dataset *record.csv*, ed in particolare la variabile *enhanced_speed..m.s.*, in quanto, pur fornendo informazioni simili a quelle date dal sensore gps, esse sono state indicate dai ricercatori come più precise.

```{r Record (giorno 1), include=FALSE, out.height='60%', out.width='60%'}
# Record Day 1 ----------------------------------------------------------------

path.reco <- paste0(current.path,"/Dataset/record1.csv")
# Path per dati record relativi al primo giorno

record <- read.csv(path.reco)
# Scarico i dati relativi ai record rilevati nella prima giornata 

record$X <- NULL
# Cancello contatore di riga

ind.reco <- which(record$utc_s..s. == ID_image_day1)
# Prendo identificativo relativo all'immagine mostrata sopra

knitr::kable(head(record[ind.reco,1:5], 3))
knitr::kable(head(record[ind.reco,6:8], 3))
# Visualizzo dati relativi a record per il frame mostratro sopra


```

I dataset relativi al magnetometro e al gps quindi non sono stati utilizzati.

## 3.2 Definizione della variabile risposta e analisi esplorativa

A questo punto, si procede a creare le etichette per classificare le immagini.

```{r Caricamento della y e di variabili collegate, include=FALSE}
# Librerie utilizzate -----------------------------------------------------
library(tidyverse)
library(flsa)
library(gridExtra)
library(data.table)
library(DataExplorer)
library(ggpubr)

# Carico SD_z_Acc e Z_acc -------------------------------------------------

Sd_z_Acc <- read.csv("Dataset/Y_Sd_Acc.csv")
# Deviazioni standard z Accelerometro su mille secondi

Z_Acc <- read.csv("Dataset/Z_Acc.csv")
# Misurazioni z Accelerometro per millisecodo


# Carico Record ---------------------------------------------------------------

Record <- bind_rows(read.csv("Dataset/record1.csv"),
                    read.csv("Dataset/record2.csv"),
                    read.csv("Dataset/record3.csv"),
                    read.csv("Dataset/record4.csv"),
                    read.csv("Dataset/record5.csv"))
# Informazioni aggiuntive di cui teniamo la velocità

# Carico SD_y_Gryo e Y_Gyro ---------------------------------------------------

Sd_y_Gyro <- read.csv("Dataset/Y_Sd_Gyro.csv")
# Deviazioni standard y Giroscopio su mille secondi

Y_Gyro <- read.csv("Dataset/Y_Gyro.csv")
# Misurazioni y Giroscopio per millisecondo

# Carico labels kaggle ----------------------------------------------------

labels1_kaggle <- read.csv("Dataset/tsm_1_labels.csv")
# Caricamento delle prime labels proposte sul sito kaggle

ID_images_in <- as.numeric(gsub("s.*","",labels1_kaggle$image))
# Identificativi dell'immagine, tolgo centesimi di secondo da id

```

Si può notare in Figura 3 come il numero di rilevazioni nei vari giorni sia molto variabile.
Questo aspetto verrà tenuto in considerazione nella definizione dell'insieme di stima e di verifica nella fase di modellazione.

```{r Confronto giorni 1, echo=FALSE, out.width = '40%', out.height='40%', fig.align="center", fig.cap="Rilevazioni relative all'asse Z dell'accelerometro, divise per giorno di rilevazione", warning=FALSE}
# Plot confronti ----------------------------------------------------------

ampiezze_giorni <- cumsum(c(167160,95370,479400,514830))
# Definisco ampiezze intervalli dei giorni sul totale delle rilevazioni z

plot_Zacc_tot <- ggplot(Z_Acc, aes(x = X,
                  y = x)) +
  geom_line() +
  geom_vline(xintercept = ampiezze_giorni, col = 2, lwd = 2) +
  xlab("Misurazioni totali effettuate nei 5 giorni") +
  ylab("Z dell'Accelerometro") +
  ggtitle("") + 
  theme( axis.text = element_text(size = 6),
                      axis.title = element_text(size = 6))
# Visualizzo tutte le rilevazioni relative all'asse Z dell'accelerometro

#plot_Zacc_tot

```

```{r Confronto giorni 2, echo=FALSE, out.width = '40%', out.height='40%', fig.align="center", fig.cap="Andamento delle deviazioni standard delle rilevazioni dell'asse Z nei 5 giorni", warning=FALSE}

ampiezze_giorni <- cumsum(c(length(which(Sd_z_Acc$Day == 1)),
                            length(which(Sd_z_Acc$Day == 2)),
                            length(which(Sd_z_Acc$Day == 3)),
                            length(which(Sd_z_Acc$Day == 4))))
# l'ampiezza delle unità nei giorni cambia in quanto utilizzo sd

plot_sdZacc_tot <- ggplot(Sd_z_Acc, aes(x = X, 
                     y = SD_z_Acc)) +
  geom_line() +
  geom_vline(xintercept = ampiezze_giorni, col = 2, lwd = 2) +
  xlab("Frame Totali") +
  ylab("Sd di Z dell'Accelerometro") +
  ggtitle("") +
  theme( axis.text = element_text(size = 6),
                      axis.title = element_text(size = 6))

#plot_sdZacc_tot

# Visualizzo l'andamento delle deviazioni standard nei 5 giorni
```

```{r Confronto giorni 3, echo=FALSE, out.width = '60%', out.height='60%', fig.align="center", fig.cap="Andamento delle deviazioni standard delle rilevazioni dell'asse Z nei 5 giorni", warning=FALSE}

index_ID <- which(Sd_z_Acc$ID_Acc %in% ID_images_in)
Sd_z_Acc_Kaggle <- Sd_z_Acc[index_ID,]
ampiezze_giorni <- cumsum(c(length(which(Sd_z_Acc_Kaggle$Day == 1)),
                            length(which(Sd_z_Acc_Kaggle$Day == 2)),
                            length(which(Sd_z_Acc_Kaggle$Day == 3)),
                            length(which(Sd_z_Acc_Kaggle$Day == 4))))
# Ricercatori hanno suggerito un insieme ridotto di osservazioni poiche
# in alcune immagini non era presente la strada

plot_sdZacc_eff <- ggplot(Sd_z_Acc_Kaggle, aes(x = 1:nrow(Sd_z_Acc_Kaggle), 
                            y = SD_z_Acc )) +
  geom_line() +
  geom_vline(xintercept = ampiezze_giorni, col = 2, lwd = 2, size=0.2) +
  xlab("Frame effettivi") +
  ylab("Sd Z dell'Accelerometro") +
  ggtitle("") +
  theme( axis.text = element_text(size = 6),
                      axis.title = element_text(size = 6))

plot_sdZacc_eff
# Visualizzo solo le osservazioni nelle quali i ricercatori hanno decretato 
# si vedesse la strada
```

```{r eval=FALSE, fig.cap="", include=FALSE}
grid.newpage()
# Create layout : nrow = 2, ncol = 2
pushViewport(viewport(layout = grid.layout(2, 6)))
# A helper function to define a region on the layout
define_region <- function(row, col){
  viewport(layout.pos.row = row, layout.pos.col = col)
} 
# Arrange the plots

print(plot_Zacc_tot, vp=define_region(1, 1:3))
print(plot_sdZacc_tot, vp = define_region(1, 4:6))
print(plot_sdZacc_eff, vp = define_region(2, 2:5))
```

## 3.2.1 Fused Lasso Signal Approximator

Il primo metodo utilizzato per definire le etichette è stato il *Fused Lasso Signal-Approximator*.
Data la dipendenza temporale delle osservazioni si ritiene realistico ipotizzare una penalità riguardante rilevazioni contigue.
In questo modo, approssimando l'andamento delle osservazioni, le immagini sono state suddivise in classi a seconda che siano uguali, maggiori o minori della media, definendo così i diversi livelli di ruvidezza.

Per applicare questo metodo i dati sono stati centrati.

In seguito vengono presentati soltanto i FLSA con i parametri scelti in seguito a varie prove, le quali non vengono riportati per sinteticità.

```{r Definizione del modello fused lasso, echo=TRUE}
# Fused Lasso -------------------------------------------------------------
Y <- as.numeric(scale(Sd_z_Acc$SD_z_Acc, scale = F))
# Definisco una y dalle deviazioni standard centrate per Fused Lasso

fit <- flsa(Y)
# Stimo Fused lasso
```

Per definire le etichette con due categorie si è impostata una penalità lasso prossima a zero, così da neutralizzarla e privilegiare quella di fusione.

Si prendono quindi le osservazioni minori di 0 come classe 1 e quelle maggiori di 0 come classe 2, indicando rispettivamente bassa e alta ruvidezza.
Ciò è visibile in Figura 4.
Questo metodo porta ad un insieme di dati bilanciato con rispettivamente il 56% delle osservazioni classificate come 1 e 44% come 2.

```{r echo=TRUE}
pred <- flsaGetSolution(fit, lambda1 = 0.000001, lambda2 = 0.5)
# Dopo diverse prove si scelgono come parametro lasso 0.000001 e parametro di
# fusione 0.5
```

```{r Analisi esplorativa fused lasso K2, echo=FALSE, out.width = '50%', out.height='50%', fig.align="center", fig.cap="Suddivisione delle osservazioni in due classi sulla base di FLSA, con lambda1 = 0.000001, lambda2 = 0.5"}
# __ K2 __ -------------------------------------------------------------------
# 
# ggplot() +
#   geom_point(aes(x = 1:length(Y), y = Y)) +
#   geom_line(aes(x = 1:length(Y),
#                 y = flsaGetSolution(fit, 
#                                     lambda1 = 0.0025,
#                                     lambda2 = 1)[1,1,]),
#             col = "slateblue", lwd = 1) +
#   geom_hline(yintercept = 0, lty = 2, col = 2) +
#   xlab("Frame Totali") +
#   ylab("Deviazione standard su Z dell'Accelerometro (Centrati)") +
#   ggtitle(bquote(paste(lambda[1]==.(0.0025),
#                        " ",
#                        lambda[2]==.(1))))
# Visualizzo il comportamento del Fused lasso con parametro lasso pari a 
# 0.0025 e parametro di fusione 1


etichette2 <- rep(0,length(pred[1,1,]))
etichette2[pred[1,1,]>0] <- 2
etichette2[pred[1,1,]<0] <- 1
# Creo labels a due categorie 

plot_fused_k2 <- ggplot() +
  geom_point(aes(x = 1:length(Y), y = Y), col = etichette2) +
  geom_line(aes(x = 1:length(Y),
                y = pred[1,1,]),
            col = "gold", lwd = 1) +
  geom_hline(yintercept = 0, lty = 2, col = 2) +
  xlab("Frame Totali") +
  ylab("Deviazione standard su Z dell'Accelerometro (Centrati)") +
  ggtitle(bquote(paste(lambda[1]==.(0.000001),
                       " ",
                       lambda[2]==.(0.5))))
# Visualizzo la divisione delle osservazioni secondo le 2 labels create con 
# Fused lasso in precedenza

plot_fused_k2


tab_fused_k2 <- prop.table(table(etichette2))
# 
# knitr::kable(
#   tab_fused_k2, booktabs = TRUE,
#   caption = 'Suddivisione delle etichette create con Fused Lasso K2'
# )

```

Per definire tre categorie invece si è applicata una penalità lasso molto diversa da zero, così da favorire la selezione delle osservazioni.
Di conseguenza si definiscono tre classi, aggiungendo, rispetto al caso precedente, le osservazioni uguali a zero, determinando così tre livelli di ruvidezza.
Si veda Figura 5.
Anche in questo caso l'insieme risultati è bilanciato con rispettivamente 35%, 33% e 32%.

```{r include=FALSE}
pred <- flsaGetSolution(fit, 
                        lambda1 = 0.1,
                        lambda2 = 0.5)
# Dopo diverse prove si scelgono come parametro lasso 0.1 e parametro di
# fusione 0.5

```

```{r Analisi esplorativa fused lasso K3, echo=FALSE, out.width = '50%', out.height='50%', fig.align="center", fig.cap="Suddivisione delle osservazioni in tre classi sulla base dell'FLSA, con lambda1 = 0.1, lambda2 = 0.5"}
# __ K3 __ -------------------------------------------------------------------
# Esempio che abbiamo tolto
# ggplot() +
#   geom_point(aes(x = 1:length(Y), y = Y)) +
#   geom_line(aes(x = 1:length(Y),
#                 y = flsaGetSolution(fit, 
#                                     lambda1 = 0.25,
#                                     lambda2 = 1)[1,1,]),
#             col = "slateblue", lwd = 1) +
#   geom_hline(yintercept = 0, lty = 2, col = 2) +
#   xlab("Frame Totali") +
#   ylab("Deviazione standard su Z dell'Accelerometro (Centrati)") +
#   ggtitle(bquote(paste(lambda[1]==.(0.25),
#                        " ",
#                        lambda[2]==.(1))))
# Visualizzo il comportamento del Fused lasso con parametro lasso pari a 
# 0.25 e parametro di fusione 1

etichette3 <- rep(2,length(pred))
etichette3[pred[1,1,]>0] <- 3
etichette3[pred[1,1,]<0] <- 1
# Creo labels a due categorie 

plot_fused_k3 <- ggplot() +
  geom_point(aes(x = 1:length(Y), y = Y), col = etichette3) +
  geom_line(aes(x = 1:length(Y),
                y = pred[1,1,]),
            col = "gold", lwd = 1) +
  geom_hline(yintercept = 0, lty = 2, col = 2) +
  xlab("Frame Totali") +
  ylab("Deviazione standard su Z dell'Accelerometro (Centrati)") +
  ggtitle(bquote(paste(lambda[1]==.(0.1),
                       " ",
                       lambda[2]==.(0.5))))
# Visualizzo la divisione delle osservazioni secondo le 3 labels create con 
# Fused lasso in precedenza

plot_fused_k3

tab_fused_k3 <- prop.table(table(etichette3))

# knitr::kable(
#   tab_fused_k3, booktabs = TRUE,
#   caption = 'Suddivisione delle etichette create con Fused Lasso K3'
# )
```

\newpage

## 3.2.2 K-means Clustering

```{r Caricamento dati k-means, include=FALSE}
# K-Means -----------------------------------------------------------------
Data_cluster <- bind_cols(Sd_y_Gyro[,2:3],
                          SD_z_Acc = Sd_z_Acc[,"SD_z_Acc"])
names(Data_cluster)[1] <- "ID"
names(Record)[2] <- "ID"
Data_cluster <- right_join(Record[,c("ID","enhanced_speed..m.s.")],
                           Data_cluster,
                           by = "ID")
ind_NA_speed <- which(is.na(Data_cluster$enhanced_speed..m.s.))
media_speed <- mean(na.omit(Data_cluster$enhanced_speed..m.s.))
Data_cluster$enhanced_speed..m.s.[ind_NA_speed] <- media_speed
# Creo un dataset su cui applicare un cluster K-means, risultano 7
# valori relativi ai record sulla velocita a NA che vengono settati
# con la relativa media

```

Come secondo metodo per la definizione delle etichette, si è deciso di seguire quello usato dai ricercatori, ossia il *K-means Clustering*, aggiungendo però altre due variabili per la definizione dei gruppi.
Nello specifico, oltre alla deviazione standard di *accelerometer\$calibrated_accel_z..g.*, si utilizza anche la deviazione standard di *gyroscope\$calibrated_gyro_y..deg.s.* e la velocità *record\$enhanced_speed..m.s.*.

La scelta è data dal fatto che la presenza di buche o tratti di terreno impervio può causare, oltre che oscillazioni in alto e in basso, anche rotazioni del veicolo sul proprio asse.
Inoltre, ci si aspetta che anche la velocità dia un contributo nei movimenti della bici.
Ciò è confermato anche dalla matrice di correlazione in Figura 6, che mostra correlazione positiva tra tutte le variabili.

Dal momento che le variabili sono misurate su scale diverse, si è deciso di standardizzare i dati prima di effettuare il clustering.

```{r Analisi esplorativa k-means, echo=FALSE, out.width = '50%', out.height='50%', fig.align="center", fig.cap="Andamento delle deviazioni standard delle osservazioni relative alla variabile y del giroscopio"}
# Esplorativa Cluster -------------------------------------------

ampiezze_giorni <- cumsum(c(167160,95370,479400,514830))
# Definisco ampiezze intervalli dei giorni sul totale delle rilevazioni y
# 
# ggplot(Y_Gyro, aes(x = X,
#                    y = x)) +
#   geom_line() +
#   geom_vline(xintercept = ampiezze_giorni, col = 2, lwd = 2) +
#   xlab("Misurazioni totali effettuate nei 5 giorni") +
#   ylab("Valori assunti dall'assasse Y del Giroscopio") +
#   ggtitle("")
# Visualizzo tutte le rilevazioni relative all'asse Z dell'accelerometro

ampiezze_giorni <- cumsum(c(length(which(Sd_z_Acc$Day == 1)),
                            length(which(Sd_z_Acc$Day == 2)),
                            length(which(Sd_z_Acc$Day == 3)),
                            length(which(Sd_z_Acc$Day == 4))))
# l'ampiezza delle unità nei giorni cambia in quanto utilizzo sd

plot_sd_gyro <- ggplot(Sd_y_Gyro, aes(x = X, 
                      y = SD_y_Gyro)) +
  geom_line() +
  geom_vline(xintercept = ampiezze_giorni, col = 2, lwd = 2) +
  xlab("Frame Totali") +
  ylab("Sd di Y del giroscopio") +
  ggtitle("")
# Visualizzo l'andamento delle deviazioni standard nei 5 giorni

```

```{r Plot della velocità rispetto ai frame totali, echo=FALSE, fig.align="center", fig.cap="Andamento della velocità su tutti i frame", out.height='50%', out.width='60%'}
plot_speed <- ggplot(Data_cluster, aes(x = 1:15020, 
                         y = enhanced_speed..m.s.)) +
  geom_line() +
  geom_vline(xintercept = ampiezze_giorni, col = 2, lwd = 2) +
  xlab("Frame Totali") +
  ylab("Velocità") +
  ggtitle("")
# Visualizzo l'andamento della velocità nei 5 giorni

```

```{r Plot sd gyro e speed, eval=FALSE, fig.cap=" In alto", include=FALSE}
grid.newpage()
# Create layout : nrow = 2, ncol = 2
pushViewport(viewport(layout = grid.layout(6, 2)))
# A helper function to define a region on the layout
define_region <- function(row, col){
  viewport(layout.pos.row = row, layout.pos.col = col)
} 
# Arrange the plots

print(plot_sd_gyro, vp=define_region(1:3, 1:2))
print(plot_speed, vp = define_region(4:6, 1:2))
```

```{r Plot della correlazione, echo=FALSE, fig.align="center", out.height='50%', out.width='50%', fig.cap="Correlazione tra le variabili utilizzate per il k-means clustering"}
library(ggcorrplot)
corr <- round(cor(Data_cluster[,-1]), 1)
ggcorrplot(corr, hc.order = TRUE,
           outline.col = "white",
           ggtheme = ggplot2::theme_gray, 
           legend.title = "Correlazione",
           colors = c("#6D9EC1", "white", "#E46726"), lab=T)
# Visualizzo infine la correlazione tra SD_Z_Acc SD_Y_gyro e Velocità

```

In questo caso la definizione delle etichette attraverso *k-means* ha portato in entrambi i casi a insiemi di dati sbilanciati.
Lo sbilanciamento può portare dei problemi durante la modellazione, quindi questo verrà tenuto in considerazione applicando opportuni accorgimenti successivamente.

```{r Bilanciamento K-means clustering, include=FALSE}
# K-means con k = 2
Kmeans_k2 <- kmeans(scale(Data_cluster[,-1]), centers = 2)
```

```{r eval=FALSE, include=FALSE}
knitr::kable(
  prop.table(table(Kmeans_k2$cluster)), booktabs = TRUE,
  caption = 'Suddivisione delle etichette create con K-means Clustering con due classi')
```

```{r include=FALSE}
# K-means con k = 3
Kmeans_k3 <- kmeans(scale(Data_cluster[,-1]), centers = 3)
```

```{r eval=FALSE, include=FALSE}
knitr::kable(prop.table(table(Kmeans_k3$cluster)), booktabs = TRUE,
  caption = 'Suddivisione delle etichette create con K-means Clustering con tre classi')
```

```{r Grafici di confronto finale, eval=FALSE, include=FALSE}
# Kaggle VS Fused VS Kmeans -----------------------------------------------
Sd_z_Acc$Etichette_Fused_K3 <- factor(etichette3)
Sd_z_Acc$Etichette_Fused_K2 <- factor(etichette2)
# Salvo per ogni immagine l'etichette trovate in precedenza con Fused Lasso


Sd_z_Acc$Etichette_Kmeans_K3 <- factor(Kmeans_k3$cluster)
Sd_z_Acc$Etichette_Kmeans_K2 <- factor(Kmeans_k2$cluster)
# Salvo per ogni immagine l'etichette trovate in precedenza con K-means


index_ID <- which(Sd_z_Acc$ID_Acc %in% ID_images_in)
Sd_z_Acc_Kaggle <- Sd_z_Acc[index_ID,]
# Seleziono solo quelle immagini selzionate dai ricercatori

data.help <- data.frame(ID_Acc = ID_images_in, 
                        Etichette_Kaggle_K2 = factor(labels1_kaggle$tsm1_k2),
                        Etichette_Kaggle_K3 = factor(labels1_kaggle$tsm1_k3))
Sd_z_Acc_Kaggle <- inner_join(Sd_z_Acc_Kaggle, data.help, by = "ID_Acc")
# Tramite un dataset di aiuto creo un dataset contenente le standard deviation
# sull'asse z dell'accelerometro e tutte le etichette da confrontare di seguito
# e da usare in fase di modellazione 
```

```{r Plot fused k2, eval=FALSE, include=FALSE, fig.align="center", fig.cap="Suddivisione", out.height='60%', out.width='60%'}
plot_Fused_k2 <- 
  ggplot(Sd_z_Acc_Kaggle, aes(x = 1:nrow(Sd_z_Acc_Kaggle), 
                              y = SD_z_Acc,
                              color = Etichette_Fused_K2)) +
  geom_point(shape=20) +
  xlab("Frame") +
  ylab("Sd su Z dell'Accelerometro") +
  ggtitle("")+
  theme(legend.position = "none", 
                      axis.text = element_text(size = 6),
                      axis.title = element_text(size = 6),
        legend.text = element_text(size = 6),
        legend.title = element_text(size = 6))


#plot_Fused_k2
```

```{r Plot Kaggle k2,eval=FALSE, include=FALSE, fig.align="center", fig.cap="Suddivisione", out.height='60%', out.width='60%'}

plot_Kaggle_k2 <- 
  ggplot(Sd_z_Acc_Kaggle, aes(x = 1:nrow(Sd_z_Acc_Kaggle), 
                              y = SD_z_Acc,
                              color = Etichette_Kaggle_K2)) +
  geom_point(shape=20) +
  xlab("Frame") +
  ylab("Sd su Z dell'Accelerometro") +
  ggtitle("")+
  theme(legend.position = "none", 
                      axis.text = element_text(size = 6),
                      axis.title = element_text(size = 6),
        legend.text = element_text(size = 6),
        legend.title = element_text(size = 6))


#plot_Kaggle_k2
```

```{r Plot Kmeans K2, eval=FALSE, include=FALSE, fig.align="center", fig.cap="Suddivisione", out.height='60%', out.width='60%'}
plot_Kmeans_k2 <- 
  ggplot(Sd_z_Acc_Kaggle, aes(x = 1:nrow(Sd_z_Acc_Kaggle), 
                              y = SD_z_Acc,
                              color = Etichette_Kmeans_K2)) +
  geom_point(shape=20) +
  xlab("Frame") +
  ylab("Sd su Z dell'Accelerometro") +
  ggtitle("")+
  theme(legend.position = "right", 
                      axis.text = element_text(size = 6),
                      axis.title = element_text(size = 6),
        legend.text = element_text(size = 6),
        legend.title = element_text(size = 6))


#plot_Kmeans_k2
```

```{r echo=FALSE, eval=FALSE, include=FALSE, fig.align="center", fig.cap="Confronto etichette a due classi", out.width='60%'}
grid.newpage()
# Create layout : nrow = 2, ncol = 2
pushViewport(viewport(layout = grid.layout(2, 6)))
# A helper function to define a region on the layout
define_region <- function(row, col){
  viewport(layout.pos.row = row, layout.pos.col = col)
} 
# Arrange the plots

print(plot_Fused_k2, vp=define_region(1, 1:3))
print(plot_Kaggle_k2, vp = define_region(1, 4:6))
print(plot_Kmeans_k2, vp = define_region(2, 2:5))
```

```{r Plot Fused k3, eval=FALSE, include=FALSE, fig.align="center", out.height='60%', out.width='60%', fig.cap="Suddivisione delle osservazioni in tre classi individuata dal fused lasso signal approximator"}
plot_Fused_k3 <-
  ggplot(Sd_z_Acc_Kaggle, aes(x = 1:nrow(Sd_z_Acc_Kaggle), 
                              y = SD_z_Acc,
                              color = Etichette_Fused_K3)) +
   geom_point(shape=20) +
  xlab("Frame") +
  ylab("Sd su Z dell'Accelerometro") +
  ggtitle("") +
  theme(legend.position = "top", 
                      axis.text = element_text(size = 6),
                      axis.title = element_text(size = 6),
        legend.text = element_text(size = 6),
        legend.title = element_text(size = 6))

#plot_Fused_k3
```

```{r Plot Kaggle K3, eval=FALSE, include=FALSE, fig.align="center", out.height='60%', out.width='70%', fig.cap="Suddivisione originaria delle osservazioni in tre classi individuata dai ricercatori"}
plot_Kaggle_k3 <- 
  ggplot(Sd_z_Acc_Kaggle, aes(x = 1:nrow(Sd_z_Acc_Kaggle), 
                              y = SD_z_Acc,
                              color = Etichette_Kaggle_K3)) +
  geom_point(shape=20) +
  xlab("Frame") +
  ylab("Sd su Z dell'Accelerometro") +
  ggtitle("") +
  theme(legend.position = "top", 
                      axis.text = element_text(size = 6),
                      axis.title = element_text(size = 6),
        legend.text = element_text(size = 6),
        legend.title = element_text(size = 6))

#plot_Kaggle_k3
```

```{r Plot Kmeans K3, eval=FALSE, include=FALSE, fig.align="center", out.height='60%', out.width='60%', fig.cap="Suddivisione delle osservazioni in tre classi individuata dal K-Means Clustering"}
plot_Kmeans_k3 <- 
  ggplot(Sd_z_Acc_Kaggle, aes(x = 1:nrow(Sd_z_Acc_Kaggle), 
                              y = SD_z_Acc,
                              color = Etichette_Kmeans_K3)) +
   geom_point(shape=20) +
  xlab("Frame") +
  ylab("Sd su Z dell'Accelerometro") +
  ggtitle("") +
  theme(legend.position = "top", 
        axis.text = element_text(size = 6),
        axis.title = element_text(size = 6),
        legend.text = element_text(size = 6),
        legend.title = element_text(size = 6))

#plot_Kmeans_k3
```

```{r Confronti K3, eval=FALSE, include=FALSE, fig.align="center", fig.cap="Confronto etichette con tre classi", out.width='120%'}
grid.newpage()
# Create layout : nrow = 2, ncol = 2
pushViewport(viewport(layout = grid.layout(6, 8)))
# A helper function to define a region on the layout
define_region <- function(row, col){
  viewport(layout.pos.row = row, layout.pos.col = col)
} 
# Arrange the plots

# print(plot_Fused_k3, vp=define_region(1:3, 1:4))
# print(plot_Kaggle_k3, vp = define_region(1:3, 5:8))
# print(plot_Kmeans_k3, vp = define_region(4:6, 3:6))
```

```{r Creazione dataset finale, eval=FALSE, include=FALSE}
## 3.2.3 Creazione del dataset finale
# -------------------------------------------------------------------------
# Creo dataset per modellazione  ------------------------------------------

px1 <- fread("Dataset/Pixel_day1.csv")
names(px1)[2] <- "ID_images_in"
px2 <- fread("Dataset/Pixel_day2.csv")
names(px2)[2] <- "ID_images_in"
px3 <- fread("Dataset/Pixel_day3.csv")
names(px3)[2] <- "ID_images_in"
px4 <- fread("Dataset/Pixel_day4.csv")
names(px4)[2] <- "ID_images_in"
px5 <- fread("Dataset/Pixel_day5.csv")
names(px5)[2] <- "ID_images_in"
# Carico i dataset relativi ai pixxel delle immagini nei 5 diversi giorni
# cambio il nome dell'identificativo al fine di poter unire i pixel
# con le etichette trovate in precedenza

px_tot <- bind_rows(px1,
                    px2,
                    px3,
                    px4,
                    px5)
px_tot$V1 <- NULL
# Unisco tutti i pixxel di tutti e 5 i giorni in un unico dataset

Sd_z_Acc_Kaggle$X <- NULL
names(Sd_z_Acc_Kaggle)[1] <- "ID_images_in"
# Sistemo il dataset con le etichette e la standard deviation delle z 

id_outka <- which(duplicated(Sd_z_Acc_Kaggle$ID_images_in))
id_outpx <- which(duplicated(px_tot$ID_images_in))
# Ci si accorge che due frame hanno secondi ripetuti, in quanto per noi 
# l'identificativo corrispondeva al secondo del video, si isolano gli 
# identificativi ripetuti per poi aggiungerli di seguito, tale operazione
# è dovuta al comportamento della funzione "inner_join" che ci permette 
# tramite un campo identificativo di aggregare due record di dataset distinti,
# se pero ci sono dei campi identificativi ripetuti, vengono considerati come 
# unici per tanto alla fine sarebbero state create 4 righe in piu di quelle che
# realmente ci sarebbero dovute essere

Sd_z_Acc_Kaggle2 <- Sd_z_Acc_Kaggle[-id_outka,]
px_tot2 <- px_tot[-id_outpx,]
Dati <- inner_join(Sd_z_Acc_Kaggle2, px_tot2, by = "ID_images_in")
Sd_z_Acc_Kaggle2 <- Sd_z_Acc_Kaggle[id_outka,]
px_tot2 <- px_tot[id_outpx,]
Dati2 <- inner_join(Sd_z_Acc_Kaggle2, px_tot2, by = "ID_images_in")
Dati <- bind_rows(Dati,
                  Dati2)
# Si uniscono i due dataset creati, il primo contenente "ID_images_in" unici, 
# con lunghezza 7066, mentre il secondo contenente solo due valori, quelli 
# ripetuti

rm(list=setdiff(ls(), "Dati"))
# Rimuoviamo tutti gli oggetti dall'ambiente attuale

write.csv(Dati, file = "Dataset/Data_k2_k3_models.csv")
# Salvo il dataset finale nella WD "Dataset"

```

## 3.3 Dataset alternativi

Coscienti del fatto che il ridimensionamento dell'immagine sia solo uno dei metodi per approcciare il problema della riduzione della complessità e della dimensionalità, decidiamo di provare alcune alternative.

```{r include=FALSE}
# Librerie utilizzate -----------------------------------------------------

# install.packages("BiocManager")
# BiocManager::install("EBImage")
library(EBImage)
library(imager)
library(data.table)
library(flsa)
library(matrixcalc)

# Carico labels kaggle ----------------------------------------------------

labels1_kaggle <- read.csv("Dataset/tsm_1_labels.csv")
# Caricamento delle prime labels proposte sul sito kaggle

ID_images_in <- as.numeric(gsub("s.*","",labels1_kaggle$image))
# Identificativi dell'immagine, tolgo centesimi di secondo da id


# Carico Data Models ------------------------------------------------------

dati <- data.frame(fread("Dataset/Data_k2_k3_models.csv"))
# Caricamento dataset per modellazione 

```

Verifichiamo se effettivamente esiste un pattern comune nelle immagini, andando a creare un'immagine media, facendo la media per colonna in tutto il dataset.
Com'è possibile notare dall'immagine in Figura 7.a, il sentiero si concentra nella parte centrale ed inferiore dell'immagine, mentre ai lati si trova presumibilmente la vegetazione; nella parte superiore spesso è invece presente il cielo o squarci di questo.

```{r Immagine media Fused Lasso, include=FALSE}
# Immagine media --------------------------------------------------------

medie.colonne <- colMeans(dati[,-c(1:10)])
# calcolo la media di ogni pixel per tutte le immagini

immagine.media <- matrix(as.numeric(medie.colonne),
                          byrow = T,
                          ncol = 340,
                          nrow = 190)
# Passo da vettore a matrice della media dei pixel

```

```{r Immagine media, echo=FALSE, fig.align="center", fig.cap="Rappresentazione dell'immagine media effettuata sui risultati del fused lasso", out.height='60%', out.width='50%'}
#plot(Image(immagine.media))
# Visualizzo la media dei pixel
```

Un altro metodo possibile per capire come trattare le varie immagini è utilizzare il *Fused Lasso Signal Approximator* per eliminare del rumore presente nelle immagini andando ad appiattire la dinamica dei pixel.
In questo caso la penalità, in riferimento all'immagine, agisce per colonna.
Come si può vedere dall'immagine in Figura 7.b questo metodo porta ad identificare in particolare la parte inferiore e superiore dell'immagine, lasciando la parte centrale più scura.

Un'altra possibile alternativa sarebbe quella di utilizzare il *Fused Lasso 2D*, che penalizza i pixel vicini sia per colonna che per riga; si decide comunque di non intraprendere questa strada in primis per il carico computazionale ma anche perché riteniamo che non porti a risultati significativamente diversi.

```{r Denoising Fused Lasso, echo=FALSE}
# Fused Signal Approximator -----------------------------------------------
fit.FSA <- flsa(as.numeric(medie.colonne))
FSA.px <- as.vector(flsaGetSolution(fit.FSA, lambda1 = 0.2, lambda2 = 0.5))
# Fused lasso su tutte le medie dei pixel vettorizzati

immagine.FSA <- matrix(FSA.px, byrow = T, ncol = 340, nrow = 190)
# Passo da vettore a matrice per i valori stimati dal fused sulla media
# dei pixel

fit.FSA.1 <- flsa(as.numeric(dati[1,-c(1:10)]))
FSA.px.1 <- as.vector(flsaGetSolution(fit.FSA.1, lambda1 = 0.2, lambda2 = 0.5))
immagine.FSA.1 <- matrix(FSA.px.1,
                         byrow = T,
                         ncol = 340,
                         nrow = 190)
# Riproduco quanto fatto in precedenza per la prima immagine
```

```{r Confronto tra Fused su pixel medi e prima immagine, echo=FALSE, fig.align="center", fig.cap="Confronto tra immagine media (immagine a sinistra, (a.)), Fused Signal Approximator su pixel medi (immagine centrale (b.)) e Fused Signal Approximator sull'immagine mostrata in Figura 2 (immagine a destra (c.))", out.height='60%', out.width='60%'}

par(mfrow = c(1,3))
plot(Image(immagine.media))
# Visualizzo immagine media

plot(Image(immagine.FSA))
# Visualizzo Fused Signal Approximator su pixel medii

plot(Image(immagine.FSA.1))
# Visualizzo Fused Signal Approximator su pixel prima immagine
```

```{r Creazione dataset fused, eval=FALSE, include=FALSE}
# Creo Dataset Fused ------------------------------------------------------

data.mat <- matrix(NA,
                   ncol = ncol(dati[,-c(1:10)]),
                   nrow = nrow(dati))
# Inizializzo una matrice vuota per caricarci successivamente 
# i pixel denoisati dal fused

for(i in 1:nrow(dati)){
  fit <- flsa(as.numeric(dati[i,-c(1:10)]))
  data.mat[i,] <- as.vector(flsaGetSolution(fit,
                                            lambda1 = 0.2,
                                            lambda2 = 0.5))
}
# Data l'impossibilità di valutare singolarmente ogni immagine si utilizzano
# i parametri utilizzati sopra per applicare denoise

```

Dalle informazioni ricavate dall'immagine media e dal Fused Lasso Signal Approximator si è deciso di ritagliare poi le immagini conservando solo la parte riguardante il terreno.
Si sono utilizzate le immagini a delle dimensioni iniziali pari a 340x200 pixels, tagliate in modo tale da ottenere una dimensione pari a 170x100.
Un esempio è mostrato in Figura 8.
L'obiettivo è quello di conservare più informazione riguardo la sezione d'interesse.

```{r Plot immagine ridotta, echo=FALSE}
# Immagine ridotta -----------------------------------------------------------

path_image_day1 <- "/Dataset/Images/Images/2020-07-28"
setwd(paste0(current.path,path_image_day1))
# Set WD nel percorso della cartella contenente le immagini della prima
# giornata 

k = 0
# Nel caso presente siamo interessati a visualizzare una sola immagine
# precisamente la prima appartenente all'insieme di immagini utilizzate 
# dai ricercatori di kaggle, a tal proposito si utilizza un contatore
# che ci permetta di uscire dalla cartella delle immagini del primo giorno
# una volta trovata la prima immagine 

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_image_day1 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_image_day1 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente
    
    Image_day1 <- load.image(immagine)
    # Salvo immagine di interesse
    
    k = k +1
    # Porto avanti il contatore
  }
  
  if(k>0) break
  # Esco dal ciclo una volta presa la prima immagine
  
}


```

```{r Confronto tra immagine originale e imamgine tagliata, echo=FALSE, fig.align="center", fig.cap="Confronto tra immagine originale e immagine tagliata", out.height='60%', out.width='60%'}
par(mfrow=c(1,2))
plot(Image(imager::resize(Image_day1, 
                          size_x = 200,
                          size_y = 340))[,,,1])
# Visualizzo immagine nella grandezza in cui è stata salvata

plot(Image(imager::resize(Image_day1, 
                          size_x = 200,
                          size_y = 340))[,,,1])
polygon(c(0,49,49,0),c(0,0,340,340),col = 1)
polygon(c(150,200,200,150),c(0,0,340,340),col = 1)
polygon(c(0,200,200,0),c(0,0,170,170),col = 1)
abline(v = 49, col = 3, lwd = 4)
abline(v = 150, col = 3, lwd = 4)
segments(0,170,200,170,col = 4,lwd = 4)
# Evidenzio la parte dell'immagine che andremo a tenere

```

```{r Creazione dataset immagini ridotte, eval=FALSE, include=FALSE}
# Creo Dataset Imagini ridotte --------------------------------------------
data <- matrix(NA,
               ncol = 17100,
               nrow = 7068)
ID_images_day_in <- numeric(7068)
# Inizializzo sia una matrice vuota dove salvare i valori dei pixel vettorizzati
# sia un vettore vuoto dove salvare i corrispondenti identificativi

# _________ ---------------------------------------------------------------
# 2020-07-28 --------------------------------------------------------------

path_image_day1 <- "/Dataset/Images/Images/2020-07-28"
setwd(paste0(current.path,path_image_day1))
# Set WD nel percorso della cartella contenente le immagini della prima
# giornata

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  

  ID_images_day1 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day1 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente

    im <- load.image(immagine)
    im <- imager::resize(im, size_x = 200, size_y = 340)
    data[i,] <- matrixcalc::vec(t(im[-c(1:49,150:200),170:340,,1]))[,1]
    # Salvo per ogni immagine 100*171 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day_in[i] <- ID_images_day1
    # Salvo identificativo delle immagini che sono tenute
    
  }
  
}

# _________ ---------------------------------------------------------------
# 2020-09-23 --------------------------------------------------------------

path_image_day2 <- "/Dataset/Images/Images/2020-09-23"
setwd(paste0(current.path,path_image_day2))
# Set WD nel percorso della cartella contenente le immagini della seconda
# giornata 

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_images_day1 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day1 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente
    
    im <- load.image(immagine)
    im <- imager::resize(im, size_x = 200, size_y = 340)
    data[i,] <- matrixcalc::vec(t(im[-c(1:49,150:200),170:340,,1]))[,1]
    # Salvo per ogni immagine 100*171 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day_in[i] <- ID_images_day1
    # Salvo identificativo delle immagini che sono tenute
    
  }
  
}

# _________ ---------------------------------------------------------------
# 2020-09-24 --------------------------------------------------------------

path_image_day3 <- "/Dataset/Images/Images/2020-09-24"
setwd(paste0(current.path,path_image_day3))
# Set WD nel percorso della cartella contenente le immagini della terza
# giornata 
for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_images_day1 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day1 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente

    im <- load.image(immagine)
    im <- imager::resize(im, size_x = 200, size_y = 340)
    data[i,] <- matrixcalc::vec(t(im[-c(1:49,150:200),170:340,,1]))[,1]
    # Salvo per ogni immagine 100*171 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day_in[i] <- ID_images_day1
    # Salvo identificativo delle immagini che sono tenute
  }
}

# _________ ---------------------------------------------------------------
# 2020-09-29 --------------------------------------------------------------

path_image_day4 <- "/Dataset/Images/Images/2020-09-29"
setwd(paste0(current.path,path_image_day4))
# Set WD nel percorso della cartella contenente le immagini della quarta
# giornata 
for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 

  ID_images_day1 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day1 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente

    im <- load.image(immagine)
    im <- imager::resize(im, size_x = 200, size_y = 340)
    data[i,] <- matrixcalc::vec(t(im[-c(1:49,150:200),170:340,,1]))[,1]
    # Salvo per ogni immagine 100*171 pixxel vettorizzati come vettori
    # riga
    
    ID_images_day_in[i] <- ID_images_day1
    # Salvo identificativo delle immagini che sono tenute
    
  }
  
}

# _________ ---------------------------------------------------------------
# 2020-10-02 --------------------------------------------------------------


path_image_day5 <- "/Dataset/Images/Images/2020-10-02"
setwd(paste0(current.path,path_image_day5))
# Set WD nel percorso della cartella contenente le immagini della quinta
# giornata 

for (immagine in list.files()){
  # Ciclo per tutte le immagini contenute nella cartella 
  
  ID_images_day1 <- as.numeric(gsub("s.*","",substr(immagine, 1,15)))
  # Salvo l'identificativo dell'immagine corrente 
  
  if(ID_images_day1 %in% ID_images_in){
    # Specifico condizione per la quale sono interessato a salvare i 
    # pixxel dell'immagine corrente

    im <- load.image(immagine)
    im <- imager::resize(im, size_x = 200, size_y = 340)
    data[i,] <- matrixcalc::vec(t(im[-c(1:49,150:200),170:340,,1]))[,1]
    # Salvo per ogni immagine 100*171 pixxel vettorizzati come vettori
    # riga
    
    
    ID_images_day_in[i] <- ID_images_day1
    # Salvo identificativo delle immagini che sono tenute
    
  }
  
}

# _________ ---------------------------------------------------------------
# Salvo risultati ---------------------------------------------------------

nome.dataset.matrice.fused <- paste0(current.path,"/Dataset/Matrice_fused.csv")
nome.dataset.pixel.piccoli <- paste0(current.path,"/Dataset/Pixel_piccoli.csv")
# Definisco dove e con che nome salvare i dati appena creati

write.csv(data.mat,
          file = nome.dataset.matrice.fused)
# Salvo i pixel denoisati

write.csv(data, 
          file = nome.dataset.pixel.piccoli)
# Salvo dati in Directory "Progetto"



```

\newpage

# 4. Modelli

Si decide di dividere il dataset in due parti: un insieme di train e uno di test.
In particolare si è deciso di tenere come insieme di test i dati relativi al terzo giorno, in quanto dotato di una numerosità sufficiente, e i rimanenti giorni come training set.
Un altro modo in cui si sarebbe potuto dividere il dataset sarebbe stato quello cronologico, utilizzando i primi 4 giorni come training e l'ultimo come test, oppure tenere nel train tutte le parti relative ai primi km di riprese e le ultime parti come test.

Inoltre, si fissano i 5 fold per la convalida incrociata utile per la regolarizzazione dei modelli, così da renderli comparabili.

Si mostreranno quindi le performance di ciascun modello su ogni etichetta, analizzandone eventuali fallimenti.

Come già detto in precedenza, i dataset con le etichette ottenute con k-means clustering e le etichette originali definite dai ricercatori risultano essere sbilanciate, con proporzioni visibili in Tabella 1.

```{r Tabella proporzioni, echo=FALSE, fig.cap="Bilanciamento delle etichette nei vari dataset", out.height="50%", out.width="50%" }
load("Environment/tabella_prop.RData")
knitr::kable(round(tab.prop, 3), caption="Bilanciamento delle etichette nei vari dataset")
```

Al fine di tenere conto di tale sbilanciamento si è deciso di utilizzare come metrica l'indice Kappa di Cohen, definito come: $$ \kappa = \frac{p_0 - p_e}{1-p_e} $$

dove $p_e$ è la probabilità di classificare corretamente in modo casuale, mentre $p_0$ è l'accuracy totale.
Esso, durante l'adattamento dei modelli, dovrà essere massimizzato.

Questo indice infatti consente di dare un'indicazione di quanto il modello stia classificando meglio rispetto ad un modello che classifica a caso, supponendo che quest'ultimo stia utilizzando semplicemente le frequenze di ciascuna classe.

Onde evitare di definire delle procedure ad hoc per la massimizzazione di tale indice, si è deciso di utilizzare comunque le funzioni di cross validation dei modelli utilizzati, in quanto la maggior parte di essi consentono comunque di ottenere in output la stima del predittore lineare per ogni parametro di penalizzazione testato per l'insieme *out of fold* utilizzando il tasso di errata classificazione.
Si è quindi provveduto nei vari casi a calcolare il corrispondente valore di $\kappa$ riproducendo quindi una convalida incrociata.

In alcuni modelli, dove il solo utilizzo di tale indice non sarà sufficiente, si utilizzerà anche una soglia specifica nella definizione delle previsioni.

```{r Loading degli environment necessari, include=FALSE}
load("Environment/Modelli_Fused_def1_RMD.RData")
load("Environment/Modelli_Kmeans_def3.RData")
load("Environment/Modelli_Kaggle_def1.RData")
```

```{r include=FALSE}
# Librerie utilizzate -----------------------------------------------------
library(data.table)
library(glmnet)
library(sparseSVM)
library(ncvreg)
library(pamr)
library(tidyverse)
require(doMC)
```

```{r Divisione Train e Test, eval=FALSE, include=FALSE}
current.path <- getwd()
# Workdirectory "Progetto"

# Carico dati --------------------------------------------------------

dati <- data.frame(fread("Dataset/Data_k2_k3_models.csv"))
# carico il dataset con i pixel presi inizialmente

id.test <- which(dati$Day == 3)
# Definisco di usare come insieme di verifica il terzo giorno 

train <- dati[-id.test,]
test <- dati[id.test,]
# Creo insieme di stima e insieme di verifica

rm(dati)
# Cancello i dati dall'ambiente attuale cosi da avere meno memoria occupata

# _____________ -----------------------------------------------------------

set.seed(42)
fold <- sample(5, nrow(train), replace = T)
# Per coerenza definisco dei fold uguali per tutti i modelli che verranno
# stimati in CV

X_train <- as.matrix(train[,-c(1:10)])
X_test <- as.matrix(test[,-c(1:10)])
# Poichè la maggior parte delle funzioni per modellare richiede che la matrice
# del disegno sia un oggetto matrice converto gli insiemi di stima e verifica 
# in matrici
```

## 4.1 Lasso Logistico

Il primo modello adattato è la regressione logistica con penalità lasso (norma L1).
Per quanto riguarda le etichette *fused lasso*, si effettua una convalida incrociata su 5 fold su una griglia di valori di $log(\lambda)$ che va da -5 a -2.5, di lunghezza 100.
Utilizzando il $log(\lambda)$ che minimizza l'errore di classificazione, pari a -3.889, vengono selezionati 60 coefficienti compresa l'intercetta.

```{r Codice lasso da mostrare, eval=FALSE}
Lasso_logistico_Fused <- cv.glmnet(X_train, factor(train$Etichette_Fused_K2),
                                   type.measure = "class", family = "binomial",
                                   alpha = 1, nfolds = 5, foldid = fold,
                                   lambda = lambda.grid, parallel = T,
                                   trace.it = 1, seed = 42)
```

```{r Lasso Logstico Fused, eval=FALSE, include=FALSE}
# Lasso Logistico Fused -------------------------------------------------------------
lambda.grid <- exp(seq(-5,-2.5,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia è il risultato di prove precedenti che
# non vengono riportate

registerDoMC(cores = 5)
# Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
# che intendo utilizzare per parallelizzare il modello 

Lasso_logistico_Fused <- cv.glmnet(X_train,
                                   factor(train$Etichette_Fused_K2),
                                   type.measure = "class",
                                   family = "binomial",
                                   alpha = 1,
                                   nfolds = 5,
                                   foldid = fold,
                                   lambda = lambda.grid,
                                   parallel = T,
                                   trace.it = 1,
                                   seed = 42)
# Viene stimato un modello logistico con penalita lasso, in CV a 5 fold con 
# metrica di ottimizzazione l'errore di errata classificazione,
# viene oltretutto fissato un seed per la riproducibilità dei risultati

plot(Lasso_logistico_Fused)
# Visualizzo andamento Errore di classificazione per i vari lambda

n.coef_Lasso_Fused <- length(which(coef(Lasso_logistico_Fused, s=Lasso_logistico_Fused$lambda.min)!=0))
# Numero di coefficienti diversi da 0 con lambda.min


y.hat.lasso_Fused <- predict(Lasso_logistico_Fused, 
                             s = Lasso_logistico_Fused$lambda.min, 
                             newx = X_test, 
                             type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.lasso_Fused <- tabella.sommario(test$Etichette_Fused_K2 , 
                                        y.hat.lasso_Fused)
Metriche_Lasso_Fused <- indici.errore(Tabella.lasso_Fused)
# Salvo le varie metriche ottenute da tale modello 

```

```{r Plot Lasso CV Fused, echo=FALSE, fig.cap="Andamento dell'errore di classificazione medio al variare di lambda Fused", out.height="60%", out.width="60%", fig.align="center"}
#load("Environment/Modelli_Fused_def1_RMD.RData")
#plot(Lasso_logistico_Fused)
# Visualizzo andamento Errore di classificazione per i vari lambda
```

Si vede poi in Figura 13.a quali sono i coefficienti selezionati dal modello, visualizzati nella posizione effettiva.
Nonostante sia di difficile interpretazione, possiamo comunque notare una maggiore densità di pixel nella parte inferiore e centrale dell'immagine.

Per quanto riguarda le etichette k-means, si utilizza una griglia di $log(\lambda)$ che va da -7 a -4, utilizzando come metrica di ottimizzazione il tasso di errata classificazione.
Viene specificato in *cv.glmnet* l'opzione *keep = T*, che consente di tenere la stima del predittore lineare per ogni $log(\lambda)$ testato per le osservazioni *out of fold*.
A questo punto è possibile riprodurre una convalida incrociata utilizzando il Kappa di Cohen.
Massimizzando quindi questo indice, si provvede a selezionare un $\lambda$ pari a 0.0099911, che determina la selezione di 634 variabili, contro le 69 che sarebbero state scelte utilizzando l'errore di classificazione, il quale avrebbe portato tuttavia a predire soltanto la classe più numerosa.
Si riporta in Figura 9 a fini esplicativi il grafico di Kappa di Cohen al variare di $\lambda$, confrontato con il relativo tasso di errata classificazione medio.

```{r Lasso Kmeans da mostrare, eval=FALSE}
Lasso_logistico_Kmeans <- cv.glmnet(X_train, factor(train$Etichette_Kmeans_K2),
                                    type.measure = "class", family = "binomial",
                                    alpha = 1, nfolds = 5, foldid = fold,
                                    lambda = lambda.grid.lasso, parallel = T,
                                    trace.it = 1, seed = 42, keep = T)

eta.lasso <- Lasso_logistico_Kmeans$fit.preval
pi.train.lasso <- exp(eta.lasso)/(1+exp(eta.lasso))
pred.train.class.lasso <- ifelse(pi.train.lasso > 1/2, 2, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals.lasso <- matrix(NA, ncol(pred.train.class.lasso), 5)
# Una metrica ragionevole per il caso presente di sbilanciamento è il Kappa di
# cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class.lasso)){
  for(j in 1:5){
    ind.fold.out <- which(Lasso_logistico_Kmeans$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class.lasso[ind.fold.out,i])) != 1){
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
      
      tab <- table(pred.train.class.lasso[ind.fold.out,i], 
                   train$Etichette_Kmeans_K2[ind.fold.out])
      kappa.vals.lasso[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
    }
  }
}
```

```{r Lasso Kmeans, eval=FALSE, include=FALSE}
# Lasso Logistico Kmeans -------------------------------------------------------------

lambda.grid.lasso <- exp(seq(-7,-4,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia è il risultato di prove precedenti che
# non vengono riportate

registerDoMC(cores = 20)
# Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
# che intendo utilizzare per parallelizzare il modello 

Lasso_logistico_Kmeans <- cv.glmnet(X_train,
                                    factor(train$Etichette_Kmeans_K2),
                                    type.measure = "class",
                                    family = "binomial",
                                    alpha = 1,
                                    nfolds = 5,
                                    foldid = fold,
                                    lambda = lambda.grid.lasso,
                                    parallel = T,
                                    trace.it = 1,
                                    seed = 42,
                                    keep = T)
# Viene stimato un modello logistico con penalita lasso, in CV a 5 fold con 
# metrica di ottimizzazione l'errore di errata classificazione,
# viene oltretutto fissato un seed per la riproducibilità dei risultati.
# Con l'etichetta presente però occorre fare una considerazione, ci troviamo 
# in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
# metrica come l'errore di errata classificazione potrebbe portare a conclusioni
# inestatte, per tanto viene impostato un keep = T all'interno del modello, 
# che permette di avere per ogni lambda la stima del predittore lineare delle
# unita statistiche che stanno nell'out of fold, cosi da riprodurre una 
# CV fatta a mano utilizzando i risultati ottenuti dalla funzione cv.glment

plot(Lasso_logistico_Kmeans)
# Visualizzo errore di previsione, si nota la scarsa efficacia della metrica
length(which(coef(Lasso_logistico_Kmeans, s = Lasso_logistico_Kmeans$lambda.min)!=0))
# [1] 48


eta.lasso <- Lasso_logistico_Kmeans$fit.preval
pi.train.lasso <- exp(eta.lasso)/(1+exp(eta.lasso))
pred.train.class.lasso <- ifelse(pi.train.lasso > 1/2, 2, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals.lasso <- matrix(NA, ncol(pred.train.class.lasso), 5)
# Una metrica ragionevole per il caso presente di sbilanciamento è il Kappa di
# cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class.lasso)){
  
  for(j in 1:5){
    ind.fold.out <- which(Lasso_logistico_Kmeans$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class.lasso[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class.lasso[ind.fold.out,i], 
                   train$Etichette_Kmeans_K2[ind.fold.out])
      kappa.vals.lasso[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

lambda.k.lasso <- Lasso_logistico_Kmeans$lambda[which.max(error.Lambda.lasso)]
# Definisco il lambda che massimizza Kappa


n.coef_Lasso_Kmeans <- length(which(coef(Lasso_logistico_Kmeans,s =  lambda.k.lasso)!=0))


y.hat.lasso_Kmeans <- predict(Lasso_logistico_Kmeans, 
                              s =  lambda.k.lasso, 
                              newx = X_test, 
                              type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.lasso_Kmeans  <- tabella.sommario(y.hat.lasso_Kmeans,
                                          test$Etichette_Kmeans_K2)
Metriche_Lasso_Kmeans <- indici.errore(Tabella.lasso_Kmeans)
# Salvo le varie metriche ottenute da tale modello 


```

```{r Codice grafici Lasso Kmeans, eval=FALSE, include=FALSE}
# Grafici
error.Lambda.lasso.miss <- rev(Lasso_logistico_Kmeans$cvm)
se.Lambda.lasso.miss <- rev(Lasso_logistico_Kmeans$cvsd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k.lasso <- se.Lambda.lasso.miss
`MissClassificationError` <-  error.Lambda.lasso.miss
`Log(Lambda)` <- log(lambda.grid.lasso)
data.plot.cv.lasso.miss <- tibble(`MissClassificationError`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 


srs.acc_Kmeans.lasso.miss <- ggplot(data.plot.cv.lasso.miss, 
                                    mapping = aes(x = `Log(Lambda)`, 
                                                  y = `MissClassificationError`)) +
  ylim(c(min(`MissClassificationError` - stderrcv.k.lasso)-0.005,
         max(`MissClassificationError` + stderrcv.k.lasso)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, 
                 y = `MissClassificationError` + stderrcv.k.lasso),
             shape = 95, size = 2, color="grey") +
  geom_point(aes(x = `Log(Lambda)`, 
                 y = `MissClassificationError` - stderrcv.k.lasso),
             shape = 95, size = 2, color="grey") +
  geom_segment(aes(x = `Log(Lambda)`,
                   y = `MissClassificationError` - stderrcv.k.lasso,
                   xend = `Log(Lambda)`, 
                   yend = `MissClassificationError` + stderrcv.k.lasso), color="grey") +
  geom_point(col = "firebrick1", size = 1, shape=20) +
  ggtitle("Misclassification Error CV Lasso - K-means Labels")+ theme(legend.position = "bottom", 
                                                     axis.text = element_text(size = 8),
                                                     axis.title = element_text(size = 8),
                                                     legend.title=element_text(size=10), 
                                                     legend.text=element_text(size=9))
srs.acc_Kmeans.lasso.miss


error.Lambda.lasso <- apply(kappa.vals.lasso,1,mean)
se.Lambda.lasso <- apply(kappa.vals.lasso,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k.lasso <- se.Lambda.lasso
`Kappa Cohen` <-  error.Lambda.lasso
`Log(Lambda)` <- log(Lasso_logistico_Kmeans$lambda)
data.plot.cv.lasso <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 


srs.acc_Kmeans.lasso <- ggplot(data.plot.cv.lasso, 
                               mapping = aes(x = `Log(Lambda)`, 
                                             y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k.lasso)-0.005,
         max(`Kappa Cohen` + stderrcv.k.lasso)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k.lasso),
             shape = 95, size = 2, color="grey") +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.lasso),
             shape = 95, size = 2, color="grey") +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.lasso,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k.lasso), color="grey") +
  geom_point(col = "firebrick1", size = 1, shape=20) +
  ggtitle("Kappa Cohen CV Lasso - K-means labels")+ theme(legend.position = "bottom", 
                                         axis.text = element_text(size = 8),
                                         axis.title = element_text(size = 8),
                                         legend.title=element_text(size=10), 
                                         legend.text=element_text(size=9))
srs.acc_Kmeans.lasso
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

```

```{r Plot Kappa Lasso Kmeans, echo=FALSE, out.height="60%", out.width="60%", fig.cap="Andamento dell'errore di classificazione medio (in alto) e del Kappa di Cohen (in basso) al variare di log-lambda nel Lasso Logistico con etichette k-means", fig.align="center", warning=FALSE}
#par(mfrow=c(1,2))
#load("Environment/Modelli_Kmeans_def1.RData")
#load("Environment/Env_Modelli.RData")
grid.arrange(srs.acc_Kmeans.lasso.miss + labs(
  title = "Errore di classificazione in CV ",
  y = "Errore di classificazione", x = "Log(lambda)"), 
  srs.acc_Kmeans.lasso + labs(
    title = "Kappa di Cohen Lasso in CV",
    y = "Kappa di Cohen", x = "Log(lambda)"), nrow=2, top="Lasso - Etichette K-means")
```

Anche per le etichette Kaggle si effettua la stessa procedura utilizzata per k-means.
Si seleziona in questo caso un $\lambda$ pari a 0.002130252, che determina la selezione di 2 476 variabili.

```{r eval=FALSE, include=FALSE}
#load("Environment/Modelli_Kaggle_def1.RData")
```

```{r Lasso Kaggle, eval=FALSE, include=FALSE}
# Lasso Logistico -------------------------------------------------------------

lambda.grid.lasso <- exp(seq(-7,-4,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia è il risultato di prove precedenti che
# non vengono riportate

registerDoMC(cores = 20)
# Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
# che intendo utilizzare per parallelizzare il modello 

Lasso_logistico_Kaggle <- cv.glmnet(X_train,
                                    factor(train$Etichette_Kaggle_K2),
                                    type.measure = "class",
                                    family = "binomial",
                                    alpha = 1,
                                    nfolds = 5,
                                    foldid = fold,
                                    lambda = lambda.grid.lasso,
                                    parallel = T,
                                    trace.it = 1,
                                    seed = 42,
                                    keep = T)

plot(Lasso_logistico_Kaggle)
# Visualizzo errore di previsione, si nota la scarsa efficacia della metrica

eta.lasso <- Lasso_logistico_Kaggle$fit.preval
pi.train.lasso <- exp(eta.lasso)/(1+exp(eta.lasso))
pred.train.class.lasso <- ifelse(pi.train.lasso > 1/2, 2, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals.lasso <- matrix(NA, ncol(pred.train.class.lasso), 5)
# Una metrica ragionevole per il caso presente di sbilanciamento è il Kappa di
# cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class.lasso)){
  
  for(j in 1:5){
    ind.fold.out <- which(Lasso_logistico_Kaggle$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class.lasso[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class.lasso[ind.fold.out,i], 
                   train$Etichette_Kaggle_K2[ind.fold.out])
      kappa.vals.lasso[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda.lasso <- apply(kappa.vals.lasso,1,mean)
se.Lambda.lasso <- apply(kappa.vals.lasso,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k.lasso <- se.Lambda.lasso
`Kappa Cohen` <-  error.Lambda.lasso
`Log(Lambda)` <- log(Lasso_logistico_Kaggle$lambda)
data.plot.cv.lasso <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kaggle.lasso <- ggplot(data.plot.cv.lasso, 
                               mapping = aes(x = `Log(Lambda)`, 
                                             y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k.lasso)-0.005,
         max(`Kappa Cohen` + stderrcv.k.lasso)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k.lasso),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.lasso),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.lasso,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k.lasso)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen CV Lasso")
srs.acc_Kaggle.lasso
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

lambda.k.lasso <- Lasso_logistico_Kaggle$lambda[which.max(error.Lambda.lasso)]
# Definisco il lambda che massimizza Kappa


n.coef_Lasso_Kaggle <- length(which(coef(Lasso_logistico_Kaggle, s = lambda.k.lasso)!=0))


y.hat.lasso_Kaggle <- predict(Lasso_logistico_Kaggle, 
                              s =  lambda.k.lasso, 
                              newx = X_test, 
                              type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.lasso_Kaggle  <- tabella.sommario(y.hat.lasso_Kaggle,
                                          test$Etichette_Kaggle_K2)
Metriche_Lasso_Kaggle <- indici.errore(Tabella.lasso_Kaggle)
# Salvo le varie metriche ottenute da tale modello 


```

## 4.2 Elastic Net

Si propone poi un modello *Elastic Net* logistico, che può essere utile in questo caso per tenere conto della collinearità che ci si aspetta essere presente tra i pixel.
Questo modello infatti ci consentirà di selezionare più coefficienti rispetto al modello precedente grazie all'utilizzo anche della penalità *ridge* (norma L2).

Quindi, oltre alla regolarizzazione di $\lambda$, verrà selezionato tramite convalida incrociata anche il parametro $\alpha$, che pesa le due penalità.

Nel caso delle etichette *fused lasso*, la prima regolarizzazione porta ad un $\alpha$ pari a 0.3, dando quindi un peso alla penalità L1 pari a 0.3, contro un peso alla penalità ridge di 0.7.
L'errore di classificazione medio al variare di $\alpha$ è visibile in Figura 10.

Il $\lambda$ selezionato invece è pari a 0.06062607 con un numero di coefficienti selezionati che ammonta a 116 compresa l'intercetta, cosa che conferma quanto detto in precedenza.

```{r Elastic net Logistico Fused Lasso, eval=FALSE}
# Elastic net Logistico Fused Lasso -------------------------------------------------------
lambda.grid <- exp(seq(-5,-2.5,l=100))
alpha.grid.enet <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha.enet <- rep(NA, length(alpha.grid))
se.alpha.enet <- rep(NA, length(alpha.grid))

for(i in 1:length(alpha.grid.enet)){
  registerDoMC(cores = 5)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train, factor(train$Etichette_Fused_K2), alpha = alpha.grid.enet[i],
                   type.measure = "class", family = "binomial", nfolds = 5,
                   foldid = fold, lambda = lambda.grid, parallel = T,
                   trace.it = 1, seed = 42)
  # Viene stimato un modello logistico con penalita Elastic-Net, in CV a 5 fold con 
  # metrica di ottimizzazione l'errore di errata classificazione, per ogni valore di alpha
  # contenuto in alpha.grid, viene oltretutto fissato un seed per la riproducibilità dei risultati
  error.alpha.enet[i] <- fit$cvm[fit$index["min",]]
  se.alpha.enet[i] <- fit$cvsd[fit$index["min",]]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}
```

```{r eval=FALSE, include=FALSE}
stderrcv.k.enet <- se.alpha.enet
`MissClassificationError` <-  error.alpha.enet
`Alpha` <- alpha.grid.enet
data.plot.cv.enet <- tibble(`MissClassificationError`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_ENet_Fused <- ggplot(data.plot.cv.enet, 
                        mapping = aes(x = `Alpha`, 
                                      y = `MissClassificationError`)) +
  ylim(c(min(`MissClassificationError` - stderrcv.k.enet)-0.005,
         max(`MissClassificationError` + stderrcv.k.enet)+0.005))+
  geom_point(aes(x = `Alpha`, y = `MissClassificationError` + stderrcv.k.enet),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `MissClassificationError` - stderrcv.k.enet),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `MissClassificationError` - stderrcv.k.enet,
                   xend = `Alpha`, 
                   yend = `MissClassificationError` + stderrcv.k.enet)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("MissClassificationError  CV")
srs.acc_ENet_Fused
# Visualizzazione dell'errore di classificazione per i diversi alpha 

error.alpha.enet[which.min(error.alpha.enet)]
# Minimo dell'errore di classificazione al variare dell'alpha

best.alpha_Fused <- alpha.grid.enet[which.min(error.alpha.enet)]
best.alpha_Fused

# Viene definito l'alpha per il quale si ottiene l'errore di classificazione 
# minore

E.Net_logistico_Fused <- cv.glmnet(X_train,
                                   factor(train$Etichette_Fused_K2),
                                   type.measure = "class",
                                   family = "binomial",
                                   alpha = best.alpha_Fused,
                                   nfolds = 5,
                                   foldid = fold,
                                   lambda = lambda.grid,
                                   parallel = T,
                                   trace.it = 1,
                                   seed = 42)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Fused, in CV a 5 fold con metrica di ottimizzazione l'errore di 
# errata classificazione, per ogni valore di alpha contenuto in alpha.grid,
# viene oltretutto fissato un seed per la riproducibilità dei risultati

plot(E.Net_logistico_Fused)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net


n.coef_E.Net_logistico_Fused <- length(which(coef(E.Net_logistico_Fused, s=E.Net_logistico_Fused$lambda.min)!=0))
# Numero di coefficienti diversi da 0 con lambda.min

y.hat.E.Net_Fused <- predict(E.Net_logistico_Fused, 
                             s = E.Net_logistico_Fused$lambda.min, 
                             newx = X_test, 
                             type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.E.Net_Fused <- tabella.sommario(test$Etichette_Fused_K2 , 
                                        y.hat.E.Net_Fused)
Metriche_E.Net_Fused <- indici.errore(Tabella.E.Net_Fused)
# Salvo le varie metriche ottenute da tale modello 

```

```{r Grafico Alpha EN Fused Lasso, echo=FALSE, fig.align="center", fig.cap="Andamento del tasso di errata classificazione medio al variare di alpha per le etichette fused lass, dato il miglior lambda", out.height='50%', out.width='50%'}
srs.acc_ENet_Fused + labs(
  title = "Errore di classificazione Lasso in CV (etichette Fused Lasso)",
  y = "Errore di classificazione", x = "Alpha")


#srs.acc_ENet_Fused
# Visualizzazione dell'errore di classificazione per i diversi alpha 
```

Per quanto riguarda _k-means_, si adatta lo stesso procedimento precedente, valutando però l'indice Kappa di Cohen.
Si adattano i modelli su una griglia di $log(\lambda)$ che va da -7 a -4 e si valutano 9 valori di $\alpha$ equispaziati da 0.1 a 0.9, scegliendo un valore pari a 0.7.
Si seleziona un valore di $\lambda$ pari a 0.01273201 corrispondente a 873 coefficienti diversi da 0.

```{r Elastic Net Kmeans, eval=FALSE, include=FALSE}
# Elastic net Logistico -------------------------------------------------------
lambda.grid.enet <- exp(seq(-7,-4,l=100))
# Come in precedenza si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid.enet <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha.enet <- rep(NA, length(alpha.grid.enet))
se.alpha.enet <- rep(NA, length(alpha.grid.enet))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard

for(alpha in 1:length(alpha.grid.enet)){
  
  registerDoMC(cores = 20)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train,
                   factor(train$Etichette_Kmeans_K2),
                   alpha = alpha.grid.enet[alpha],
                   type.measure = "class",
                   family = "binomial",
                   nfolds = 5,
                   foldid = fold,
                   lambda = lambda.grid.enet,
                   parallel = T,
                   trace.it = 1,
                   seed = 42,
                   keep = T)
  # Viene stimato un modello logistico con penalita Elastic-Net, in CV a 5 fold
  # con metrica di ottimizzazione l'errore di errata classificazione,
  # viene oltretutto fissato un seed per la riproducibilità dei risultati.
  # Con l'etichetta presente però occorre fare una considerazione, ci troviamo 
  # in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
  # metrica come l'errore di errata classificazione potrebbe portare a 
  # conclusioni inestatte, per tanto viene impostato un keep = T all'interno del
  # modello, che permette di avere per ogni lambda la stima del predittore 
  # lineare delle unita statistiche che stanno nell'out of fold, cosi da
  # riprodurre una CV fatta a mano utilizzando i risultati ottenuti dalla 
  # funzione cv.glment
  
  eta.enet <- fit$fit.preval
  pi.train.enet <- exp(eta.enet)/(1+exp(eta.enet))
  pred.train.class.enet <- ifelse(pi.train.enet > 1/2, 2, 1)
  # Mi riconduco dal predittore linare alle classi stimate 
  
  kappa.vals.enet <- matrix(NA, ncol(pred.train.class.enet), 5)
  # Una metrica ragionevole per il caso presente di sbilanciamento è il Kappa di
  # cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
  # numero di lambda, e numero di colonne pari ai fold utilizzati
  
  for( i in 1:ncol(pred.train.class.enet)){
    
    for(j in 1:5){
      ind.fold.out <- which(fit$foldid == j)
      # Definisco il fold corrente
      
      if (length(unique(pred.train.class.enet[ind.fold.out,i])) != 1)
        # Condizione per la quale si valuta la metrica relativa a quel fold, di
        # un lambda se e solo se le classi stimate non hanno solo un livello
      {
        
        tab <- table(pred.train.class.enet[ind.fold.out,i], 
                     train$Etichette_Kmeans_K2[ind.fold.out])
        kappa.vals.enet[i,j] <- Kappa(tab)$Unweighted[1]
        # Calcolo Kappa e salvo il risultato per il j-esimo fold
        
      }
    }
  }
  
  error.Lambda.enet <- apply(kappa.vals.enet,1,mean)
  se.Lambda.enet <- apply(kappa.vals.enet,1,sd)
  # Calcolo Kappa medio per ogni fold e relativa deviazione standard
  
  error.alpha.enet[alpha] <- error.Lambda.enet[which.max(error.Lambda.enet)]
  se.alpha.enet[alpha] <- se.Lambda.enet[which.max(error.Lambda.enet)]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

stderrcv.k.enet.alpha <- se.alpha.enet
`Kappa Cohen` <-  error.alpha.enet
`Alpha` <- alpha.grid.enet
data.plot.cv.enet.alpha <- tibble(`Kappa Cohen`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Kmeans.alpha.enet <- ggplot(data.plot.cv.enet.alpha, 
                               mapping = aes(x = `Alpha`, 
                                             y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k.enet.alpha)-0.005,
         max(`Kappa Cohen` + stderrcv.k.enet.alpha)+0.005))+
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` + stderrcv.k.enet.alpha),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k.enet.alpha),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k.enet.alpha,
                   xend = `Alpha`, 
                   yend = `Kappa Cohen` + stderrcv.k.enet.alpha)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen CV Elastic Net")
srs.acc_Kmeans.alpha.enet
# Visualizzazione di Kappa per i diversi alpha 

error.alpha.enet[which.max(error.alpha.enet)]
# Massimo di Kappa al variare dell'alpha

best.alpha_Kmeans.enet <- alpha.grid.enet[which.max(error.alpha.enet)]
best.alpha_Kmeans.enet
# Viene definito l'alpha per il quale si ottiene il Kappa maggiore

E.Net_logistico_Kmeans <- cv.glmnet(X_train,
                                    factor(train$Etichette_Kmeans_K2),
                                    type.measure = "class",
                                    family = "binomial",
                                    alpha = best.alpha_Kmeans.enet,
                                    nfolds = 5,
                                    foldid = fold,
                                    lambda = lambda.grid.enet,
                                    parallel = T,
                                    trace.it = 1,
                                    seed = 42,
                                    keep = T)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Kmeans

plot(E.Net_logistico_Kmeans)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net

eta.enet.fit <- E.Net_logistico_Kmeans$fit.preval
pi.train.enet.fit <- exp(eta.enet.fit)/(1+exp(eta.enet.fit))
pred.train.class.enet.fit <- ifelse(pi.train.enet.fit > 1/2, 2, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals.enet.fit <- matrix(NA, ncol(pred.train.class.enet.fit), 5)
# Inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class.enet.fit)){
  
  for(j in 1:5){
    ind.fold.out <- which(E.Net_logistico_Kmeans$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class.enet.fit[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class.enet.fit[ind.fold.out,i], 
                   train$Etichette_Kmeans_K2[ind.fold.out])
      kappa.vals.enet.fit[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda.enet <- apply(kappa.vals.enet.fit,1,mean)
se.Lambda.enet <- apply(kappa.vals.enet.fit,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k.enet.lambda <- se.Lambda.enet
`Kappa Cohen` <-  error.Lambda.enet
`Log(Lambda)` <- log(E.Net_logistico_Kmeans$lambda)
data.plot.cv.enet.lambda <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans.Enet.Lambda <- ggplot(data.plot.cv.enet.lambda, 
                              mapping = aes(x = `Log(Lambda)`, 
                                            y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k.enet.lambda)-0.005,
         max(`Kappa Cohen` + stderrcv.k.enet.lambda)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k.enet.lambda),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.enet.lambda),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.enet.lambda,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k.enet.lambda)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen CV Elastic Net - Lambda")
srs.acc_Kmeans.Enet.Lambda
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

lambda.k.enet <- E.Net_logistico_Kmeans$lambda[which.max(error.Lambda.enet)]
# Definisco il lambda che massimizza Kappa


n.coef_ENet_Kmeans <- length(which(coef(E.Net_logistico_Kmeans,s =  lambda.k.enet)!=0))


y.hat.E.Net_Kmeans <- predict(E.Net_logistico_Kmeans,
                              s = lambda.k.enet,
                              newx = X_test,
                              type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.E.Net_Kmeans <- tabella.sommario(y.hat.E.Net_Kmeans,test$Etichette_Kmeans_K2)
Metriche_E.Net_Kmeans <- indici.errore(Tabella.E.Net_Kmeans)
# Salvo le varie metriche ottenute da tale modello 


```

Stessa procedura viene adottata anche per _Kaggle_, sempre utilizzando come metrica per la cross validation il Kappa di Cohen con il procedimento già esplicitato in precedenza.
Si sceglie un $\alpha$ pari a 0.9 e un $\lambda$ pari a 0.009403563.
I coefficienti selezionati sono 738.

```{r Elastic Net Kaggle, eval=FALSE, include=FALSE}
# Elastic net Logistico -------------------------------------------------------

lambda.grid.enet <- exp(seq(-7,-4,l=100))
# Come in precedenza si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid.enet <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha.enet <- rep(NA, length(alpha.grid.enet))
se.alpha.enet <- rep(NA, length(alpha.grid.enet))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard

for(alpha in 1:length(alpha.grid.enet)){
  
  registerDoMC(cores = 20)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train,
                   factor(train$Etichette_Kaggle_K2),
                   alpha = alpha.grid.enet[alpha],
                   type.measure = "class",
                   family = "binomial",
                   nfolds = 5,
                   foldid = fold,
                   lambda = lambda.grid.enet,
                   parallel = T,
                   trace.it = 1,
                   seed = 42,
                   keep = T)
  # Viene stimato un modello logistico con penalita Elastic-Net, in CV a 5 fold
  # con metrica di ottimizzazione l'errore di errata classificazione,
  # viene oltretutto fissato un seed per la riproducibilità dei risultati.
  # Con l'etichetta presente però occorre fare una considerazione, ci troviamo 
  # in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
  # metrica come l'errore di errata classificazione potrebbe portare a 
  # conclusioni inestatte, per tanto viene impostato un keep = T all'interno del
  # modello, che permette di avere per ogni lambda la stima del predittore 
  # lineare delle unita statistiche che stanno nell'out of fold, cosi da
  # riprodurre una CV fatta a mano utilizzando i risultati ottenuti dalla 
  # funzione cv.glment
  
  eta.enet <- fit$fit.preval
  pi.train.enet <- exp(eta.enet)/(1+exp(eta.enet))
  pred.train.class.enet <- ifelse(pi.train.enet > 1/2, 0, 1)
  # Mi riconduco dal predittore linare alle classi stimate 
  
  kappa.vals.enet.alpha <- matrix(NA, ncol(pred.train.class.enet), 5)
  # Una metrica ragionevole per il caso presente di sbilanciamento è il Kappa di
  # cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
  # numero di lambda, e numero di colonne pari ai fold utilizzati
  
  for( i in 1:ncol(pred.train.class.enet)){
    
    for(j in 1:5){
      ind.fold.out <- which(fit$foldid == j)
      # Definisco il fold corrente
      
      if (length(unique(pred.train.class.enet[ind.fold.out,i])) != 1)
        # Condizione per la quale si valuta la metrica relativa a quel fold, di
        # un lambda se e solo se le classi stimate non hanno solo un livello
      {
        
        tab <- table(pred.train.class.enet[ind.fold.out,i], 
                     train$Etichette_Kaggle_K2[ind.fold.out])
        kappa.vals.enet.alpha[i,j] <- Kappa(tab)$Unweighted[1]
        # Calcolo Kappa e salvo il risultato per il j-esimo fold
        
      }
    }
  }
  
  error.Lambda.enet <- apply(kappa.vals.enet,1,mean)
  se.Lambda.enet <- apply(kappa.vals.enet,1,sd)
  # Calcolo Kappa medio per ogni fold e relativa deviazione standard
  
  error.alpha.enet[alpha] <- error.Lambda.enet[which.max(error.Lambda.enet)]
  se.alpha.enet[alpha] <- se.Lambda.enet[which.max(error.Lambda.enet)]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

stderrcv.k.enet.alpha <- se.alpha.enet
`Kappa Cohen` <-  error.alpha.enet
`Alpha` <- alpha.grid.enet
data.plot.cv.enet.alpha <- tibble(`Kappa Cohen`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Kaggle.alpha.enet <- ggplot(data.plot.cv.enet.alpha, 
                               mapping = aes(x = `Alpha`, 
                                             y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k.enet.alpha)-0.005,
         max(`Kappa Cohen` + stderrcv.k.enet.alpha)+0.005))+
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` + stderrcv.k.enet.alpha),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k.enet.alpha),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k.enet.alpha,
                   xend = `Alpha`, 
                   yend = `Kappa Cohen` + stderrcv.k.enet.alpha)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen CV Alpha - Elastic Net")
srs.acc_Kaggle.alpha.enet
# Visualizzazione di Kappa per i diversi alpha 

error.alpha.enet[which.max(error.alpha.enet)]
# Massimo di Kappa al variare dell'alpha

best.alpha_Kaggle.enet <- alpha.grid.enet[which.max(error.alpha.enet)]
best.alpha_Kaggle.enet
# Viene definito l'alpha per il quale si ottiene il Kappa maggiore

E.Net_logistico_Kaggle <- cv.glmnet(X_train,
                                    factor(train$Etichette_Kaggle_K2),
                                    type.measure = "class",
                                    family = "binomial",
                                    alpha = best.alpha_Kaggle.enet,
                                    nfolds = 5,
                                    foldid = fold,
                                    lambda = lambda.grid.enet,
                                    parallel = T,
                                    trace.it = 1,
                                    seed = 42,
                                    keep = T)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Kaggle

plot(E.Net_logistico_Kaggle.enet)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net

eta.enet <- E.Net_logistico_Kaggle$fit.preval
pi.train.enet <- exp(eta.enet)/(1+exp(eta.enet))
pred.train.class.enet.l <- ifelse(pi.train.enet > 1/2, 0, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals.enet.lambda <- matrix(NA, ncol(pred.train.class.enet), 5)
# Inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class.enet.l)){
  
  for(j in 1:5){
    ind.fold.out <- which(E.Net_logistico_Kaggle$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class.enet.l[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class.enet.l[ind.fold.out,i], 
                   train$Etichette_Kaggle_K2[ind.fold.out])
      kappa.vals.enet.lambda[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda.enet <- apply(kappa.vals.enet.lambda,1,mean)
se.Lambda.enet <- apply(kappa.vals.enet.lambda,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k.enet.lambda <- se.Lambda.enet
`Kappa Cohen` <-  error.Lambda.enet
`Log(Lambda)` <- log(E.Net_logistico_Kaggle$lambda)
data.plot.cv.enet.lambda <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kaggle.Enet.lambda <- ggplot(data.plot.cv.enet.lambda, 
                              mapping = aes(x = `Log(Lambda)`, 
                                            y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k.enet.lambda)-0.005,
         max(`Kappa Cohen` + stderrcv.k.enet.lambda)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k.enet.lambda),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.enet.lambda),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.enet.lambda,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k.enet.lambda)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kaggle.Enet.lambda
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

lambda.k.enet <- E.Net_logistico_Kaggle$lambda[which.max(error.Lambda.enet)]
# Definisco il lambda che massimizza Kappa


n.coef_E.Net_Kaggle <- length(which(coef(E.Net_logistico_Kaggle,s =  lambda.k.enet)!=0))

y.hat.E.Net_Kaggle <- predict(E.Net_logistico_Kaggle,
                              s = lambda.k.enet,
                              newx = X_test,
                              type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.E.Net_Kaggle <- tabella.sommario(test$Etichette_Kaggle_K2 ,
                                         y.hat.E.Net_Kaggle)
Metriche_E.Net_Kaggle <- indici.errore(Tabella.E.Net_Kaggle)
# Salvo le varie metriche ottenute da tale modello
```

## 4.3 SCAD E MCP

Si propongono poi due modelli con penalità non convessa, al fine di ridurre la distorsione che caratterizza il lasso.
Si decide di non implementare una procedura di lasso adattivo, in quanto essa, svolgendosi in due passi e, volendo definire un peso per ogni coefficiente (ad esempio con una regressione ridge), sarebbe stato troppo oneroso per la nostra applicazione.
In entrambi i casi ci si aspetta che vengano selezionati meno coefficienti rispetto ai modelli precedenti.
Partendo sempre dalle etichette fused, sia per SCAD che per MCP, si utilizzano i valori di $\gamma$ proposti di default dalla libreria *ncvreg*, pari rispettivamente a 3.7 e 3.

Per quanto riguarda SCAD, il $\gamma$ scelto rende la regione non convessa molto ampia.
Il $\lambda$ selezionato dalla procedura di cross validation, pari a 0.01981706, determina un numero di variabili diverse da 0 pari a 31.
Il valore di $\lambda$ più piccolo per cui la regione è convessa risulta essere pari a 0.0525133; ne consegue che, come visibile in Figura 11.a, il modello stimato ricade nella regione di non convessità. 
Ne consegue che il risultato non è affidabile, in quanto si potrebbe essere incorsi in un minimo locale.

Per quanto riguarda invece MCP, il $\gamma$ scelto fa sì che il $\lambda$ ottimo ricada all’interno della regione localmente
convessa, rendendo il risultato affidabile. Vengono selezionate infatti 27 variabili diverse da 0, con un limite
che invece questa volte si trova in corrispondenza di un numero di variabili pari a 99.
Ciò è visibile nella Figura 11.b.

```{r SCAD Fused, eval=FALSE, include=FALSE}
# SCAD --------------------------------------------------------------------

lambda.grid <- exp(seq(-3.8,-4.2,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia è il risultato di prove precedenti che
# non vengono riportate

SCAD_logistico_Fused <- cv.ncvreg(X_train,
                                  factor(train$Etichette_Fused_K2),
                                  family = "binomial",
                                  penalty = "SCAD",
                                  nfolds = 5,
                                  fold = fold,
                                  trace = T,
                                  lambda = lambda.grid,
                                  seed = 42)
# Viene stimato un modello logistico con penalita SCAD, in CV a 5 fold con 
# metrica di ottimizzazione la verosimiglianza negativa binomiale,
# viene oltretutto fissato un seed per la riproducibilità dei risultati



n.coef_SCAD_Fused <- SCAD_logistico_Fused$min
# Numero di coefficienti diversi da 0

SCAD_logistico_Fused$fit$convex.min
# Limite di variabili tenute prima che la funzione diventi non convessa

plot(SCAD_logistico_Fused)
# Visualizzo andamento della verosimiglianza negativa per i vari lambda


SCAD_logistico_Fused.fit <- ncvreg(X_train,
                                  factor(train$Etichette_Fused_K2),
                                  family = "binomial",
                                  penalty = "SCAD",
                                  #lambda = lambda.grid, 
                                  trace=T)

plot(SCAD_logistico_Fused.fit)

y.hat.SCAD_Fused <- predict(SCAD_logistico_Fused, 
                            X_test, 
                            s = SCAD_logistico_Fused$lambda.min, 
                            type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.SCAD_Fused <- tabella.sommario(test$Etichette_Fused_K2 ,
                                       y.hat.SCAD_Fused)
Metriche_SCAD_Fused <- indici.errore(Tabella.SCAD_Fused)
# Salvo le varie metriche ottenute da tale modello 

```

```{r MCP Fused, eval=FALSE, include=FALSE}
# MCP ---------------------------------------------------------------------

lambda.grid <- exp(seq(-3.7,-4,l=100))

MCP_logistico_Fused <- cv.ncvreg(X_train,
                                 factor(train$Etichette_Fused_K2),
                                 family = "binomial",
                                 penalty = "MCP",
                                 nfolds = 5,
                                 fold = fold,
                                 trace = T,
                                 lambda = lambda.grid,
                                 seed = 42)
# Viene stimato un modello logistico con penalita SCAD, in CV a 5 fold con 
# metrica di ottimizzazione la verosimiglianza negativa binomiale,
# viene oltretutto fissato un seed per la riproducibilità dei risultati


n.coef_MCP_Fused <- MCP_logistico_Fused$min
# Numero di coefficienti diversi da 0

MCP_logistico_Fused$fit$convex.min
# Limite di variabili tenute prima che la funzione diventi non convessa


plot(MCP_logistico_Fused)
# Visualizzo andamento della verosimiglianza negativa per i vari lambda


MCP_logistico_Fused.fit <- ncvreg(X_train,
                                  factor(train$Etichette_Fused_K2),
                                  family = "binomial",
                                  penalty = "MCP",
                                  #lambda = lambda.grid, 
                                  trace=T)
plot(MCP_logistico_Fused.fit)

y.hat.MCP_Fused <- predict(MCP_logistico_Fused, 
                           X_test, 
                           s = MCP_logistico_Fused$lambda.min, 
                           type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.MCP_Fused <- tabella.sommario(test$Etichette_Fused_K2 ,
                                      y.hat.MCP_Fused)
Metriche_MCP_Fused <- indici.errore(Tabella.MCP_Fused)
# Salvo le varie metriche ottenute da tale modello 

```

```{r Confronto coefficienti SCAD MCP, fig.cap="Andamento dei coefficienti di SCAD (grafico a sinistra (a.)) e MCP (grafico a destra (b.)) al variare di lambda per il fused lasso", fig.align="center", echo=FALSE, out.height='60%', out.width='60%'}
#load("Environment/Modelli_Fused_def1.RData")
library(ncvreg)
par(mfrow=c(1,2))
plot(SCAD_logistico_Fused.fit)
abline(v=SCAD_logistico_Fused$lambda.min)
plot(MCP_logistico_Fused.fit)
abline(v=MCP_logistico_Fused$lambda.min)
```

Per quanto riguarda invece k-means, si utilizza un valore di $\gamma$ elevato, pari a 20, per permettere di selezionare modelli con più variabili ed evitare di incorrere nella previsione di una sola classe.
Per quanto riguarda MCP, si seleziona un $\lambda$ pari a 0.007454109, corrispondente ad un numero di variabili pari a 83.
Il $\lambda$ più piccolo per cui la funzione risulta convessa è pari a 0.00922615, contro un $\lambda$ minimo selezionato dalla cross validation pari a 0.007454109, che cade quindi nella regione non convessa e non rende i risultati affidabili.
La stessa cosa viene effettuata anche per SCAD.
In questo caso si seleziona un valore di $\lambda$ pari a 0.005617783, che consente di selezionare 89 variabili; anche in questo caso, il modello corrispondente cade nella regione di non convessità.

```{r SCAD Kmeans, eval=FALSE, include=FALSE}
# SCAD --------------------------------------------------------------------
lambda.grid.scad <- exp(seq(-3,-7,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia è il risultato di prove precedenti che
# non vengono riportate

SCAD_logistico_Kmeans <- cv.ncvreg(X_train,
                                   factor(train$Etichette_Kmeans_K2),
                                   family = "binomial",
                                   penalty = "SCAD",
                                   nfolds = 5,
                                   fold = fold,
                                   trace = T,
                                   lambda = lambda.grid.scad,
                                   gamma = 20,
                                   seed = 42,
                                   returnY = T)
# Viene stimato un modello logistico con penalita SCAD, in CV a 5 fold con 
# metrica di ottimizzazione la verosimiglianza negativa binomiale,
# viene oltretutto fissato un seed per la riproducibilità dei risultati.
# Con l'etichetta presente però occorre fare una considerazione, ci troviamo 
# in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
# metrica come l'errore di errata classificazione potrebbe portare a conclusioni
# inestatte, per tanto viene impostato un returnY = T all'interno del modello, 
# che permette di avere per ogni lambda la stima del predittore lineare delle
# unita statistiche che stanno nell'out of fold, cosi da riprodurre una 
# CV fatta a mano utilizzando i risultati ottenuti dalla funzione cv.ncvreg

plot(SCAD_logistico_Kmeans)
# Visualizzo andamento della verosimiglianza negativa per i vari lambda

eta.scad <- SCAD_logistico_Kmeans$Y
pi.train.scad <- exp(eta.scad)/(1+exp(eta.scad))
pred.train.class.scad <- ifelse(pi.train.scad > 0.7, 2, 1)
# Mi riconduco dal predittore linare alle classi stimate, nel caso presente
# utilizzo come soglia di classificazione la proporzione campionaria delle 
# classi nell'insime di stima

kappa.vals.scad <- matrix(NA, ncol(pred.train.class.scad), 5)
# Inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class.scad)){
  
  for(j in 1:5){
    ind.fold.out <- which(SCAD_logistico_Kmeans$fold == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class.scad[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class.scad[ind.fold.out,i], 
                   train$Etichette_Kmeans_K2[ind.fold.out])
      kappa.vals.scad[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda.scad <- apply(kappa.vals.scad,1,mean)
se.Lambda.scad <- apply(kappa.vals.scad,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k.scad <- se.Lambda.scad
`Kappa Cohen` <-  error.Lambda.scad
`Log(Lambda)` <- log(SCAD_logistico_Kmeans$lambda)
data.plot.cv.scad <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans.SCAD <- ggplot(data.plot.cv.scad, 
                              mapping = aes(x = `Log(Lambda)`, 
                                            y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k.scad)-0.005,
         max(`Kappa Cohen` + stderrcv.k.scad)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k.scad),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.scad),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.scad,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k.scad)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen CV - SCAD")
srs.acc_Kmeans.SCAD
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

lambda.k.scad <- SCAD_logistico_Kmeans$lambda[which.max(error.Lambda.scad)]
# Definisco il lambda che massimizza Kappa

n.coef_SCAD_Kmeans <- length(which(coef(SCAD_logistico_Kmeans, s =  lambda.k.scad)!=0))

SCAD_logistico_Kmeans.fit <- ncvreg(X_train,
                                   factor(train$Etichette_Kmeans_K2),
                                   family = "binomial",
                                   penalty = "SCAD",
                                   trace = T,
                                   gamma = 20)


plot(SCAD_logistico_Kmeans.fit)
# Grafico dei beta selezionati al variare di lambda


SCAD_logistico_Kmeans.fit$convex.min




y.hat.SCAD_Kmeans <- predict(SCAD_logistico_Kmeans, 
                             X_test, 
                             s = lambda.k.scad, 
                             type = "response")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la 
# probabilità delle classi 

y.hat.SCAD_Kmeans <- ifelse(y.hat.SCAD_Kmeans > 0.7, 2, 1)
# Utilizzo come soglia la proporzione campionaria dei livelli nell'insieme di
# stima

Tabella.SCAD_Kmeans <- tabella.sommario(y.hat.SCAD_Kmeans, test$Etichette_Kmeans_K2)
Metriche_SCAD_Kmeans <- indici.errore(Tabella.SCAD_Kmeans)
# Salvo le varie metriche ottenute da tale modello 

```

```{r MCP Kmeans, eval=FALSE, include=FALSE}
# MCP ---------------------------------------------------------------------

lambda.grid.mcp <- exp(seq(-3,-7,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia è il risultato di prove precedenti che
# non vengono riportate

MCP_logistico_Kmeans <- cv.ncvreg(X_train,
                                  factor(train$Etichette_Kmeans_K2),
                                  family = "binomial",
                                  penalty = "MCP",
                                  nfolds = 5,
                                  fold = fold,
                                  trace = T,
                                  lambda = lambda.grid.mcp,
                                  gamma = 20,
                                  seed = 42,
                                  returnY = T)
# Viene stimato un modello logistico con penalita MCP, in CV a 5 fold con 
# metrica di ottimizzazione la verosimiglianza negativa binomiale,
# viene oltretutto fissato un seed per la riproducibilità dei risultati.
# Con l'etichetta presente però occorre fare una considerazione, ci troviamo 
# in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
# metrica come l'errore di errata classificazione potrebbe portare a conclusioni
# inestatte, per tanto viene impostato un returnY = T all'interno del modello, 
# che permette di avere per ogni lambda la stima del predittore lineare delle
# unita statistiche che stanno nell'out of fold, cosi da riprodurre una 
# CV fatta a mano utilizzando i risultati ottenuti dalla funzione cv.ncvreg

plot(MCP_logistico_Kmeans)
# Visualizzo andamento della verosimiglianza negativa per i vari lambda

eta.mcp <- MCP_logistico_Kmeans$Y
pi.train.mcp <- exp(eta.mcp)/(1+exp(eta.mcp))
pred.train.class.mcp <- ifelse(pi.train.mcp > 0.7, 2, 1)
# Mi riconduco dal predittore linare alle classi stimate, nel caso presente
# utilizzo come soglia di classificazione la proporzione campionaria delle 
# classi nell'insime di stima

kappa.vals.mcp <- matrix(NA, ncol(pred.train.class.mcp), 5)
# Inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class.mcp)){
  
  for(j in 1:5){
    ind.fold.out <- which(MCP_logistico_Kmeans$fold == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class.mcp[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class.mcp[ind.fold.out,i], 
                   train$Etichette_Kmeans_K2[ind.fold.out])
      kappa.vals.mcp[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda.mcp <- apply(kappa.vals.mcp,1,mean)
se.Lambda.mcp <- apply(kappa.vals.mcp,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k.mcp <- se.Lambda.mcp
`Kappa Cohen` <-  error.Lambda.mcp
`Log(Lambda)` <- log(MCP_logistico_Kmeans$lambda)
data.plot.cv.mcp <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans.MCP <- ggplot(data.plot.cv.mcp, 
                             mapping = aes(x = `Log(Lambda)`, 
                                           y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k.mcp)-0.005,
         max(`Kappa Cohen` + stderrcv.k.mcp)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k.mcp),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.mcp),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k.mcp,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k.mcp)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen CV MCP")
srs.acc_Kmeans.MCP
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

lambda.k.mcp <- MCP_logistico_Kmeans$lambda[which.max(error.Lambda.mcp)]
# Definisco il lambda che massimizza Kappa

n.coef_MCP_Kmeans <- length(which(coef(MCP_logistico_Kmeans, s =  lambda.k.mcp)!=0))



MCP_logistico_Kmeans.fit <- ncvreg(X_train,
                                    factor(train$Etichette_Kmeans_K2),
                                    family = "binomial",
                                    penalty = "MCP",
                                    trace = T,
                                    gamma = 20)


plot(MCP_logistico_Kmeans.fit)
# Grafico dei beta selezionati al variare di lambda

MCP_logistico_Kmeans.fit$convex.min

y.hat.MCP_Kmeans <- predict(MCP_logistico_Kmeans, 
                            X_test, 
                            s = lambda.k.mcp, 
                            type = "response")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la 
# probabilità delle classi 

y.hat.MCP_Kmeans <- ifelse(y.hat.MCP_Kmeans > 0.7, 2, 1)
# Utilizzo come soglia la proporzione campionaria dei livelli nell'insieme di
# stima

Tabella.MCP_Kmeans <- tabella.sommario(y.hat.MCP_Kmeans, test$Etichette_Kmeans_K2)
Metriche_MCP_Kmeans <- indici.errore(Tabella.MCP_Kmeans)
# Salvo le varie metriche ottenute da tale modello 

```

Non si sono ottenuti risultati soddisfacenti con le etichette Kaggle. Come visibile in Figura 12, infatti, l'andamento del Kappa di Cohen ha un comportamento sostanzialmnete asimmetrico tra test set e cross validation.
Si può notare come, basandosi sul Kappa massimo ottenuto in cross validation, si tenda a scegliere un $\lambda$ nel test set che rappresenta un minimo locale, quando in realtà il modello migliore nel test set è molto più complesso.

```{r SCAD Kaggle, eval=FALSE, include=FALSE}
# SCAD --------------------------------------------------------------------

lambda.grid <- exp(seq(-3,-7,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia è il risultato di prove precedenti che
# non vengono riportate

SCAD_logistico_Kaggle <- cv.ncvreg(X_train,
                                   factor(train$Etichette_Kaggle_K2),
                                   family = "binomial",
                                   penalty = "SCAD",
                                   nfolds = 5,
                                   fold = fold,
                                   trace = T,
                                   lambda = lambda.grid,
                                   gamma = 20,
                                   seed = 42,
                                   returnY = T)
# Viene stimato un modello logistico con penalita SCAD, in CV a 5 fold con 
# metrica di ottimizzazione la verosimiglianza negativa binomiale,
# viene oltretutto fissato un seed per la riproducibilità dei risultati.
# Con l'etichetta presente però occorre fare una considerazione, ci troviamo 
# in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
# metrica come l'errore di errata classificazione potrebbe portare a conclusioni
# inestatte, per tanto viene impostato un returnY = T all'interno del modello, 
# che permette di avere per ogni lambda la stima del predittore lineare delle
# unita statistiche che stanno nell'out of fold, cosi da riprodurre una 
# CV fatta a mano utilizzando i risultati ottenuti dalla funzione cv.ncvreg

plot(SCAD_logistico_Kaggle)
# Visualizzo andamento della verosimiglianza negativa per i vari lambda

eta <- SCAD_logistico_Kaggle$Y
pi.train <- exp(eta)/(1+exp(eta))
pred.train.class <- ifelse(pi.train > 0.70, 0, 1)
# Mi riconduco dal predittore linare alle classi stimate, nel caso presente
# utilizzo come soglia di classificazione la proporzione campionaria delle 
# classi nell'insime di stima

kappa.vals.scad.kaggle <- matrix(NA, ncol(pred.train.class), 5)
miss.vals.scad.kaggle <- matrix(NA, ncol(pred.train.class), 5)
# Inizializzo due matrici vuote con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati


for( i in 1:ncol(pred.train.class)){
  
  for(j in 1:5){
    ind.fold.out <- which(SCAD_logistico_Kaggle$fold == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class[ind.fold.out,i], 
                   train$Etichette_Kaggle_K2[ind.fold.out])
      kappa.vals.scad.kaggle[i,j] <- Kappa(tab)$Unweighted[1]
      miss.vals.scad.kaggle[i,j] <- indici.errore(
        tabella.sommario(
          pred.train.class[ind.fold.out,i],
          train$Etichette_Kaggle_K2[ind.fold.out]
        )
      )$Errata.Classificazione
      # Calcolo metriche e salvo il risultato per il j-esimo fold
      
      
    }
  }
}

error.Lambda.k1 <- apply(kappa.vals.scad.kaggle,1,mean)
se.Lambda.k1 <- apply(kappa.vals.scad.kaggle,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k.lam1 <- se.Lambda.k1
`Kappa Cohen` <-  error.Lambda.k1
`Log(Lambda)` <- log(SCAD_logistico_Kaggle$lambda)
data.plot.cv.k.lam1 <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kaggle.SCAD.k <- ggplot(data.plot.cv.k.lam1, 
                                mapping = aes(x = `Log(Lambda)`, y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_line(aes(color = "firebrick1"), size = 1, shape=1) +
  geom_ribbon(aes(ymin = `Kappa Cohen` - stderrcv.k, ymax = `Kappa Cohen` + stderrcv.k)
              , alpha = 0.1) +
  ggtitle("Kappa Cohen CV - SCAD")+ 
  theme(legend.position = "bottom", axis.text = element_text(size = 8),
        axis.title = element_text(size = 8),
        legend.title=element_text(size=10), 
        legend.text=element_text(size=9))
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 


error.Lambda.miss <- apply(miss.vals.scad.kaggle,1,mean)
se.Lambda.miss <- apply(miss.vals.scad.kaggle,1,sd)
# Calcolo Miss medio per ogni fold e relativa deviazione standard

stderrcv.k.l.miss <- se.Lambda.miss
`Miss` <-  error.Lambda.miss
`Log(Lambda)` <- log(SCAD_logistico_Kaggle$lambda)
data.plot.cv.miss <- tibble(`Miss`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kaggle.SCAD.miss <- ggplot(data.plot.cv.miss, 
                                   mapping = aes(x = `Log(Lambda)`, 
                                                 y = `Miss`)) +
  ylim(c(min(`Miss` - stderrcv.k.l.miss)-0.005,
         max(`Miss` + stderrcv.k.l.miss)+0.005))+
  geom_line(aes(color = "firebrick1"), size = 1, shape=20) +
  geom_ribbon(aes(ymin = `Miss` - stderrcv.k.l.miss, ymax = `Miss` + stderrcv.k.l.miss)
              , alpha = 0.1) + 
  ggtitle("Err. di classificazione CV - SCAD")+ 
  theme(legend.position = "bottom", axis.text = element_text(size = 8),
        axis.title = element_text(size = 8), legend.title=element_text(size=10), 
        legend.text=element_text(size=9)) + 
  labs(y = "Errore di classificazione", x = "Log(lambda)")
# Visualizzazione della metrica Miss per i diversi lambda, in scala logaritmica 

eta <- predict(SCAD_logistico_Kaggle, 
               X_test, 
               type = "link",
               lambda = lambda.grid)
pi.test <- exp(eta)/(1+exp(eta))
pred.test.class <- ifelse(pi.test > 0.7, 0, 1)
# Mi riconduco dal predittore linare alle classi stimate sull'insieme di 
# verifica, nel caso presente utilizzo come soglia di classificazione 
# la proporzione campionaria delle classi nell'insime di stima

kappa.vals.scad.kaggle.lambda <- rep(NA, length(lambda.grid))
miss.vals.scad.kaggle.lambda <- rep(NA, length(lambda.grid))
# Inizializzo due vettori vuoti i quali conterranno i valori della metrica 
# kappa e dell'errore di errata classificazione ottenuti sull'insieme di 
# verifica

for( i in 1:ncol(pred.test.class)){
  
  if (length(unique(pred.test.class[,i])) != 1)
    # Condizione per la quale si valuta la metrica relativa a quel valore di
    # un lambda se e solo se le classi stimate non hanno solo un livello
  {
    
    tab <- table(pred.test.class[,i],
                 test$Etichette_Kaggle_K2)
    kappa.vals.scad.kaggle.lambda[i] <- Kappa(tab)$Unweighted[1]
    miss.vals.scad.kaggle.lambda[i] <-  indici.errore(
      tabella.sommario(pred.test.class[,i], 
                       test$Etichette_Kaggle_K2))$Errata.Classificazione
    # Si salvano le metriche all'interno delle liste vuote
    
  }
}


`Kappa Cohen Validation` <-  kappa.vals.scad.kaggle.lambda
`Log(Lambda)` <- log(SCAD_logistico_Kaggle$lambda)
data.plot.kl <- tibble(`Kappa Cohen Validation`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 


srs.acc_Kaggle.SCAD.k2 <- srs.acc_Kaggle.SCAD.k + 
  geom_line(data = data.plot,
            aes(x = `Log(Lambda)`, 
                y =  `Kappa Cohen Validation`,
                color = "slateblue"), size = 1, shape=20) +
  scale_color_identity(name = "",
                       breaks = c("slateblue", "firebrick1"),
                       labels = c("Test","CV"),
                       guide = "legend")
# Visualizzo confronti tra andamento kappa su cv e su 
# insieme di verifica

`Miss Validation` <-  unlist(miss.vals.scad.kaggle.lambda)
`Log(Lambda)` <- log(SCAD_logistico_Kaggle$lambda)
data.plot <- tibble(`Miss Validation`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kaggle.SCAD.miss2 <- srs.acc_Kaggle.SCAD.miss + 
  geom_line(data = data.plot,
            mapping = aes(x = `Log(Lambda)`, 
                          y =  `Miss Validation`,
                          color = "slateblue"),size = 1, shape=20) +
  scale_color_identity(name = "",
                       breaks = c("slateblue", "firebrick1"),
                       labels = c("Test","CV"),
                       guide = "legend")
# Visualizzo confronti tra andamento errore di classificazione su cv
# e su insieme di verifica



```

```{r Plot SCAD Fallimento Kaggle, echo=FALSE, fig.cap="Confronto tra errore di classificazione e kappa di cohen in CV e sul test set su SCAD con le labels Kaggle ", fig.align="center", warning=FALSE, out.width="60%", out.height="60%"}
load("Environment/SCAD_Kaggle_def.RData")
grid.arrange(srs.acc_Kaggle.SCAD.miss2 + labs(
  title = "Errore di Classificazione "), 
  srs.acc_Kaggle.SCAD.k2 + labs(title = "Kappa di Cohen "), 
  ncol=2, top="SCAD - Etichette Kaggle")
```

## 4.4 SVM

Si adatta poi un SVM sparso con penalità *elastic net*, selezionando prima $\alpha$ e poi $\lambda$.
Si utilizza una griglia di $\lambda$ più ristretta, aspettandosi un comportamento simile a quello del lasso.

Per quanto riguarda le etichette fused, viene selezionato in questo caso un $\alpha$ pari a 1, che corrisponde quindi ad una penalità L1.
Il corrispondente $\lambda$ selezionato tramite successiva convalida incrociata risulta 0.0432.
Inaspettatamente, questo modello seleziona ben 5.163 covariate, molte di più rispetto agli altri metodi; inoltre, l'errore di classificazione in cross validation risulta essere maggiore rispetto a quello riscontrabile nel test set.

```{r SVM Elastic Fused, eval=FALSE, include=FALSE}
# SVM - Elastic Fused ----------------------------------------------------------

lambda.grid <- exp(seq(-4,-3,l=100))
# Si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid <- c(1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha <- rep(NA, length(alpha.grid))
se.alpha <- rep(NA, length(alpha.grid))
# Per far si che il modello SVM abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard, nel caso presente nella griglia viene messo anche il 
# valore 1 relativo alla penalità lasso per valutare quale sia il valore di 
# alpha che ottimizza l'errore

for(i in 1:length(alpha.grid)){
  
  fit <- cv.sparseSVM(X_train,
                      ifelse(train$Etichette_Fused_K2==1,-1,1),
                      nfolds = 5,
                      fold.id = fold,
                      trace = T, 
                      alpha = alpha.grid[i],
                      seed = 42,
                      lambda = lambda.grid)
  # Viene stimato un modello SVM con penalita Elastic-Net,
  # in CV a 5 fold con metrica di ottimizzazione l'errore di errata
  # classificazione, per ogni valore di alpha contenuto in alpha.grid,
  # viene oltretutto fissato un seed per la riproducibilità dei risultati
  
  error.alpha[i] <- fit$cve[which.min(fit$cve)]
  se.alpha[i] <- fit$cvse[which.min(fit$cve)]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

error.alpha[which.min(error.alpha)]
# Minimo dell'errore di classificazione al variare dell'alpha

best.alpha_Fused_SVM <- alpha.grid[which.min(error.alpha)]
best.alpha_Fused_SVM
# Viene definito l'alpha per il quale si ottiene l'errore di classificazione 
# minore

E.Net_SVM_Fused <- cv.sparseSVM(X_train,
                                ifelse(train$Etichette_Fused_K2==1,-1,1),
                                nfolds = 5,
                                fold.id = fold,
                                alpha = best.alpha_Fused_SVM,
                                trace = T,
                                lambda = lambda.grid,
                                seed = 42)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Fused, in CV a 5 fold con metrica di ottimizzazione l'errore di 
# errata classificazione, per ogni valore di alpha contenuto in alpha.grid,
# viene oltretutto fissato un seed per la riproducibilità dei risultati

plot(E.Net_SVM_Fused, xvar="lambda")
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello SVM Elastic-Net


n.coef_E.Net_SVM_Fused <- length(which(coef(E.Net_SVM_Fused, lambda=E.Net_SVM_Fused$lambda.min)!=0))



y.hat.E.Net.SVM_Fused <- predict(E.Net_SVM_Fused,
                                 X_test,
                                 lambda = E.Net_SVM_Fused$lambda.min,
                                 type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

y.hat.E.Net.SVM_Fused <- ifelse(y.hat.E.Net.SVM_Fused == -1,1,2)
# Per migliorare le prestazioni del modello, questo viene stimato cambiando
# i livelli della variabile risposta in -1, 1 per tanto una volta stimato,
# per avere metriche coerenti si riportano i livelli di partenza 

Tabella.E.Net_Fused_SVM <- tabella.sommario(test$Etichette_Fused_K2 , 
                                            y.hat.E.Net.SVM_Fused)
Metriche_E.Net_SVM_Fused <- indici.errore(Tabella.E.Net_Fused_SVM)
# Salvo le varie metriche ottenute da tale modello 

```

```{r Grafico Alpha SVM, include=FALSE, eval=FALSE, out.height='40%', out.width='40%', fig.align="center", fig.cap="Andamento del tasso di errata classificazione al variare di alpha per l'SVM"}

stderrcv.k <- se.alpha
`MissClassificationError` <-  error.alpha
`Alpha` <- alpha.grid
data.plot.cv <- tibble(`MissClassificationError`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Fused_SVM <- ggplot(data.plot.cv, 
                            mapping = aes(x = `Alpha`, 
                                          y = `MissClassificationError`)) +
  ylim(c(min(`MissClassificationError` - stderrcv.k)-0.005,
         max(`MissClassificationError` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Alpha`, y = `MissClassificationError` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `MissClassificationError` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `MissClassificationError` - stderrcv.k,
                   xend = `Alpha`, 
                   yend = `MissClassificationError` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("MissClassificationError  CV")
srs.acc_Fused_SVM
# Visualizzazione dell'errore di classificazione per i diversi alpha 

```

```{r Immagine SVM e Lasso, echo=FALSE,fig.align="center", fig.cap="Rappresentazione dei pixel selezionati dal Lasso (a sinistra, (a.)) e dall'SVM (a destra (b.)) relative alle etichette fused", out.height='60%', out.width='60%'}
load("Environment/Modelli_Fused_def1_RMD.RData")
par(mfrow=c(1,2))
v <- rep(0,64600)
v[which(coef(Lasso_logistico_Fused, s=Lasso_logistico_Fused$lambda.min)!=0)]=1
v <- matrix(v, 190, 340)
plot(EBImage::Image(v))


svm.im <- rep(0,64600)
svm.im[which(coef(E.Net_SVM_Fused, s=E.Net_SVM_Fused$lambda.min)!=0)]=1
svm.im <- matrix(svm.im, 190, 340)
plot(EBImage::Image(svm.im))
```

In Figura 13.b sono visibili i pixel selezionati dall'SVM sempre per quanto riguarda le etichette fused.
Si nota una notevole concentrazione nella parte centrale dell'immagine.
Interessante risulta anche il confronto con i pixel selezionati dal lasso.

Adattando il modello alle etichette Kaggle, si individua un valore di $\alpha$ ottimo pari a 0.7 e un $\lambda$ pari a 0.1223328.
Il numero di coefficienti diversi da 0 selezionati è 150.

```{r SVM Kaggle, eval=FALSE, include=FALSE}
# SVM - Elastic Kaggle  ----------------------------------------------------------

lambda.grid.svm <- exp(seq(-4.5,-2,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia è il risultato di prove precedenti che
# non vengono riportate

validation <- which(train$Day %in% c(1,2,5))
# Poiche la funzione cv.sparseSVM non restituisce i predittori lineari per
# ogni fold, si usa come soluzione l'utilizo di un terzo insieme quello di 
# validazione per la scelta dei parametri, tale insieme comprenderà le 
# osservazioni relative al primo secondo e quinto giorno

alpha.grid.svm <- c(1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
kappa.vals.alpha.svm <- rep(NA,length(alpha.grid.svm))
kappa.vals.lambda.svm <- rep(NA, length(lambda.grid.svm))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializza
# un vettore che conterrà la metrica Kappa sull'insieme di validazione


for(alpha in 1:length(alpha.grid.svm)){
  
  fit <- sparseSVM(X_train[-validation,],
                   ifelse(train$Etichette_Kaggle_K2==1,-1,1)[-validation],
                   lambda = lambda.grid.svm,
                   alpha = alpha.grid.svm[alpha])
  # Viene stimato un modello SVM con penalità Elastic-Net, sull'insieme di 
  # stima senza insieme di validazione
  
  pred.train.class.svm.al <- predict(fit, X_train[validation,], type = "class")
  # Vengono calcolate le previsioni di tipo classe sull'insieme di validazione
  
  pred.train.class.svm.al <- ifelse(pred.train.class.svm.al == -1, 1, 0)
  # Poichè in fase di stima i modelli avevano come variabile risposta una 
  # variabile codificata con i valori -1, 1, i valori previsiti vengono
  # riportati nella loro vera scala 1, 2.
  
  print(alpha)
  
  for( i in 1:ncol(pred.train.class.svm.al)){
    
    if (length(unique(pred.train.class.svm.al[,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel valore di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class.svm.al[,i], 
                   train$Etichette_Kaggle_K2[validation])
      kappa.vals.lambda.svm[i] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per l'i-esimo lambda
      
    }
  }
  
  kappa.vals.alpha.svm[alpha] <- max(kappa.vals.lambda.svm)
  # Salvo il valore massimo del Kappa per un dato alpha, al variare di lambda
}

`Kappa Cohen  CV` <-  kappa.vals.alpha.svm
`Alpha` <- alpha.grid.svm
data.plot.cv.svm.alpha <- tibble(`Kappa Cohen  CV`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Kaggle_SVM <- ggplot(data.plot.cv.svm.alpha, 
                             mapping = aes(x = `Alpha`, 
                                           y = `Kappa Cohen  CV`)) +
  ylim(c(min(`Kappa Cohen  CV`)-0.01,
         max(`Kappa Cohen  CV`)+0.01))+
  geom_point(col = "slateblue", size = 3) +
  ggtitle("Kappa Cohen  CV  CV")
srs.acc_Kaggle_SVM
# Visualizzazione di Kappa per i diversi alpha 

kappa.vals.alpha.svm[which.max(kappa.vals.alpha.svm)]
# Massimo di Kappa al variare dell'alpha

best.alpha_Kaggle_SVM <- alpha.grid.svm[which.max(kappa.vals.alpha.svm)]
best.alpha_Kaggle_SVM
# Viene definito l'alpha per il quale si ottiene il Kappa maggiore

E.Net_SVM_Kaggle <- sparseSVM(X_train[-validation,],
                              ifelse(train$Etichette_Kaggle_K2==1,
                                     -1,1)[-validation],
                              lambda = lambda.grid.svm,
                              alpha = best.alpha_Kaggle_SVM)
# Stimo un modello SVM con penalità alpha data dalla massimizzazione del 
# Kappa tramite insieme di convalida

pred.train.class.SVM <- predict(E.Net_SVM_Kaggle, 
                            X_train[validation,], 
                            type = "class")
# Calcolo valori previsti di tipo classe sull'insieme di validazione

pred.train.class.SVM <- ifelse(pred.train.class.SVM == -1, 1, 0)
# Riporto i livelli delle preveisioni a quelli di partenza

kappa.vals.lambda.SVM <- rep(NA, length(lambda.grid.svm))
# Inizializzo un vettore vuoto lungo quanto la griglia di lambda

for( i in 1:ncol(pred.train.class.SVM)){
  
  if (length(unique(pred.train.class.SVM[,i])) != 1)
    # Condizione per la quale si valuta la metrica relativa a quel valore di
    # un lambda se e solo se le classi stimate non hanno solo un livello
  {
    
    tab <- table(pred.train.class.SVM[,i],
                 train$Etichette_Kaggle_K2[validation])
    kappa.vals.lambda.SVM[i] <- Kappa(tab)$Unweighted[1]
    # Calcolo Kappa e salvo il risultato per l'i-esimo lambda
    
    
    
  }
}


`Kappa Cohen Validation` <-  kappa.vals.lambda.SVM
`Log(Lambda)` <- log(lambda.grid.svm)
data.plot.lambda.SVM.validation <- tibble(`Kappa Cohen Validation`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kaggle_SVM.kappa.validation <- 
  ggplot(data.plot.lambda.SVM.validation, 
         mapping = aes(x = `Log(Lambda)`, 
                       y = `Kappa Cohen Validation`)) +
  ylim(c(-0.05,0.05)) +
  geom_point(col = "slateblue", size = 3) +
  ggtitle("Kappa Cohen Validation")
srs.acc_Kaggle_SVM.kappa.validation
# Visualizzo andamento kappa su insieme di validazione

lambda.k.SVM <- lambda.grid[which.max(kappa.vals.lambda.SVM)]
# Salvo il miglior lambda

E.Net_SVM_Kaggle <- sparseSVM(X_train,
                              ifelse(train$Etichette_Kaggle_K2==1,
                                     -1,1),
                              lambda = lambda.k.SVM,
                              alpha = best.alpha_Kaggle_SVM)
# Ristimo il modello su tutto l'insieme di stima


n.coef_SVM_Kaggle <- length(which(coef(E.Net_SVM_Kaggle, s =  lambda.k.SVM)!=0))

y.hat.E.Net.SVM_Kaggle <- predict(E.Net_SVM_Kaggle, 
                                  X_test, 
                                  type = "class")
# Calcolo le previsioni di tipo classe

y.hat.E.Net.SVM_Kaggle <- ifelse(y.hat.E.Net.SVM_Kaggle == -1, 1, 0)
# Riporto i livelli delle preveisioni a quelli di partenza

Tabella.E.Net.SVM_Kaggle <- tabella.sommario(y.hat.E.Net.SVM_Kaggle,test$Etichette_Kaggle_K2 )
Metriche_E.Net.SVM_Kaggle <- indici.errore(Tabella.E.Net.SVM_Kaggle)
# Salvo le varie metriche ottenute da tale modello
```


Per quanto riguarda le etichette k-means, la scelta dei parametri viene effettuata su un insieme di validazione composto dai giorni 1, 2 e 5.
Con tale procedura si ottiene un valore di $\alpha$ pari a 0.8.
Si ristima pertanto un modello su una griglia di $log(\lambda)$ che va da -4.5 a -2, scegliendo, tramite la massimizzazione del Kappa di Cohen come precedemente descritto, un valore di $log(\lambda)$ pari a -2.833333, portandoci a selezionare un modello con una complessità elevata, in particolare con 15 972 coefficienti.
Come si può notare in Figura 14.b, in corrispondenza del massimo per il Kappa di Cohen nell'insieme di validazione (curva blu), si ottiene un massimo locale sull'insieme di verifica (curva rossa). Si nota infatti come le due curve sui due diversi insiemi seguano lo stesso andamento.
Tale massimizzazione va però in conflitto con l'errore di classificazione; si nota infatti in Figura 14.a come in prossimità di valori di $log(\lambda)$ pari a -2.833333, l'errore di classificazione nell'insieme di verifica sia intorno al suo massimo.

```{r SVM Kmeans, eval=FALSE, include=FALSE}
# SVM - Elastic  ----------------------------------------------------------

lambda.grid.svm <- exp(seq(-4.5,-2,l=100))
# Definisco una griglia di lambda che mi permetta di esplorare un sottospazio
# ben definito di valori, tale griglia C( il risultato di prove precedenti che
# non vengono riportate

validation <- which(train$Day %in% c(1,2,5))
# Poiche la funzione cv.sparseSVM non restituisce i predittori lineari per
# ogni fold, si usa come soluzione l'utilizo di un terzo insieme quello di 
# validazione per la scelta dei parametri, tale insieme comprenderC  le 
# osservazioni relative al primo secondo e quinto giorno

alpha.grid.svm <- c(1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
kappa.vals.alpha.svm <- rep(NA,length(alpha.grid.svm))
kappa.vals.lambda.svm <- rep(NA, length(lambda.grid.svm))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializza
# un vettore che conterrC  la metrica Kappa sull'insieme di validazione


for(alpha in 1:length(alpha.grid.svm)){
  
  fit <- sparseSVM(X_train[-validation,],
                   ifelse(train$Etichette_Kmeans_K2==1,-1,1)[-validation],
                   lambda = lambda.grid.svm,
                   alpha = alpha.grid.svm[alpha])
  # Viene stimato un modello SVM con penalitC  Elastic-Net, sull'insieme di 
  # stima senza insieme di validazione
  
  pred.train.class.svm.cv <- predict(fit, X_train[validation,], type = "class")
  # Vengono calcolate le previsioni di tipo classe sull'insieme di validazione
  
  pred.train.class.svm.cv <- ifelse(pred.train.class.svm.cv == -1, 1, 2)
  # PoichC( in fase di stima i modelli avevano come variabile risposta una 
  # variabile codificata con i valori -1, 1, i valori previsiti vengono
  # riportati nella loro vera scala 1, 2.
  
  print(alpha)
  
  for( i in 1:ncol(pred.train.class.svm.cv)){
    
    if (length(unique(pred.train.class.svm.cv[,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel valore di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class.svm.cv[,i], 
                   train$Etichette_Kmeans_K2[validation])
      kappa.vals.lambda.svm[i] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per l'i-esimo lambda
      
    }
  }
  
  kappa.vals.alpha.svm[alpha] <- max(kappa.vals.lambda.svm)
  # Salvo il valore massimo del Kappa per un dato alpha, al variare di lambda
}


`Kappa Cohen Validation` <-  unlist(kappa.vals.alpha.svm)
`Alpha` <- alpha.grid.svm
data.plot.kappa.val <- tibble(`Kappa Cohen Validation`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans_SVM.kappa <- 
  ggplot(data.plot.kappa.val, 
         mapping = aes(x = `Alpha`, 
                       y = `Kappa Cohen Validation`)) +
  ylim(c(0,0.15)) +
  geom_point(aes(color = "slateblue"), size = 1, shape=20) +
  ggtitle("Kappa Cohen Validation")+ theme(legend.position = "bottom", 
                                           axis.text = element_text(size = 8),
                                           axis.title = element_text(size = 8),
                                           legend.title=element_text(size=10), 
                                           legend.text=element_text(size=9))
srs.acc_Kmeans_SVM.kappa
# Visualizzo andamento kappa su insieme di validazione


kappa.vals.alpha.svm[which.max(kappa.vals.alpha.svm)]
# Massimo di Kappa al variare dell'alpha

best.alpha_Kmeans_SVM <- alpha.grid.svm[which.max(kappa.vals.alpha.svm)]
best.alpha_Kmeans_SVM
# Viene definito l'alpha per il quale si ottiene il Kappa maggiore

E.Net_SVM_Kmeans <- sparseSVM(X_train[-validation,],
                              ifelse(train$Etichette_Kmeans_K2==1,
                                     -1,1)[-validation],
                              lambda = lambda.grid.svm,
                              alpha = best.alpha_Kmeans_SVM)
# Stimo un modello SVM con penalitC  alpha data dalla massimizzazione del 
# Kappa tramite insieme di convalida

pred.train.class.svm <- predict(E.Net_SVM_Kmeans, 
                                X_train[validation,], 
                                type = "class")
# Calcolo valori previsti di tipo classe sull'insieme di validazione

pred.train.class.svm <- ifelse(pred.train.class.svm == -1, 1, 2)
# Riporto i livelli delle preveisioni a quelli di partenza

kappa.vals.lambda.svm <- rep(NA, length(lambda.grid.svm))
miss.vals.lambda.svm <- rep(NA, length(lambda.grid.svm))
# Inizializzo due vettori vuoti i quali conterranno i valori della metrica 
# kappa e dell'errore di errata classificazione ottenuti sull'insieme di 
# validazione

for( i in 1:ncol(pred.train.class.svm)){
  
  if (length(unique(pred.train.class.svm[,i])) != 1)
    # Condizione per la quale si valuta la metrica relativa a quel valore di
    # un lambda se e solo se le classi stimate non hanno solo un livello
  {
    
    tab <- table(pred.train.class.svm[,i],
                 train$Etichette_Kmeans_K2[validation])
    kappa.vals.lambda.svm[i] <- Kappa(tab)$Unweighted[1]
    
    miss.vals.lambda.svm[i] <- indici.errore(
      tabella.sommario(pred.train.class.svm[,i], 
                       train$Etichette_Kmeans_K2[validation])
    )$Errata.Classificazione
    # Si salvano le metriche all'interno delle liste vuote
    
  }
}

`Miss Validation` <-  unlist(miss.vals.lambda.svm)
`Log(Lambda)` <- log(E.Net_SVM_Kmeans$lambda)
data.plot.miss.val <- tibble(`Miss Validation`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 


srs.acc_Kmeans_SVM.miss <- 
  ggplot(data.plot.miss.val, 
         mapping = aes(x = `Log(Lambda)`, 
                       y = `Miss Validation`)) +
  ylim(c(0.28,0.5)) +
  geom_point(aes(color = "slateblue"), size = 1, shape=20) +
  ggtitle("Misclassification Error Validation")+ theme(legend.position = "bottom", 
                                                       axis.text = element_text(size = 8),
                                                       axis.title = element_text(size = 8),
                                                       legend.title=element_text(size=10), 
                                                       legend.text=element_text(size=9))
# Visualizzo andamento errore di classificazione su insieme di validazione
srs.acc_Kmeans_SVM.miss


`Kappa Cohen Validation` <-  unlist(kappa.vals.lambda.svm)
`Log(Lambda)` <- log(E.Net_SVM_Kmeans$lambda)
data.plot.kappa.val <- tibble(`Kappa Cohen Validation`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans_SVM.kappa <- 
  ggplot(data.plot.kappa.val, 
         mapping = aes(x = `Log(Lambda)`, 
                       y = `Kappa Cohen Validation`)) +
  ylim(c(0.025,0.15)) +
  geom_point(aes(color = "slateblue"), size = 1, shape=20) +
  ggtitle("Kappa Cohen Validation")
srs.acc_Kmeans_SVM.kappa
# Visualizzo andamento kappa su insieme di validazione

E.Net_SVM_Kmeans <- sparseSVM(X_train,
                              ifelse(train$Etichette_Kmeans_K2==1,
                                     -1,1),
                              lambda = lambda.grid.svm,
                              alpha = best.alpha_Kmeans_SVM)
# Ristimo il modello su tutto l'insieme di stima

pred.test.class.svm.train <- predict(E.Net_SVM_Kmeans, 
                                     X_test, 
                                     type = "class")
# Calcolo le previsioni di tipo classe

pred.test.class.svm.train <- ifelse(pred.test.class.svm.train == -1, 1, 2)
# Riporto i livelli delle preveisioni a quelli di partenza

kappa.vals.lambda.svm.tr <- rep(NA, length(lambda.grid.svm))
miss.vals.lambda.svm.tr <- rep(NA, length(lambda.grid.svm))
# Inizializzo due vettori vuoti i quali conterranno i valori della metrica 
# kappa e dell'errore di errata classificazione ottenuti sull'insieme di 
# validazione

for( i in 1:ncol(pred.test.class.svm.train)){
  
  if (length(unique(pred.test.class.svm.train[,i])) != 1)
    # Condizione per la quale si valuta la metrica relativa a quel valore di
    # un lambda se e solo se le classi stimate non hanno solo un livello
  {
    
    tab <- table(pred.test.class.svm.train[,i],
                 test$Etichette_Kmeans_K2)
    kappa.vals.lambda.svm.tr[i] <- Kappa(tab)$Unweighted[1]
    miss.vals.lambda.svm.tr[i] <-  indici.errore(
      tabella.sommario(pred.test.class.svm.train[,i], 
                       test$Etichette_Kmeans_K2))$Errata.Classificazione
    # Si salvano le metriche all'interno delle liste vuote
    
  }
}
`Miss Validation` <-  unlist(miss.vals.lambda.svm.tr)
`Log(Lambda)` <- log(E.Net_SVM_Kmeans$lambda)
data.plot.svm.miss.lam <- tibble(`Miss Validation`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans_SVM.miss_tot <- srs.acc_Kmeans_SVM.miss + 
  geom_point(data = data.plot.svm.miss.lam,
             mapping = aes(x = `Log(Lambda)`, 
                           y =  `Miss Validation`,
                           color = "red"), size = 1, shape=20) +
  scale_color_identity(name = "Legenda",
                       breaks = c("slateblue", "red"),
                       labels = c("Validation","Test"),
                       guide = "legend")

srs.acc_Kmeans_SVM.miss_tot
# Visualizzo confronti tra andamento errore di classificazione su insieme
# di validazione e su insieme di verifica


`Kappa Cohen Validation` <-  kappa.vals.lambda.svm.tr
`Log(Lambda)` <- log(E.Net_SVM_Kmeans$lambda)
data.plot <- tibble(`Kappa Cohen Validation`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans_SVM.kappa_tot <- srs.acc_Kmeans_SVM.kappa + 
  geom_point(data = data.plot,
             mapping = aes(x = `Log(Lambda)`, 
                           y =  `Kappa Cohen Validation`,
                           color = "red"), size = 1, shape=20) +
  scale_color_identity(name = "Legenda",
                       breaks = c("slateblue", "red"),
                       labels = c("Validation","Test"),
                       guide = "legend")

srs.acc_Kmeans_SVM.kappa_tot
# Visualizzo confronti tra andamento kappa su insieme di validazione e su 
# insieme di verifica


lambda.k.svm <- E.Net_SVM_Kmeans$lambda[which.max(kappa.vals.lambda.svm)]
# Salvo il miglior lambda
n.coef_E.Net_SVM_Kmeans <- length(which(coef(E.Net_SVM_Kmeans, lambda=lambda.k.svm)!=0))


y.hat.E.Net.SVM_Kmeans <- predict(E.Net_SVM_Kmeans, 
                                  X_test, 
                                  lambda = lambda.k.svm,
                                  type = "class")
# Calcolo le previsioni di tipo classe

y.hat.E.Net.SVM_Kmeans <- ifelse(y.hat.E.Net.SVM_Kmeans == -1, 1, 2)
# Riporto i livelli delle preveisioni a quelli di partenza

Tabella.E.Net.SVM_Kmeans <- tabella.sommario(test$Etichette_Kmeans_K2 ,
                                             y.hat.E.Net.SVM_Kmeans)
Metriche_E.Net.SVM_Kmeans <- indici.errore(Tabella.E.Net.SVM_Kmeans)
# Salvo le varie metriche ottenute da tale modello

```

```{r Grafico Kmeans, echo=FALSE, fig.cap="Andamento errore di classificazione e kappa al variare di lambda nell'SVM con etichette k-means", out.height="60%", out.width="60%", fig.align="center"}
#load("Environment/SVM_Kmeans_K2_def1.RData")
#grid.arrange(srs.acc_Kmeans_SVM.miss_tot,srs.acc_Kmeans_SVM.kappa_tot, ncol=2)

grid.arrange(srs.acc_Kmeans_SVM.miss_tot + labs(
  title = "Errore di Classificazione ",
  y = "Errore di classificazione", x = "Log(lambda)") + theme(legend.title = element_blank()),
  srs.acc_Kmeans_SVM.kappa_tot + labs(
    title = "Kappa di Cohen",
    y = "Kappa di Cohen", x = "Log(lambda)") +
  theme(legend.position = "bottom", legend.title = element_blank(), axis.text = element_text(size = 8), axis.title = element_text(size = 8)), ncol=2, top="SVM - Etichette K-means")
```

\newpage
## 4.5 NSC

Infine, si adatta un modello *Nearest Shrunken Centroids*, in cui si vanno a stimare i centroidi, penalizzandoli, per poi utilizzarli con la regola del centroide più vicino.
Questo in genere consente di selezionare solo un piccolo sottoinsieme di centroidi diversi da 0 (e conseguentemente di variabili).

Al contrario degli altri modelli, per i quali la selezione de parametri di regolarizzazione viene effettuata tramite cross validation, in questo caso si effettua tale selezione sull'insieme di stima, in quanto la cross validation porta a scegliere il modello più semplice che porterebbe ad una classificazione casuale e alla previsione di una sola classe.

Per le etichette fused, come è possibile vedere dai grafici, dopo aver centrato e scalato i dati, solo una piccola porzione di centroidi per ogni classe risulta essere diversa dai centroidi generali.
Ciò è coerente con i risultati forniti dai modelli precedenti.
Per entrambe le classi vengono selezionate le stesse variabili, che sono in totale 73.

```{r NSC Fused codice da mostrare, eval =FALSE}
# NSC ---------------------------------------------------------------------
result_Fused <- list(x = t(scale(X_train)), y = factor(train$Etichette_Fused_K2))
# Definisco una lista contenente la matrice del disegno e la variabile risposta

nsc_Fused <- pamr.train(result_Fused)
# Stimo il modello

best.t_Fused <- nsc_Fused$threshold[which.min(nsc_Fused$errors)]
# Delimito la miglior soglia (parametro di regolarizzazione) in base all'errore
# ottenuto sul train

nsc.opt_Fused <- pamr.train(result_Fused, threshold = best.t_Fused)
# Ristimo il modello per la soglia ottimale
```

```{r Modello NSC Fused, eval=FALSE, include=FALSE}
y.hat.nsc_Fused <- pamr.predict(nsc.opt_Fused, 
                                t(scale(X_test)), 
                                threshold = best.t_Fused)
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

scen_Fused <- pamr.predict(nsc.opt_Fused, 
                           result_Fused$x, 
                           threshold = best.t_Fused, 
                           type="cent")
# Calcolo i centroidi ovvero i valori delle variabili diverse da zero tenute
# dal modello al fine di poterle visualizzare 

dif_Fused <- (scen_Fused - nsc.opt_Fused$centroid.overall)/(nsc.opt_Fused$sd)
# Poiche i valori delle covariate vengono passate standardizzate, per effetuare
# un confronto sensato si standardizzano anche i valori dei centroidi 

n.coef_NSC_Fused <- length(which(dif_Fused[,1]!=0))
# Numero di coefficienti diversi da 0
length(which(dif_Fused[,1]!=0))
length(which(dif_Fused[,2]!=0))
# risulta uguale anche per la classe 2

which(dif_Fused[,1]!=0)==which(dif_Fused[,1]!=0)
which(dif_Fused[,1]!=0)==which(dif_Fused[,2]!=0)
# Pixel diversi da 0: risultano uguali per entrambe le classi

Tabella.nsc_Fused <- tabella.sommario(y.hat.nsc_Fused, test$Etichette_Fused_K2)
Metriche_NSC_Fused <- indici.errore(Tabella.nsc_Fused)
# Salvo le varie metriche ottenute da tale modello 

```

```{r Plot NSC, echo=FALSE, fig.align="center", fig.cap="Grafico NSC per etichette fused: in blu i centroidi ristretti (shrunken), in grigio la media specifica della classe per ogni pixel", out.height="60%", out.width="60%"}
par(mfrow = c(1,2))
plot(y = 1:length(nsc.opt_Fused$centroids[,1]), 
     x = nsc.opt_Fused$centroids[,1], 
     xlim=c(-0.15,0.15),
     col="lightgrey", 
     type="l",
     main = "Etichetta 1",
     xlab = "Centroidi",
     ylab = "Pixel")
abline(v=0, col="slateblue")
lines(y = 1:length(dif_Fused[,1]),
      x = dif_Fused[,1], 
      col="blue", 
      lwd=3)

plot(y = 1:length(nsc.opt_Fused$centroids[,2]), 
     x = nsc.opt_Fused$centroids[,2],
     xlim=c(-0.15,0.15), 
     col="lightgrey", 
     type="l",
     main = "Etichetta 2",
     xlab = "Centroidi",
     ylab = "Pixel")
abline(v=0, col="slateblue")
lines(y = 1:length(dif_Fused[,2]),
      x = dif_Fused[,2], 
      col="blue", 
      lwd=3)
par(mfrow = c(1,1))
# Visualizzo il comportamento medio dei pixel in base ai centrodi stimati

```

```{r Immagine NSC, eval=FALSE, include=FALSE}
nsc.im <- rep(0,64600)
nsc.im[which(dif_Fused[,1]!=0)]=1
nsc.im <- matrix(nsc.im, 190, 340)
plot(Image(nsc.im))
```

Sulle etichette k-means e kaggle, invece, non è stato possibile ricondursi a risultati soddisfacenti, conseguentemente non si riportano i relativi risultati.

## 4.6 Risultati finali

Si analizzano per prima cosa le metriche derivanti dalle etichette fused lasso.

```{r Confronto Metriche Fused, eval=FALSE, include=FALSE}
# Confronti Finali --------------------------------------------------------

Metriche_tot_Fused = sapply(grep("Metriche", ls(), value = T), get)
colnames(Metriche_tot_Fused) <- gsub("Metriche_","",colnames(Metriche_tot_Fused))
# Salvo tutte le metriche di tutti i modelli in un unico oggetto al
# fine di poter confrontare meglio tutti i modelli 

Tab_tot_Fused = sapply(grep("Tabella", ls(), value = T), get)
colnames(Tab_tot_Fused) <- gsub("Tabella_","",colnames(Tab_tot_Fused))
kappa.finali <- c()
for(i in 1:6){
  kappa.finali <- c(kappa.finali,Kappa(matrix(Tab_tot_Fused[,i], 2,2, byrow=F))$Unweighted[1])
}
names(kappa.finali) = gsub("Tabella_","",colnames(Tab_tot_Fused))
kappa.finali <- data.frame(kappa.finali)
names(kappa.finali) <- "Kappa di Cohen"
knitr::kable(kappa.finali)
Tab_Kappa_Fused <- kappa.finali



Tab_ncoef.Fused <- sapply(grep("n.coef", ls(), value = T), get)
names(Tab_ncoef.Fused) <- gsub("Metriche_","",colnames(Metriche_tot_Fused))
# Tabella con il numero di coefficienti tenuti dai modelli
Tab_ncoef.Fused <- as.data.frame(Tab_ncoef.Fused)
colnames(Tab_ncoef.Fused) <- c("Numero di coefficienti diversi da 0")
```

```{r echo=FALSE, fig.cap="Tabelle riassuntive relative al fused lasso", out.height='40%', out.width='40%'}
load("Environment/models_stats.RData")

tab.tot.fused <- rbind(Metriche_tot_Fused, round(Tab_Kappa_Fused$`Kappa di Cohen`, 3), Tab_ncoef.Fused$`Numero di coefficienti diversi da 0`)
row.names(tab.tot.fused)[11] <- "Coefficienti diversi da 0"
row.names(tab.tot.fused)[10] <- "Kappa di Cohen"

colnames(tab.tot.fused)=c("E.Net", "E.Net SVM", "Lasso", "MCP", "NSC", "SCAD")
knitr::kable(tab.tot.fused, caption="Risultati Fused")
```

Come visibile dalla Tabella 2, considerando il tasso di errata classificazione, il modello migliore risulta essere NSC, anche se più del 98% delle osservazioni vengono predette da esso come 1, risultato che non ci si augurerebbe da un buon modello.
I restanti modelli producono approssimativamente gli stessi risultati.
In particolare, SCAD è in grado di ottenere lo stesso errore di errata classificazione pur utilizzando 31 variabili, quando gli altri modelli ne tengono un numero superiore.

Infine, si può notare come SVM produca il Kappa di Cohen più elevato tra tutti i modelli, nonostante l'accuracy sia la più bassa.
Questo indica che le previsioni sono più bilanciate e maggiormente in grado di discriminare tra le due classi, come visibile dai valori di sensibilità e specificità, diversi rispetto agli altri modelli.

Passando poi alle etichette k-means, come è possibile vedere dalla Tabella 3, l'errore di classificazione rimane in media simile per tutti i modelli stimati con le etichette fused, fatta eccezione per SCAD e MCP che peggiorano visibilmente.
Tuttavia, si ottiene con l'Elastic Net SVM il Kappa di Cohen maggiore tra tutti i modelli stimati, a scapito però di un elevato numero di coefficienti.
Tenendo in considerazione l'errore di classificazione, il Lasso ottiene i risultati migliori, con un errore pari a 0.378.
SCAD e MCP, pur ottenendo un errore di classificazione più alto, che si attesta circa a 0.48, consentono un miglior bilanciamento di sensibilità e specificità, assieme anche un numero di coefficienti basso, attorno agli 80.


```{r eval=FALSE, include=FALSE}
# Confronti Finali --------------------------------------------------------
Metriche_tot_Kmeans = sapply(grep("Metriche", ls(), value = T), get)
colnames(Metriche_tot_Kmeans) <- gsub("Metriche_","",colnames(Metriche_tot_Kmeans))
knitr::kable(Metriche_tot_Kmeans)

Tab_tot_Kmeans = sapply(grep("Tabella", ls(), value = T), get)
colnames(Tab_tot_Kmeans) <- gsub("Tabella_","",colnames(Tab_tot_Kmeans))
kappa.finali <- c()
for(i in 1:4){
  kappa.finali <- c(kappa.finali,Kappa(matrix(Tab_tot[,i], 2,2, byrow=F))$Unweighted[1])
}
names(kappa.finali) = gsub("Tabella_","",colnames(Tab_tot_Kmeans))
kappa.finali <- data.frame(kappa.finali)
names(kappa.finali) <- "Kappa di Cohen"
knitr::kable(kappa.finali)
Tab_Kappa_Kmeans <- kappa.finali

Tab_ncoef.Kmeans <- sapply(grep("n.coef", ls(), value = T), get)
#names(Tab_ncoef.Kmeans) <- gsub("Metriche_","",colnames(Metriche_tot))
# Tabella con il numero di coefficienti tenuti dai modelli
Tab_ncoef.Kmeans <- as.data.frame(Tab_ncoef.Kmeans)
colnames(Tab_ncoef.Kmeans) <- c("Numero di coefficienti diversi da 0")
knitr::kable(Tab_ncoef.Kmeans)
```

```{r echo=FALSE}
tab.tot.kmeans <- rbind(Metriche_tot_Kmeans, round(Tab_Kappa_Kmeans$`Kappa di Cohen`, 3), Tab_ncoef.Kmeans$`Numero di coefficienti diversi da 0`)
row.names(tab.tot.kmeans)[3] <- "Sensibilità"
row.names(tab.tot.kmeans)[4] <- "Specificità"
row.names(tab.tot.kmeans)[11] <- "Coefficienti diversi da 0"
row.names(tab.tot.kmeans)[10] <- "Kappa di Cohen"

colnames(tab.tot.kmeans)=c("El. Net SVM", "El. Net", "Lasso", "MCP", "SCAD")
knitr::kable(tab.tot.kmeans, caption="Risultati K-Means")
```

Infine, per quanto riguarda le etichette Kaggle, come già detto in precedenza, MCP e SCAD non sono stati tenuti in considerazione poiché non fornivano risultati sufficienti.
Si riportano quindi i risultati relativi a Elastic Net, con SVM e glmnet, e il Lasso.

Rispetto alle altre etichette, queste in oggetto, forniscono i risultati peggiori, con errore di classificazione bassi associati a specificità molto elevata.
Infatti, anche il Kappa di Cohen risulta essere negativo o leggermente positivo in tutti i casi.

In generale le etichette Kaggle portano anche ad un numero di variabili selezionate più elevato rispetto alle altre etichette, a riprova della scarsa efficacia dei modelli.

```{r eval=FALSE, include=FALSE}
Metriche_tot_Kaggle = sapply(grep("Metriche", ls(), value = T), get)
colnames(Metriche_tot_Kaggle) <- gsub("Metriche_","",colnames(Metriche_tot))
knitr::kable(Metriche_tot_Kaggle)


Tab_tot_Kaggle = sapply(grep("Tabella", ls(), value = T), get)
colnames(Tab_tot_Kaggle) <- gsub("Tabella_","",colnames(Tab_tot_Kaggle))

kappa.finali <- c()
for(i in 1:4){
  kappa.finali <- c(kappa.finali,
                    Kappa(matrix(Tab_tot[,i], 2,2, byrow=F))$Unweighted[1])}
names(kappa.finali) = gsub("Tabella_","",colnames(Tab_tot))
kappa.finali <- data.frame(kappa.finali)
names(kappa.finali) <- "Kappa di Cohen"
knitr::kable(kappa.finali)
Tab_Kappa_Kaggle <- kappa.finali
# Tabella con kappa di cohen finale

Tab_ncoef.Kaggle <- sapply(grep("n.coef", ls(), value = T), get)
# Tabella con il numero di coefficienti tenuti dai modelli
Tab_ncoef.Kaggle <- as.data.frame(Tab_ncoef.Kaggle)
colnames(Tab_ncoef.Kaggle) <- c("Numero di coefficienti diversi da 0")
knitr::kable(Tab_ncoef.Kaggle)
# Coefficienti diversi da 0
```

```{r Metriche Kaggle, echo=FALSE}
tab.tot.kaggle <- rbind(Metriche_tot_Kaggle, 
                        round(Tab_Kappa_Kaggle[1:3],3),
                        Tab_ncoef.Kaggle$`Numero di coefficienti diversi da 0`)
row.names(tab.tot.kaggle)[11] <- "Coefficienti diversi da 0"
row.names(tab.tot.kaggle)[10] <- "Kappa di Cohen"
colnames(tab.tot.kaggle)=c("El. Net SVM", "El. Net", "Lasso")
knitr::kable(tab.tot.kaggle, caption="Risultati Kaggle")
```

## 4.3 Dataset Alternativi

Infine, si è deciso di applicare i modelli migliori per ciascuna etichetta ai relativi dataset modificati.
Per ogni etichetta si adatta un modello Elastic Net, in quanto aveva portato a risultati più stabili; per le etichette Fused si adatta anche NSC, in quanto aveva mostrato una buona performance.

Per quanto riguarda le immagini su cui è stato applicato il *Fused Lasso Signal Approximator*, i risultati ottenuti sono riportati in Tabella 5.
Si può notare come Elastic Net per le etichette fused peggiori leggermente l'errore di classificazione rispetto all'analogo sulle immagini originali; rimane invece pressoché identico per l'Elastic Net effettuato sulle etichette k-means e per l'NSC sulle etichette Fused.
Non si riportano i risultati per l'Elastic Net effettuato sulle etichette Kaggle in quanto non hanno consentito una classificazione soddisfacente.
In genere quindi non si può affermare che questa tecnica sia vantaggiosa in quanto porta anche a ridondanza dei dati e variabili vicine tra loro saranno simili.

```{r FLSA Codice, eval=FALSE, include=FALSE}
rm(list=ls())
load("Modelli_FLSA_def1.RData")
current.path <- getwd()

# Workdirectory "Progetto"

# Librerie utilizzate -----------------------------------------------------

library(data.table)
library(glmnet)
library(sparseSVM)
library(ncvreg)
library(pamr)
library(tidyverse)
require(doMC)
library(vcd)

# Carico dati --------------------------------------------------------

dati <- fread("Dataset/Matrice_fused.csv")
dati$V1 <- NULL
# Carico la matrice dei pixel ai quali C( stato applicato flsa

dati <- data.frame(fread("Dataset/Data_k2_k3_models.csv")[,c(1:10)],
                   dati)

# Prendo le etichette da modellare

id.test <- which(dati$Day == 3)
# Definisco di usare come insieme di verifica il terzo giorno 

train <- dati[-id.test,]
test <- dati[id.test,]
# Creo insieme di stima e insieme di verifica

rm(dati)
# Cancello i dati dall'ambiente attuale cosi da avere meno memoria occupata

# _____________ -----------------------------------------------------------
# Definizione Metriche ----------------------------------------------------


tabella.sommario <- function(previsti, osservati){
  n <-  table(previsti,osservati)
  err.tot <- 1-sum(diag(n))/sum(n)
  fn <- n[1,2]/(n[1,2]+n[2,2])
  fp <- n[2,1]/(n[1,1]+n[2,1])
  print(n)
  cat("errore totale: ", format(err.tot),"\n")
  cat("falsi positivi & falsi negativi: ",format(c(fp, fn)),"\n")
  invisible(n)
}
# Funzione per calcolare tabella di Errata-Classificazione

indici.errore<-function(tabella){
  Errata.Classificazione<- round((tabella[2]+tabella[3])/sum(tabella),3)
  Accuratezza<- round((tabella[1]+tabella[4])/sum(tabella),3)
  SensibilitC <- round((tabella[4]/sum(tabella[3],tabella[4])),3)
  SpecificitC <- round((tabella[1]/sum(tabella[1],tabella[2])),3)
  alpha <- round((tabella[2]/sum(tabella[1],tabella[2])),3)
  beta <- round((tabella[3]/sum(tabella[3],tabella[4])),3)
  False.Disc.Rate<- round(alpha/(1+alpha-beta),3)
  Precisione <-round((1-beta)/(1+alpha-beta),3)
  F1.Score<- round(2*tabella[4]/(2*tabella[4]+tabella[2]+tabella[3]),3)
  
  False.positive.rate <- alpha
  False.negative.rate <- beta
  
  indici<-data.frame(Errata.Classificazione,
                     Accuratezza,
                     SensibilitC ,
                     SpecificitC ,
                     False.positive.rate,
                     False.negative.rate,
                     False.Disc.Rate,
                     Precisione,
                     F1.Score)
  return(indici)
}
# Funzione per ricavare varie metriche dalla tabella di Errata-Classificazione

# _____________ -----------------------------------------------------------

set.seed(42)
fold <- sample(5, nrow(train), replace = T)
# Per coerenza definisco dei fold uguali per tutti i modelli che verranno
# stimati in CV

X_train <- as.matrix(train[,-c(1:10)])
X_test <- as.matrix(test[,-c(1:10)])
# PoichC( la maggior parte delle funzioni per modellare richiede che la matrice
# del disegno sia un oggetto matrice converto gli insiemi di stima e verifica 
# in matrici
# Elastic net Logistico -------------------------------------------------------

lambda.grid <- exp(seq(-5,-2.5,l=100))
# Come in precedenza si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha <- rep(NA, length(alpha.grid))
se.alpha <- rep(NA, length(alpha.grid))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard


for(i in 1:length(alpha.grid)){
  
  registerDoMC(cores = 5)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train,
                   factor(train$Etichette_Fused_K2),
                   alpha = alpha.grid[i],
                   type.measure = "class",
                   family = "binomial",
                   nfolds = 5,
                   foldid = fold,
                   lambda = lambda.grid,
                   parallel = T,
                   trace.it = 1,
                   seed = 42)
  # Viene stimato un modello logistico con penalita Elastic-Net,
  # in CV a 5 fold con metrica di ottimizzazione l'errore di errata
  # classificazione, per ogni valore di alpha contenuto in alpha.grid,
  # viene oltretutto fissato un seed per la riproducibilitC  dei risultati
  
  error.alpha[i] <- fit$cvm[fit$index["min",]]
  se.alpha[i] <- fit$cvsd[fit$index["min",]]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

stderrcv.k <- se.alpha
`MissClassificationError` <-  error.alpha
`Alpha` <- alpha.grid
data.plot.cv <- tibble(`MissClassificationError`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Fused <- ggplot(data.plot.cv, 
                        mapping = aes(x = `Alpha`, 
                                      y = `MissClassificationError`)) +
  ylim(c(min(`MissClassificationError` - stderrcv.k)-0.005,
         max(`MissClassificationError` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Alpha`, y = `MissClassificationError` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `MissClassificationError` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `MissClassificationError` - stderrcv.k,
                   xend = `Alpha`, 
                   yend = `MissClassificationError` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("MissClassificationError  CV")
srs.acc_Fused
# Visualizzazione dell'errore di classificazione per i diversi alpha 

error.alpha[which.min(error.alpha)]
# Minimo dell'errore di classificazione al variare dell'alpha

best.alpha_Fused <- alpha.grid[which.min(error.alpha)]
best.alpha_Fused
# Viene definito l'alpha per il quale si ottiene l'errore di classificazione 
# minore

E.Net_logistico_Fused <- cv.glmnet(X_train,
                                   factor(train$Etichette_Fused_K2),
                                   type.measure = "class",
                                   family = "binomial",
                                   alpha = best.alpha_Fused,
                                   nfolds = 5,
                                   foldid = fold,
                                   lambda = lambda.grid,
                                   parallel = T,
                                   trace.it = 1,
                                   seed = 42)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Fused, in CV a 5 fold con metrica di ottimizzazione l'errore di 
# errata classificazione, per ogni valore di alpha contenuto in alpha.grid,
# viene oltretutto fissato un seed per la riproducibilitC  dei risultati

plot(E.Net_logistico_Fused)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net


n.coef_Enet_Fused <- length(which(coef(E.Net_logistico_Fused, s=E.Net_logistico_Fused$lambda.min)!=0))
# Numero di coefficienti diverso da 0


y.hat.E.Net_Fused <- predict(E.Net_logistico_Fused, 
                             s = E.Net_logistico_Fused$lambda.min, 
                             newx = X_test, 
                             type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.E.Net_Fused <- tabella.sommario(test$Etichette_Fused_K2 , 
                                        y.hat.E.Net_Fused)
Metriche_E.Net_Fused <- indici.errore(Tabella.E.Net_Fused)
# Salvo le varie metriche ottenute da tale modello 

# NSC ---------------------------------------------------------------------

result_Fused <- list(x = t(scale(X_train)),
                     y = factor(train$Etichette_Fused_K2))
# Definisco una lista contenente la matrice del disegno e la variabile risposta

nsc_Fused <- pamr.train(result_Fused)
# Stimo il modello

best.t_Fused <- nsc_Fused$threshold[which.min(nsc_Fused$errors)]
# Delimito la miglior soglia (parametro di regolarizzazione) in base all'errore
# ottenuto sul train

nsc.opt_Fused <- pamr.train(result_Fused, threshold = best.t_Fused)
# Ristimo il modello per la soglia ottimale

y.hat.nsc_Fused <- pamr.predict(nsc.opt_Fused, 
                                t(scale(X_test)), 
                                threshold = best.t_Fused)
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

scen_Fused <- pamr.predict(nsc.opt_Fused, 
                           result_Fused$x, 
                           threshold = best.t_Fused, 
                           type="cent")
# Calcolo i centroidi ovvero i valori delle variabili diverse da zero tenute
# dal modello al fine di poterle visualizzare 

dif_Fused <- (scen_Fused - nsc.opt_Fused$centroid.overall)/(nsc.opt_Fused$sd)
# Poiche i valori delle covariate vengono passate standardizzate, per effetuare
# un confronto sensato si standardizzano anche i valori dei centroidi 

n.coef_NSC_Fused_Flsa <- length(which(dif_Fused[,1]!=0))
length(which(dif_Fused[,2]!=0))


par(mfrow = c(1,2))
plot(y = 1:length(nsc.opt_Fused$centroids[,1]), 
     x = nsc.opt_Fused$centroids[,1], 
     col="lightgrey", 
     type="l",
     main = "Fused label 1",
     xlab = "Centroidi",
     ylab = "Pixel")
abline(v=0, col="slateblue")
lines(y = 1:length(dif_Fused[,1]),
      x = dif_Fused[,1], 
      col="blue", 
      lwd=3)

plot(y = 1:length(nsc.opt_Fused$centroids[,2]), 
     x = nsc.opt_Fused$centroids[,2], 
     col="lightgrey", 
     type="l",
     main = "Fused label 2",
     xlab = "Centroidi",
     ylab = "Pixel")
abline(v=0, col="slateblue")
lines(y = 1:length(dif_Fused[,2]),
      x = dif_Fused[,2], 
      col="blue", 
      lwd=3)
par(mfrow = c(1,1))
# Visualizzo il comportamento medio dei pixel in base ai centrodi stimati

Tabella.nsc_Fused <- tabella.sommario(test$Etichette_Fused_K2 , 
                                      y.hat.nsc_Fused)
Metriche_NSC_Fused <- indici.errore(Tabella.nsc_Fused)
# Salvo le varie metriche ottenute da tale modello 
# Elastic net Logistico -------------------------------------------------------

lambda.grid <- exp(seq(-7,-4,l=100))
# Come in precedenza si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha <- rep(NA, length(alpha.grid))
se.alpha <- rep(NA, length(alpha.grid))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard

for(alpha in 1:length(alpha.grid)){
  
  registerDoMC(cores = 5)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train,
                   factor(train$Etichette_Kmeans_K2),
                   alpha = alpha.grid[alpha],
                   type.measure = "class",
                   family = "binomial",
                   nfolds = 5,
                   foldid = fold,
                   lambda = lambda.grid,
                   parallel = T,
                   trace.it = 1,
                   seed = 42,
                   keep = T)
  # Viene stimato un modello logistico con penalita Elastic-Net, in CV a 5 fold
  # con metrica di ottimizzazione l'errore di errata classificazione,
  # viene oltretutto fissato un seed per la riproducibilitC  dei risultati.
  # Con l'etichetta presente perC2 occorre fare una considerazione, ci troviamo 
  # in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
  # metrica come l'errore di errata classificazione potrebbe portare a 
  # conclusioni inestatte, per tanto viene impostato un keep = T all'interno del
  # modello, che permette di avere per ogni lambda la stima del predittore 
  # lineare delle unita statistiche che stanno nell'out of fold, cosi da
  # riprodurre una CV fatta a mano utilizzando i risultati ottenuti dalla 
  # funzione cv.glment
  
  eta <- fit$fit.preval
  pi.train <- exp(eta)/(1+exp(eta))
  pred.train.class <- ifelse(pi.train > 1/2, 2, 1)
  # Mi riconduco dal predittore linare alle classi stimate 
  
  kappa.vals <- matrix(NA, ncol(pred.train.class), 5)
  # Una metrica ragionevole per il caso presente di sbilanciamento C( il Kappa di
  # cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
  # numero di lambda, e numero di colonne pari ai fold utilizzati
  
  for( i in 1:ncol(pred.train.class)){
    
    for(j in 1:5){
      ind.fold.out <- which(fit$foldid == j)
      # Definisco il fold corrente
      
      if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
        # Condizione per la quale si valuta la metrica relativa a quel fold, di
        # un lambda se e solo se le classi stimate non hanno solo un livello
      {
        
        tab <- table(pred.train.class[ind.fold.out,i], 
                     train$Etichette_Kmeans_K2[ind.fold.out])
        kappa.vals[i,j] <- Kappa(tab)$Unweighted[1]
        # Calcolo Kappa e salvo il risultato per il j-esimo fold
        
      }
    }
  }
  
  error.Lambda <- apply(kappa.vals,1,mean)
  se.Lambda <- apply(kappa.vals,1,sd)
  # Calcolo Kappa medio per ogni fold e relativa deviazione standard
  
  error.alpha[alpha] <- error.Lambda[which.max(error.Lambda)]
  se.alpha[alpha] <- se.Lambda[which.max(error.Lambda)]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

stderrcv.k <- se.alpha
`Kappa Cohen` <-  error.alpha
`Alpha` <- alpha.grid
data.plot.cv <- tibble(`Kappa Cohen`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Kmeans.alpha <- ggplot(data.plot.cv, 
                               mapping = aes(x = `Alpha`, 
                                             y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k,
                   xend = `Alpha`, 
                   yend = `Kappa Cohen` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kmeans.alpha
# Visualizzazione di Kappa per i diversi alpha 

error.alpha[which.max(error.alpha)]
# Massimo di Kappa al variare dell'alpha

best.alpha_Kmeans <- alpha.grid[which.max(error.alpha)]
best.alpha_Kmeans
# Viene definito l'alpha per il quale si ottiene il Kappa maggiore

E.Net_logistico_Kmeans <- cv.glmnet(X_train,
                                    factor(train$Etichette_Kmeans_K2),
                                    type.measure = "class",
                                    family = "binomial",
                                    alpha = best.alpha_Kmeans,
                                    nfolds = 5,
                                    foldid = fold,
                                    lambda = lambda.grid,
                                    parallel = T,
                                    trace.it = 1,
                                    seed = 42,
                                    keep = T)

# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Kmeans

plot(E.Net_logistico_Kmeans)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net




eta <- E.Net_logistico_Kmeans$fit.preval
pi.train <- exp(eta)/(1+exp(eta))
pred.train.class <- ifelse(pi.train > 1/2, 2, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals <- matrix(NA, ncol(pred.train.class), 5)
# Inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class)){
  
  for(j in 1:5){
    ind.fold.out <- which(E.Net_logistico_Kmeans$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class[ind.fold.out,i], 
                   train$Etichette_Kmeans_K2[ind.fold.out])
      kappa.vals[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda <- apply(kappa.vals,1,mean)
se.Lambda <- apply(kappa.vals,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k <- se.Lambda
`Kappa Cohen` <-  error.Lambda
`Log(Lambda)` <- log(E.Net_logistico_Kmeans$lambda)
data.plot.cv <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans.Enet <- ggplot(data.plot.cv, 
                              mapping = aes(x = `Log(Lambda)`, 
                                            y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kmeans.Enet
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

lambda.k.Kmeans.Enet <- E.Net_logistico_Kmeans$lambda[which.max(error.Lambda)]
# Definisco il lambda che massimizza Kappa


n.coef_Enet_Kmeans <- length(which(coef(E.Net_logistico_Kmeans, s=lambda.k.Kmeans.Enet)!=0))
# Numero di coefficienti diversi da 0

y.hat.E.Net_Kmeans <- predict(E.Net_logistico_Kmeans,
                              s = lambda.k.Kmeans.Enet,
                              newx = X_test,
                              type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.E.Net_Kmeans <- tabella.sommario(y.hat.E.Net_Kmeans, test$Etichette_Kmeans_K2)
Metriche_E.Net_Kmeans <- indici.errore(Tabella.E.Net_Kmeans)
# Salvo le varie metriche ottenute da tale modello 

save.image("ENET_Kmeans.RData")

# Elastic net Logistico -------------------------------------------------------

lambda.grid <- exp(seq(-7,-4,l=100))
# Come in precedenza si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha <- rep(NA, length(alpha.grid))
se.alpha <- rep(NA, length(alpha.grid))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard

for(alpha in 1:length(alpha.grid)){
  
  registerDoMC(cores = 5)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train,
                   factor(train$Etichette_Kaggle_K2),
                   alpha = alpha.grid[alpha],
                   type.measure = "class",
                   family = "binomial",
                   nfolds = 5,
                   foldid = fold,
                   lambda = lambda.grid,
                   parallel = T,
                   trace.it = 1,
                   seed = 42,
                   keep = T)
  # Viene stimato un modello logistico con penalita Elastic-Net, in CV a 5 fold
  # con metrica di ottimizzazione l'errore di errata classificazione,
  # viene oltretutto fissato un seed per la riproducibilitC  dei risultati.
  # Con l'etichetta presente perC2 occorre fare una considerazione, ci troviamo 
  # in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
  # metrica come l'errore di errata classificazione potrebbe portare a 
  # conclusioni inestatte, per tanto viene impostato un keep = T all'interno del
  # modello, che permette di avere per ogni lambda la stima del predittore 
  # lineare delle unita statistiche che stanno nell'out of fold, cosi da
  # riprodurre una CV fatta a mano utilizzando i risultati ottenuti dalla 
  # funzione cv.glment
  
  eta <- fit$fit.preval
  pi.train <- exp(eta)/(1+exp(eta))
  pred.train.class <- ifelse(pi.train > 1/2, 0, 1)
  # Mi riconduco dal predittore linare alle classi stimate 
  
  kappa.vals <- matrix(NA, ncol(pred.train.class), 5)
  # Una metrica ragionevole per il caso presente di sbilanciamento C( il Kappa di
  # cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
  # numero di lambda, e numero di colonne pari ai fold utilizzati
  
  for( i in 1:ncol(pred.train.class)){
    
    for(j in 1:5){
      ind.fold.out <- which(fit$foldid == j)
      # Definisco il fold corrente
      
      if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
        # Condizione per la quale si valuta la metrica relativa a quel fold, di
        # un lambda se e solo se le classi stimate non hanno solo un livello
      {
        
        tab <- table(pred.train.class[ind.fold.out,i], 
                     train$Etichette_Kaggle_K2[ind.fold.out])
        kappa.vals[i,j] <- Kappa(tab)$Unweighted[1]
        # Calcolo Kappa e salvo il risultato per il j-esimo fold
        
      }
    }
  }
  
  error.Lambda <- apply(kappa.vals,1,mean)
  se.Lambda <- apply(kappa.vals,1,sd)
  # Calcolo Kappa medio per ogni fold e relativa deviazione standard
  
  error.alpha[alpha] <- error.Lambda[which.max(error.Lambda)]
  se.alpha[alpha] <- se.Lambda[which.max(error.Lambda)]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

stderrcv.k <- se.alpha
`Kappa Cohen` <-  error.alpha
`Alpha` <- alpha.grid
data.plot.cv <- tibble(`Kappa Cohen`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Kaggle.alpha <- ggplot(data.plot.cv, 
                               mapping = aes(x = `Alpha`, 
                                             y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k,
                   xend = `Alpha`, 
                   yend = `Kappa Cohen` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kaggle.alpha
# Visualizzazione di Kappa per i diversi alpha 

error.alpha[which.max(error.alpha)]
# Massimo di Kappa al variare dell'alpha

best.alpha_Kaggle <- alpha.grid[which.max(error.alpha)]
best.alpha_Kaggle
# Viene definito l'alpha per il quale si ottiene il Kappa maggiore

lambda.grid <- exp(seq(-5,-3,l=100))


E.Net_logistico_Kaggle <- cv.glmnet(X_train,
                                    factor(train$Etichette_Kaggle_K2),
                                    type.measure = "class",
                                    family = "binomial",
                                    alpha = best.alpha_Kaggle,
                                    nfolds = 5,
                                    foldid = fold,
                                    lambda = lambda.grid,
                                    parallel = T,
                                    trace.it = 1,
                                    seed = 42,
                                    keep = T)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Kaggle

plot(E.Net_logistico_Kaggle)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net

eta <- E.Net_logistico_Kaggle$fit.preval
pi.train <- exp(eta)/(1+exp(eta))
pred.train.class <- ifelse(pi.train > 1/2, 0, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals <- matrix(NA, ncol(pred.train.class), 5)
# Inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class)){
  
  for(j in 1:5){
    ind.fold.out <- which(E.Net_logistico_Kaggle$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class[ind.fold.out,i], 
                   train$Etichette_Kaggle_K2[ind.fold.out])
      kappa.vals[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda <- apply(kappa.vals,1,mean)
se.Lambda <- apply(kappa.vals,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k <- se.Lambda
`Kappa Cohen` <-  error.Lambda
`Log(Lambda)` <- log(E.Net_logistico_Kaggle$lambda)
data.plot.cv <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kaggle.Enet <- ggplot(data.plot.cv, 
                              mapping = aes(x = `Log(Lambda)`, 
                                            y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kaggle.Enet
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

lambda.k.Enet.Kaggle <- E.Net_logistico_Kaggle$lambda[which.max(error.Lambda)]
# Definisco il lambda che massimizza Kappa


n.coef_Enet_Kaggle <- length(which(coef(E.Net_logistico_Kaggle, s=lambda.k.Enet.Kaggle)!=0))
# Numero di coefficienti diversi da 0


y.hat.E.Net_Kaggle <- predict(E.Net_logistico_Kaggle,
                              s = lambda.k.Enet.Kaggle,
                              newx = X_test,
                              type = "class")
table(y.hat.E.Net_Kaggle)
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe
table(y.hat.E.Net_Kaggle)
Tabella.E.Net_Kaggle <- tabella.sommario(y.hat.E.Net_Kaggle, test$Etichette_Kaggle_K2 )
Metriche_E.Net_Kaggle <- indici.errore(Tabella.E.Net_Kaggle)
# Salvo le varie metriche ottenute da tale modello



# _____________ -----------------------------------------------------------
# Confronti Finali --------------------------------------------------------

Metriche_tot_FLSA = sapply(grep("Metriche", ls(), value = T), get)
colnames(Metriche_tot_FLSA) <- gsub("Metriche_","",colnames(Metriche_tot_FLSA))
# Salvo tutte le metriche di tutti i modelli in un unico oggetto al
# fine di poter confrontare meglio tutti i modelli 

knitr::kable(Metriche_tot_FLSA)
# Visualizzo le diverse metriche per i diversi modelli


Tab_tot_FLSA = sapply(grep("Tabella", ls(), value = T), get)
colnames(Tab_tot_FLSA) <- gsub("Tabella_","",colnames(Tab_tot_FLSA))

kappa.finali <- c()
for(i in 1:4){
  kappa.finali <- c(kappa.finali,Kappa(matrix(Tab_tot_FLSA[,i], 2,2, byrow=F))$Unweighted[1])
}

names(kappa.finali) = gsub("Tabella_","",colnames(Tab_tot_FLSA))
kappa.finali <- data.frame(kappa.finali)
names(kappa.finali) <- "Kappa di Cohen"
knitr::kable(kappa.finali)
Tab_Kappa_FLSA <- kappa.finali
# Calcolo del Kappa di Cohen per ciascun modello

Tab_ncoef.FLSA <- sapply(grep("n.coef", ls(), value = T), get)
Tab_ncoef.FLSA <- as.data.frame(Tab_ncoef.FLSA)
colnames(Tab_ncoef.FLSA) <- c("Numero di coefficienti diversi da 0")
knitr::kable(Tab_ncoef.FLSA)
# Tabella con il numero di coefficienti tenuti dai modelli


# Salvo risultati ---------------------------------------------------------

rm(train)
rm(test)
rm(X_train)
rm(X_test)
# Elimino dall'ambiente gli insiemi di verifica e stima utilizzati

save.image("Modelli_FLSA_def1.RData")
save(Metriche_tot_FLSA, Tab_Kappa_FLSA, Tab_ncoef.FLSA, file="metriche_flsa1.RData")

```

```{r Risultati FLSA, echo=FALSE, warning=FALSE}
load("Environment/metriche_flsa1.RData")
tab.tot.flsa <- rbind(Metriche_tot_FLSA, 
                        round(Tab_Kappa_FLSA$`Kappa di Cohen`,3),
                        Tab_ncoef.FLSA$`Numero di coefficienti diversi da 0`)
row.names(tab.tot.flsa)[11] <- "Coefficienti diversi da 0"
row.names(tab.tot.flsa)[10] <- "Kappa di Cohen"
colnames(tab.tot.flsa)=c("El. Net Fused", "El. Net Kmeans ", "NSC Fused")
knitr::kable(tab.tot.flsa, caption="Risultati con dataset FLSA")

```

```{r Codice dataset ridotti, eval=FALSE, include=FALSE}
rm(list=ls())
load("Modelli_RID_def1.RData")
current.path <- getwd()
# Workdirectory "Progetto"

# Librerie utilizzate -----------------------------------------------------

library(data.table)
library(glmnet)
library(sparseSVM)
library(ncvreg)
library(pamr)
library(tidyverse)
require(doMC)
library(vcd)

# Carico dati --------------------------------------------------------

load("Modelli_RID_def.RData")
load("ENET_Kmeans_rid.RData")

dati <- fread("Dataset/Pixel_piccoli.csv")
dati$V1 <- NULL
# Carico la matrice dei pixel ai quali C( stato applicato flsa

dati <- data.frame(fread("Dataset/Data_k2_k3_models.csv")[,c(1:10)], dati)
# Prendo le etichette da modellare

id.test <- which(dati$Day == 3)
# Definisco di usare come insieme di verifica il terzo giorno 

train <- dati[-id.test,]
test <- dati[id.test,]
# Creo insieme di stima e insieme di verifica

rm(dati)
# Cancello i dati dall'ambiente attuale cosi da avere meno memoria occupata

# _____________ -----------------------------------------------------------
# Definizione Metriche ----------------------------------------------------


tabella.sommario <- function(previsti, osservati){
  n <-  table(previsti,osservati)
  err.tot <- 1-sum(diag(n))/sum(n)
  fn <- n[1,2]/(n[1,2]+n[2,2])
  fp <- n[2,1]/(n[1,1]+n[2,1])
  print(n)
  cat("errore totale: ", format(err.tot),"\n")
  cat("falsi positivi & falsi negativi: ",format(c(fp, fn)),"\n")
  invisible(n)
}
# Funzione per calcolare tabella di Errata-Classificazione

indici.errore<-function(tabella){
  Errata.Classificazione<- round((tabella[2]+tabella[3])/sum(tabella),3)
  Accuratezza<- round((tabella[1]+tabella[4])/sum(tabella),3)
  SensibilitC <- round((tabella[4]/sum(tabella[3],tabella[4])),3)
  SpecificitC <- round((tabella[1]/sum(tabella[1],tabella[2])),3)
  alpha <- round((tabella[2]/sum(tabella[1],tabella[2])),3)
  beta <- round((tabella[3]/sum(tabella[3],tabella[4])),3)
  False.Disc.Rate<- round(alpha/(1+alpha-beta),3)
  Precisione <-round((1-beta)/(1+alpha-beta),3)
  F1.Score<- round(2*tabella[4]/(2*tabella[4]+tabella[2]+tabella[3]),3)
  
  False.positive.rate <- alpha
  False.negative.rate <- beta
  
  indici<-data.frame(Errata.Classificazione,
                     Accuratezza,
                     SensibilitC ,
                     SpecificitC ,
                     False.positive.rate,
                     False.negative.rate,
                     False.Disc.Rate,
                     Precisione,
                     F1.Score)
  return(indici)
}
# Funzione per ricavare varie metriche dalla tabella di Errata-Classificazione

# _____________ -----------------------------------------------------------

set.seed(42)
fold <- sample(5, nrow(train), replace = T)
# Per coerenza definisco dei fold uguali per tutti i modelli che verranno
# stimati in CV

X_train <- as.matrix(train[,-c(1:10)])
X_test <- as.matrix(test[,-c(1:10)])
# PoichC( la maggior parte delle funzioni per modellare richiede che la matrice
# del disegno sia un oggetto matrice converto gli insiemi di stima e verifica 
# in matrici
# Elastic net Logistico -------------------------------------------------------

lambda.grid <- exp(seq(-5,-2.5,l=100))
# Come in precedenza si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha <- rep(NA, length(alpha.grid))
se.alpha <- rep(NA, length(alpha.grid))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard


for(i in 1:length(alpha.grid)){
  
  registerDoMC(cores = 5)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train,
                   factor(train$Etichette_Fused_K2),
                   alpha = alpha.grid[i],
                   type.measure = "class",
                   family = "binomial",
                   nfolds = 5,
                   foldid = fold,
                   lambda = lambda.grid,
                   parallel = T,
                   trace.it = 1,
                   seed = 42)
  # Viene stimato un modello logistico con penalita Elastic-Net,
  # in CV a 5 fold con metrica di ottimizzazione l'errore di errata
  # classificazione, per ogni valore di alpha contenuto in alpha.grid,
  # viene oltretutto fissato un seed per la riproducibilitC  dei risultati
  
  error.alpha[i] <- fit$cvm[fit$index["min",]]
  se.alpha[i] <- fit$cvsd[fit$index["min",]]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

stderrcv.k <- se.alpha
`MissClassificationError` <-  error.alpha
`Alpha` <- alpha.grid
data.plot.cv <- tibble(`MissClassificationError`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Fused <- ggplot(data.plot.cv, 
                        mapping = aes(x = `Alpha`, 
                                      y = `MissClassificationError`)) +
  ylim(c(min(`MissClassificationError` - stderrcv.k)-0.005,
         max(`MissClassificationError` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Alpha`, y = `MissClassificationError` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `MissClassificationError` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `MissClassificationError` - stderrcv.k,
                   xend = `Alpha`, 
                   yend = `MissClassificationError` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("MissClassificationError  CV")
srs.acc_Fused
# Visualizzazione dell'errore di classificazione per i diversi alpha 

error.alpha[which.min(error.alpha)]
# Minimo dell'errore di classificazione al variare dell'alpha

best.alpha_Fused <- alpha.grid[which.min(error.alpha)]
best.alpha_Fused
# Viene definito l'alpha per il quale si ottiene l'errore di classificazione 
# minore

E.Net_logistico_Fused <- cv.glmnet(X_train,
                                   factor(train$Etichette_Fused_K2),
                                   type.measure = "class",
                                   family = "binomial",
                                   alpha = best.alpha_Fused,
                                   nfolds = 5,
                                   foldid = fold,
                                   lambda = lambda.grid,
                                   parallel = T,
                                   trace.it = 1,
                                   seed = 42)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Fused, in CV a 5 fold con metrica di ottimizzazione l'errore di 
# errata classificazione, per ogni valore di alpha contenuto in alpha.grid,
# viene oltretutto fissato un seed per la riproducibilitC  dei risultati

plot(E.Net_logistico_Fused)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net

n.coef_Enet_Fused <- length(which(coef(E.Net_logistico_Fused, s=E.Net_logistico_Fused$lambda.min)!=0))
# Numero di coefficienti diverso da 0


y.hat.E.Net_Fused <- predict(E.Net_logistico_Fused, 
                             s = E.Net_logistico_Fused$lambda.min, 
                             newx = X_test, 
                             type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.E.Net_Fused <- tabella.sommario(test$Etichette_Fused_K2 , 
                                        y.hat.E.Net_Fused)
Metriche_E.Net_Fused <- indici.errore(Tabella.E.Net_Fused)
# Salvo le varie metriche ottenute da tale modello 

# NSC ---------------------------------------------------------------------

result_Fused <- list(x = t(scale(X_train)),
                     y = factor(train$Etichette_Fused_K2))
# Definisco una lista contenente la matrice del disegno e la variabile risposta

nsc_Fused <- pamr.train(result_Fused)
# Stimo il modello

best.t_Fused <- nsc_Fused$threshold[which.min(nsc_Fused$errors)]
# Delimito la miglior soglia (parametro di regolarizzazione) in base all'errore
# ottenuto sul train

nsc.opt_Fused <- pamr.train(result_Fused, threshold = best.t_Fused)
# Ristimo il modello per la soglia ottimale

y.hat.nsc_Fused <- pamr.predict(nsc.opt_Fused, 
                                t(scale(X_test)), 
                                threshold = best.t_Fused)
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

scen_Fused <- pamr.predict(nsc.opt_Fused, 
                           result_Fused$x, 
                           threshold = best.t_Fused, 
                           type="cent")
# Calcolo i centroidi ovvero i valori delle variabili diverse da zero tenute
# dal modello al fine di poterle visualizzare 

dif_Fused <- (scen_Fused - nsc.opt_Fused$centroid.overall)/(nsc.opt_Fused$sd)
# Poiche i valori delle covariate vengono passate standardizzate, per effetuare
# un confronto sensato si standardizzano anche i valori dei centroidi 


n.coef_NSC_Fused_rid <- length(which(dif_Fused[,1]!=0))
length(which(dif_Fused[,2]!=0))



par(mfrow = c(1,2))
plot(y = 1:length(nsc.opt_Fused$centroids[,1]), 
     x = nsc.opt_Fused$centroids[,1], 
     col="lightgrey", 
     type="l",
     main = "Fused label 1",
     xlab = "Centroidi",
     ylab = "Pixel")
abline(v=0, col="slateblue")
lines(y = 1:length(dif_Fused[,1]),
      x = dif_Fused[,1], 
      col="blue", 
      lwd=3)

plot(y = 1:length(nsc.opt_Fused$centroids[,2]), 
     x = nsc.opt_Fused$centroids[,2], 
     col="lightgrey", 
     type="l",
     main = "Fused label 2",
     xlab = "Centroidi",
     ylab = "Pixel")
abline(v=0, col="slateblue")
lines(y = 1:length(dif_Fused[,2]),
      x = dif_Fused[,2], 
      col="blue", 
      lwd=3)
par(mfrow = c(1,1))
# Visualizzo il comportamento medio dei pixel in base ai centrodi stimati

Tabella.nsc_Fused <- tabella.sommario(test$Etichette_Fused_K2 , 
                                      y.hat.nsc_Fused)
Metriche_NSC_Fused <- indici.errore(Tabella.nsc_Fused)
# Salvo le varie metriche ottenute da tale modello 



# Elastic net Logistico -------------------------------------------------------

lambda.grid <- exp(seq(-7,-4,l=100))
# Come in precedenza si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha <- rep(NA, length(alpha.grid))
se.alpha <- rep(NA, length(alpha.grid))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard

for(alpha in 1:length(alpha.grid)){
  
  registerDoMC(cores = 5)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train,
                   factor(train$Etichette_Kmeans_K2),
                   alpha = alpha.grid[alpha],
                   type.measure = "class",
                   family = "binomial",
                   nfolds = 5,
                   foldid = fold,
                   lambda = lambda.grid,
                   parallel = T,
                   trace.it = 1,
                   seed = 42,
                   keep = T)
  # Viene stimato un modello logistico con penalita Elastic-Net, in CV a 5 fold
  # con metrica di ottimizzazione l'errore di errata classificazione,
  # viene oltretutto fissato un seed per la riproducibilitC  dei risultati.
  # Con l'etichetta presente perC2 occorre fare una considerazione, ci troviamo 
  # in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
  # metrica come l'errore di errata classificazione potrebbe portare a 
  # conclusioni inestatte, per tanto viene impostato un keep = T all'interno del
  # modello, che permette di avere per ogni lambda la stima del predittore 
  # lineare delle unita statistiche che stanno nell'out of fold, cosi da
  # riprodurre una CV fatta a mano utilizzando i risultati ottenuti dalla 
  # funzione cv.glment
  
  eta <- fit$fit.preval
  pi.train <- exp(eta)/(1+exp(eta))
  pred.train.class <- ifelse(pi.train > 1/2, 2, 1)
  # Mi riconduco dal predittore linare alle classi stimate 
  
  kappa.vals <- matrix(NA, ncol(pred.train.class), 5)
  # Una metrica ragionevole per il caso presente di sbilanciamento C( il Kappa di
  # cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
  # numero di lambda, e numero di colonne pari ai fold utilizzati
  
  for( i in 1:ncol(pred.train.class)){
    
    for(j in 1:5){
      ind.fold.out <- which(fit$foldid == j)
      # Definisco il fold corrente
      
      if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
        # Condizione per la quale si valuta la metrica relativa a quel fold, di
        # un lambda se e solo se le classi stimate non hanno solo un livello
      {
        
        tab <- table(pred.train.class[ind.fold.out,i], 
                     train$Etichette_Kmeans_K2[ind.fold.out])
        kappa.vals[i,j] <- Kappa(tab)$Unweighted[1]
        # Calcolo Kappa e salvo il risultato per il j-esimo fold
        
      }
    }
  }
  
  error.Lambda <- apply(kappa.vals,1,mean)
  se.Lambda <- apply(kappa.vals,1,sd)
  # Calcolo Kappa medio per ogni fold e relativa deviazione standard
  
  error.alpha[alpha] <- error.Lambda[which.max(error.Lambda)]
  se.alpha[alpha] <- se.Lambda[which.max(error.Lambda)]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

stderrcv.k <- se.alpha
`Kappa Cohen` <-  error.alpha
`Alpha` <- alpha.grid
data.plot.cv <- tibble(`Kappa Cohen`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Kmeans.alpha <- ggplot(data.plot.cv, 
                               mapping = aes(x = `Alpha`, 
                                             y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k,
                   xend = `Alpha`, 
                   yend = `Kappa Cohen` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kmeans.alpha
# Visualizzazione di Kappa per i diversi alpha 

error.alpha[which.max(error.alpha)]
# Massimo di Kappa al variare dell'alpha

best.alpha_Kmeans <- alpha.grid[which.max(error.alpha)]
best.alpha_Kmeans
# Viene definito l'alpha per il quale si ottiene il Kappa maggiore

E.Net_logistico_Kmeans <- cv.glmnet(X_train,
                                    factor(train$Etichette_Kmeans_K2),
                                    type.measure = "class",
                                    family = "binomial",
                                    alpha = best.alpha_Kmeans,
                                    nfolds = 5,
                                    foldid = fold,
                                    lambda = lambda.grid,
                                    parallel = T,
                                    trace.it = 1,
                                    seed = 42,
                                    keep = T)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Kmeans

plot(E.Net_logistico_Kmeans)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net

eta <- E.Net_logistico_Kmeans$fit.preval
pi.train <- exp(eta)/(1+exp(eta))
pred.train.class <- ifelse(pi.train > 1/2, 2, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals <- matrix(NA, ncol(pred.train.class), 5)
# Inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for(i in 1:ncol(pred.train.class)){
  
  for(j in 1:5){
    ind.fold.out <- which(E.Net_logistico_Kmeans$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class[ind.fold.out,i], 
                   train$Etichette_Kmeans_K2[ind.fold.out])
      kappa.vals[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda.kmeans <- apply(kappa.vals,1,mean)
se.Lambda.kmeans <- apply(kappa.vals,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k <- se.Lambda
`Kappa Cohen` <-  error.Lambda
`Log(Lambda)` <- log(E.Net_logistico_Kmeans$lambda)
data.plot.cv <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kmeans.Enet <- ggplot(data.plot.cv, 
                              mapping = aes(x = `Log(Lambda)`, 
                                            y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kmeans.Enet
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 

lambda.k.kmeans <- E.Net_logistico_Kmeans$lambda[which.max(error.Lambda.kmeans)]
# Definisco il lambda che massimizza Kappa


n.coef_Enet_Kmeans <- length(which(coef(E.Net_logistico_Kmeans, s =  lambda.k.kmeans)!=0))
# Numero di coefficeinti diversi da 0

y.hat.E.Net_Kmeans <- predict(E.Net_logistico_Kmeans,
                              s = lambda.k.kmeans,
                              newx = X_test,
                              type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.E.Net_Kmeans <- tabella.sommario(y.hat.E.Net_Kmeans,test$Etichette_Kmeans_K2
                                         )
Metriche_E.Net_Kmeans <- indici.errore(Tabella.E.Net_Kmeans)
# Salvo le varie metriche ottenute da tale modello 




# Elastic net Logistico -------------------------------------------------------

lambda.grid <- exp(seq(-7,-4,l=100))
# Come in precedenza si tiene una griglia per lambda ristretta cosi venga 
# esplorato un sottoinsieme di valori ragionevoli 

alpha.grid <- c(0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)
error.alpha <- rep(NA, length(alpha.grid))
se.alpha <- rep(NA, length(alpha.grid))
# Per far si che il modello abbia un'adeguata penalita Elastic-Net si 
# prepara una possibile griglia di valori del parametro alpha, e si inzializzano
# due vettori che conterranno l'errore di classificazione medio e la relativa
# deviazione standard

for(alpha in 1:length(alpha.grid)){
  
  registerDoMC(cores = 5)
  # Al fine di parallelizzare la funzione cv.glmnet dichiaro il numero di core
  # che intendo utilizzare per parallelizzare il modello
  
  fit <- cv.glmnet(X_train,
                   factor(train$Etichette_Kaggle_K2),
                   alpha = alpha.grid[alpha],
                   type.measure = "class",
                   family = "binomial",
                   nfolds = 5,
                   foldid = fold,
                   lambda = lambda.grid,
                   parallel = T,
                   trace.it = 1,
                   seed = 42,
                   keep = T)
  # Viene stimato un modello logistico con penalita Elastic-Net, in CV a 5 fold
  # con metrica di ottimizzazione l'errore di errata classificazione,
  # viene oltretutto fissato un seed per la riproducibilitC  dei risultati.
  # Con l'etichetta presente perC2 occorre fare una considerazione, ci troviamo 
  # in un caso di sbilanciamento per tanto stimare i modelli utilizzando una 
  # metrica come l'errore di errata classificazione potrebbe portare a 
  # conclusioni inestatte, per tanto viene impostato un keep = T all'interno del
  # modello, che permette di avere per ogni lambda la stima del predittore 
  # lineare delle unita statistiche che stanno nell'out of fold, cosi da
  # riprodurre una CV fatta a mano utilizzando i risultati ottenuti dalla 
  # funzione cv.glment
  
  eta <- fit$fit.preval
  pi.train <- exp(eta)/(1+exp(eta))
  pred.train.class <- ifelse(pi.train > 1/2, 0, 1)
  # Mi riconduco dal predittore linare alle classi stimate 
  
  kappa.vals <- matrix(NA, ncol(pred.train.class), 5)
  # Una metrica ragionevole per il caso presente di sbilanciamento C( il Kappa di
  # cholen, per tanto inizializzo una matrice vuota con numero di righe pari al 
  # numero di lambda, e numero di colonne pari ai fold utilizzati
  
  for( i in 1:ncol(pred.train.class)){
    
    for(j in 1:5){
      ind.fold.out <- which(fit$foldid == j)
      # Definisco il fold corrente
      
      if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
        # Condizione per la quale si valuta la metrica relativa a quel fold, di
        # un lambda se e solo se le classi stimate non hanno solo un livello
      {
        
        tab <- table(pred.train.class[ind.fold.out,i], 
                     train$Etichette_Kaggle_K2[ind.fold.out])
        kappa.vals[i,j] <- Kappa(tab)$Unweighted[1]
        # Calcolo Kappa e salvo il risultato per il j-esimo fold
        
      }
    }
  }
  
  error.Lambda <- apply(kappa.vals,1,mean)
  se.Lambda <- apply(kappa.vals,1,sd)
  # Calcolo Kappa medio per ogni fold e relativa deviazione standard
  
  error.alpha[alpha] <- error.Lambda[which.max(error.Lambda)]
  se.alpha[alpha] <- se.Lambda[which.max(error.Lambda)]
  # La procedura valuta per ogni modello di parametro alpha_i il miglior 
  # errore con standard deviation associata, per poi essere tra loro confrontati
  
}

stderrcv.k <- se.alpha
`Kappa Cohen` <-  error.alpha
`Alpha` <- alpha.grid
data.plot.cv <- tibble(`Kappa Cohen`,`Alpha`)
# Vengono salvati i risultati ottenuti per i diversi alpha al fine di riprodurre
# un grafico per l'andamento dell'errore 

srs.acc_Kaggle.alpha <- ggplot(data.plot.cv, 
                               mapping = aes(x = `Alpha`, 
                                             y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Alpha`, y = `Kappa Cohen` - stderrcv.k,
                   xend = `Alpha`, 
                   yend = `Kappa Cohen` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kaggle.alpha
# Visualizzazione di Kappa per i diversi alpha 

error.alpha[which.max(error.alpha)]
# Massimo di Kappa al variare dell'alpha

best.alpha_Kaggle <- alpha.grid[which.max(error.alpha)]
best.alpha_Kaggle
# Viene definito l'alpha per il quale si ottiene il Kappa maggiore

lambda.grid <- exp(seq(-5,-3,l=100))


E.Net_logistico_Kaggle <- cv.glmnet(X_train,
                                    factor(train$Etichette_Kaggle_K2),
                                    type.measure = "class",
                                    family = "binomial",
                                    alpha = best.alpha_Kaggle,
                                    nfolds = 5,
                                    foldid = fold,
                                    lambda = lambda.grid,
                                    parallel = T,
                                    trace.it = 1,
                                    seed = 42,
                                    keep = T)
# Viene ristimato il modello logistico con penalita Elastic-Net fissata dal 
# best.alpha_Kaggle

plot(E.Net_logistico_Kaggle)
# Visualizzo l'andamento dell'errore di classificazione per i diversi lambda
# del modello Elastic-Net

eta <- E.Net_logistico_Kaggle$fit.preval
pi.train <- exp(eta)/(1+exp(eta))
pred.train.class <- ifelse(pi.train > 1/2, 0, 1)
# Mi riconduco dal predittore linare alle classi stimate 

kappa.vals <- matrix(NA, ncol(pred.train.class), 5)
# Inizializzo una matrice vuota con numero di righe pari al 
# numero di lambda, e numero di colonne pari ai fold utilizzati

for( i in 1:ncol(pred.train.class)){
  
  for(j in 1:5){
    ind.fold.out <- which(E.Net_logistico_Kaggle$foldid == j)
    # Definisco il fold corrente
    
    if (length(unique(pred.train.class[ind.fold.out,i])) != 1)
      # Condizione per la quale si valuta la metrica relativa a quel fold, di
      # un lambda se e solo se le classi stimate non hanno solo un livello
    {
      
      tab <- table(pred.train.class[ind.fold.out,i], 
                   train$Etichette_Kaggle_K2[ind.fold.out])
      kappa.vals[i,j] <- Kappa(tab)$Unweighted[1]
      # Calcolo Kappa e salvo il risultato per il j-esimo fold
      
    }
  }
}

error.Lambda <- apply(kappa.vals,1,mean)
se.Lambda <- apply(kappa.vals,1,sd)
# Calcolo Kappa medio per ogni fold e relativa deviazione standard

stderrcv.k <- se.Lambda
`Kappa Cohen` <-  error.Lambda
`Log(Lambda)` <- log(E.Net_logistico_Kaggle$lambda)
data.plot.cv <- tibble(`Kappa Cohen`,`Log(Lambda)`)
# Vengono salvati i risultati ottenuti per i diversi lambda al fine di 
# riprodurre un grafico per l'andamento dell'errore 

srs.acc_Kaggle.Enet <- ggplot(data.plot.cv, 
                              mapping = aes(x = `Log(Lambda)`, 
                                            y = `Kappa Cohen`)) +
  ylim(c(min(`Kappa Cohen` - stderrcv.k)-0.005,
         max(`Kappa Cohen` + stderrcv.k)+0.005))+
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` + stderrcv.k),
             shape = 95, size = 10) +
  geom_point(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k),
             shape = 95, size = 10) +
  geom_segment(aes(x = `Log(Lambda)`, y = `Kappa Cohen` - stderrcv.k,
                   xend = `Log(Lambda)`, 
                   yend = `Kappa Cohen` + stderrcv.k)) +
  geom_point(col = "firebrick1", size = 3) +
  ggtitle("Kappa Cohen  CV")
srs.acc_Kaggle.Enet
# Visualizzazione della metrica Kappa per i diversi lambda, in scala logaritmica 


lambda.k.kaggle <- E.Net_logistico_Kaggle$lambda[which.max(error.Lambda)]
# Definisco il lambda che massimizza Kappa


n.coef_Enet_Kaggle <- length(which(coef(E.Net_logistico_Kmeans, s =  lambda.k.kaggle)!=0))

y.hat.E.Net_Kaggle <- predict(E.Net_logistico_Kaggle,
                              s = lambda.k,
                              newx = X_test,
                              type = "class")
# Calcolo le previsioni del modello ottenndo direttamente come tipo la classe

Tabella.E.Net_Kaggle <- tabella.sommario(test$Etichette_Kaggle_K2 ,
                                         y.hat.E.Net_Kaggle)
Metriche_E.Net_Kaggle <- indici.errore(Tabella.E.Net_Kaggle)
# Salvo le varie metriche ottenute da tale modello



# _____________ -----------------------------------------------------------
# Confronti Finali --------------------------------------------------------

Metriche_tot_RID = sapply(grep("Metriche", ls(), value = T), get)
colnames(Metriche_tot_RID) <- gsub("Metriche_","",colnames(Metriche_tot_RID))
# Salvo tutte le metriche di tutti i modelli in un unico oggetto al
# fine di poter confrontare meglio tutti i modelli 

knitr::kable(Metriche_tot_RID)
# Visualizzo le diverse metriche per i diversi modelli



Tab_tot_RID = sapply(grep("Tabella", ls(), value = T), get)
colnames(Tab_tot_RID) <- gsub("Tabella_","",colnames(Tab_tot_RID))
kappa.finali <- c()
for(i in 1:4){
  kappa.finali <- c(kappa.finali,Kappa(matrix(Tab_tot_RID[,i], 2,2, byrow=F))$Unweighted[1])
}
names(kappa.finali) = gsub("Tabella_","",colnames(Tab_tot_RID))
kappa.finali <- data.frame(kappa.finali)
names(kappa.finali) <- "Kappa di Cohen"
knitr::kable(kappa.finali)
Tab_Kappa_RID <- kappa.finali
# Tabella con valore di Kappa per ciascun modello

Tab_ncoef.RID <- sapply(grep("n.coef", ls(), value = T), get)
#names(Tab_ncoef.Kmeans) <- gsub("Metriche_","",colnames(Metriche_tot))
Tab_ncoef.RID <- as.data.frame(Tab_ncoef.RID)
colnames(Tab_ncoef.RID) <- c("Numero di coefficienti diversi da 0")
knitr::kable(Tab_ncoef.RID)
# Tabella con il numero di coefficienti tenuti dai modelli



# Salvo risultati ---------------------------------------------------------

rm(train)
rm(test)
rm(X_train)
rm(X_test)
# Elimino dall'ambiente gli insiemi di verifica e stima utilizzati

save.image("Modelli_RID_def1.RData")
save(Metriche_tot_RID, Tab_Kappa_RID, Tab_ncoef.RID, file="metriche_RID1.RData")

```

```{r eval=FALSE, include=FALSE}
colnames(Metriche_tot)=c("El. Net Fused", "El. Net Kaggle", "El. Net K-means", "NSC Fused")
knitr::kable(Metriche_tot, caption="Risultati sulle immagini ridotte")
```

```{r Risultati dataset ridotti, echo=FALSE}
load("Environment/metriche_RID1.RData")
tab.tot.rid <- rbind(Metriche_tot_RID, 
                        round(Tab_Kappa_RID$`Kappa di Cohen`,3),
                        Tab_ncoef.RID$`Numero di coefficienti diversi da 0`)
row.names(tab.tot.rid)[11] <- "Coefficienti diversi da 0"
row.names(tab.tot.rid)[10] <- "Kappa di Cohen"
colnames(tab.tot.rid)=c("El. Net Fused", "El. Net Kaggle ", "El. Net Kmeans", "NSC Fused")
knitr::kable(tab.tot.rid, caption="Risultati con dataset con immagini ridotte")
```

Per quanto riguarda invece le immagini ridotte, i risultati sono visibili nella Tabella 6.
In termini di accuratezza, i risultati non variano particolarmente rispetto al dataset iniziale per l'Elastic Net adattato sulle etichette kaggle e per il NSC sulle etichette fused. Si nota invece un leggero miglioramento per l'Elastic Net sulle etichette fused, che da 0.6 va a 0.63, e sulle etichette k-means, in cui l'accuracy aumenta da 0.619 a 0.665.


# 5. Conclusione

Per concludere, si nota che, com'era da aspettarsi, nessuno dei modelli abbia una particolare capacità predittiva.
I modelli adattati in questa sede, infatti, non tengono in considerazione le interazioni e la dipendenza sia tra osservazioni che l'ordinamento delle variabili.

Particolarmente efficace, rispetto alle altre etichette, sono quelle definite tramite *Fused Lasso Signal Approximator*, che, oltre a rendere l'insieme di dati bilanciato, consente di stimare modelli che pur non raggiungendo errori minimi, forniscono un Kappa di Cohen sempre positivo.

Pur essendo a conoscenza della scarsa capacità interpretativa dei modelli adattati a questo tipo di dati, è stato comunque di interesse verificare se fossero presenti dei pattern nella selezione delle variabili nei diversi modelli.

Per questioni di tempo, prolissità del progetto, scarse performance dei modelli o limiti computazionali, si è deciso di accantonare l'adattamento di modelli con etichette multiclasse (FLSA e k-means, che sono comunque state definite), nonché ulteriori riduzioni della dimensionalità del dataset, come ad esempio attraverso PCA sparsa e denoising di immagini.

# 6. Bibliografia

-   *Statistical Learning with Sparsity: The Lasso and Generalizations*.
    (7 maggio 2015) Libro di Martin J. Wainwright, Robert Tibshirani e Trevor Hastie.

-   *Off-Road Terrain Dataset for Autonomous Vehicles - A dataset with images from a monocular camera and relevant sensor data.* <https://www.kaggle.com/datasets/magnumresearchgroup/offroad-terrain-dataset-for-autonomous-vehicles>

-   Gresenz, G., White, J., & Schmidt, D.C.
    (2021).
    "An Off-Road Terrain Dataset Including Images Labeled With Measures of Terrain Roughness." **Proceedings of the IEEE International Conference in Autonomous Systems**, 309-313.

